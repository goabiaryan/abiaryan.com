
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="Personal Blog" name="description"/>
<meta content="Abi Aryan" name="author"/>
<link href="https://abiaryan.com/machine-learning/posts/2019-09-09-game-ai/" rel="canonical"/>
<link href="../../../logo" rel="icon"/>
<meta content="mkdocs-1.4.0, mkdocs-material-8.5.6" name="generator"/>
<title>Developing A.I. using Games - Abi Aryan</title>
<link href="../../../assets/stylesheets/main.20d9efc8.min.css" rel="stylesheet"/>
<link href="../../../assets/stylesheets/palette.cbb835fc.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gdesc-inner { font-size: 0.75rem; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            
                .gscrollbar-fixer { padding-right: 15px; }
                </style><script src="../../../assets/javascripts/glightbox.min.js"></script></head>
<body data-md-color-accent="teal" data-md-color-primary="white" data-md-color-scheme="default" dir="ltr">
<script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#various-kinds-of-intelligence">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Abi Aryan" class="md-header__button md-logo" data-md-component="logo" href="../../.." title="Abi Aryan">
<svg viewbox="0 0 89 89" xmlns="http://www.w3.org/2000/svg">
<path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z"></path>
<path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5"></path>
<path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z"></path>
<path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25"></path>
</svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Abi Aryan
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Developing A.I. using Games
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="teal" data-md-color-media="" data-md-color-primary="white" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"></path></svg>
</label>
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="red" data-md-color-media="" data-md-color-primary="red" data-md-color-scheme="slate" id="__palette_2" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"></path></svg>
</label>
</form>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list"></ol>
</div>
</div>
</div>
</div>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<nav aria-label="Tabs" class="md-tabs" data-md-component="tabs">
<div class="md-tabs__inner md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../..">
      About
    </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../podcast/posts/legal-tech/">
      Podcast
    </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../ml-engineering/posts/mlops-open-problems/">
        MLOps
      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link md-tabs__link--active" href="./">
        Theoretical ML
      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../fun-zone/posts/news/">
        Fun Zone
      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../tags/">
        Archive
      </a>
</li>
</ul>
</div>
</nav>
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Abi Aryan" class="md-nav__button md-logo" data-md-component="logo" href="../../.." title="Abi Aryan">
<svg viewbox="0 0 89 89" xmlns="http://www.w3.org/2000/svg">
<path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z"></path>
<path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5"></path>
<path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z"></path>
<path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25"></path>
</svg>
</a>
    Abi Aryan
  </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../..">
        About
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../podcast/posts/legal-tech/">
        Podcast
      </a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" id="__nav_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_3">
          MLOps
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="MLOps" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
          MLOps
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../ml-engineering/posts/mlops-open-problems/">
        Open Problems in MLOps
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../ml-engineering/posts/2019-05-10-why-tensorlayer/">
        Why TensorLayer?
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" id="__nav_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_4">
          Theoretical ML
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="Theoretical ML" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
          Theoretical ML
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" data-md-toggle="toc" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
          Developing A.I. using Games
          <span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
        Developing A.I. using Games
      </a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#various-kinds-of-intelligence">
    Various Kinds of Intelligence
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#single-player-games">
    Single Player¬†Games
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#key-algorithms-for-ai-in-single-player-games">
    Key Algorithms for AI in Single Player¬†Games
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#multiplayer-games">
    Multiplayer Games
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#the-world-is-a-simulation">
    The World is a Simulation
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#but-training-ai-on-games-isnt-a-good-idea-in-the-long-run">
    But training AI on Games isn‚Äôt a good idea in the long¬†run
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#open-endedness">
    Open-Endedness
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#introducing-pcg">
    Introducing PCG
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#the-way-ahead-mixed-initiative-games">
    The Way Ahead: Mixed-Initiative Games
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2017-09-10-associative-memory/">
        Dense Associative Memory for Pattern Recognition
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../automl-problems-2018/">
        AutoML - An Overview
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" id="__nav_5" type="checkbox"/>
<label class="md-nav__link" for="__nav_5">
          Fun Zone
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="Fun Zone" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_5">
<span class="md-nav__icon md-icon"></span>
          Fun Zone
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../fun-zone/posts/news/">
        News
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../fun-zone/posts/re-learn/">
        Why Re-Learn Your Core Skills?
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../fun-zone/posts/reading/">
        Books I am Reading
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" id="__nav_6" type="checkbox"/>
<label class="md-nav__link" for="__nav_6">
          Archive
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="Archive" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_6">
<span class="md-nav__icon md-icon"></span>
          Archive
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../tags/">
        Sections
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#various-kinds-of-intelligence">
    Various Kinds of Intelligence
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#single-player-games">
    Single Player¬†Games
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#key-algorithms-for-ai-in-single-player-games">
    Key Algorithms for AI in Single Player¬†Games
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#multiplayer-games">
    Multiplayer Games
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#the-world-is-a-simulation">
    The World is a Simulation
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#but-training-ai-on-games-isnt-a-good-idea-in-the-long-run">
    But training AI on Games isn‚Äôt a good idea in the long¬†run
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#open-endedness">
    Open-Endedness
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#introducing-pcg">
    Introducing PCG
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#the-way-ahead-mixed-initiative-games">
    The Way Ahead: Mixed-Initiative Games
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<nav class="md-tags">
<a class="md-tag" href="../../../tags/#pcg">PCG</a>
<a class="md-tag" href="../../../tags/#deep-learning">deep learning</a>
<a class="md-tag" href="../../../tags/#multiagent-systems">multiagent systems</a>
</nav>
<h1>Developing A.I. using Games</h1>
<p><strong>September 9, 2019</strong> <img alt="üéÉ" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/1f383.svg" title=":jack_o_lantern:"/> <code>PUBLISHED</code></p>
<p>Intelligence is a highly abstract term that could mean several different things to several different people. In 2006, Shane Legg and Marcus Hutter decided to conduct a comprehensive literature review for the definition of intelligence in various disciplines. And thus, it paved the way for them to create a ‚Äúcomplete‚Äù definition of intelligence finally presented in their 2007 paper <a href="https://arxiv.org/abs/0712.3329%0A">Universal Intelligence: A Definition of Machine Intelligence</a>. They defined it as -</p>
<blockquote>
<p>Intelligence measures an agent‚Äôs ability to achieve goals in a wide range of environments</p>
</blockquote>
<h3 id="various-kinds-of-intelligence">Various Kinds of Intelligence<a class="headerlink" href="#various-kinds-of-intelligence" title="Permanent link">¬∂</a></h3>
<p>While the above definition sound pretty general and obviously so. <strong>A goal</strong> can be anything‚Ää‚Äî‚Ääfrom moving from place A to place B‚Ää‚Äî‚Ääto‚Ää‚Äî‚Äärecognizing cat breeds from dog breeds‚Ää‚Äî‚Ääto‚Ää‚Äî‚Ääplanning a birthday party.</p>
<p>While most earlier intelligence tests only tested for cognitive intelligence, for artificial intelligence- we have another equally important field of work to develop, often dealt in robotics, called Physical Intelligence. One can easily remark that physical intelligence can, in fact, be broken into a sequence of decisions combined with motor abilities for a machine.</p>
<p>All right, let‚Äôs take another example to mark the distinction- when we touch a hot saucepan we instantly pull our hand away, what kind of intelligence would you call it since it isn‚Äôt a goal-oriented intelligence? Maybe consciousness?! But then how do you define consciousness? Unfortunately given our own limited understanding of human cognition, defining it as consciousness is likely to send us spiraling down a never-ending <em>black hole</em> of what it means for something to be conscious or to have free will. Big, important terms. Poorly understood. But beyond the scope of this article.</p>
<p>So, for the sake of simplicity, let us call it physical intelligence i.e. our tendency to protect ourselves. <strong>Bodies</strong> by pulling our hand back or reproducing‚Ää‚Äî‚Ää<strong>minds</strong> by saying I give up when the going gets hard‚Ää‚Äî‚Ää<strong>emotions</strong> by distancing ourselves from the source of pain.</p>
<p>But, neither kind, cognitive or physical intelligence has yet been fully developed in only but biological organisms, and in varying degrees.</p>
<p>Coming to the next term in that definition, <strong>the environment</strong>. It‚Äôs obvious that goals can only be measured in a particular environment. Environments, thus, act as the deterministic bounds for these goals.</p>
<p>One of the most common domains where we explicitly seek, use and create environments is ‚ÄúGames‚Äù thus games become an excellent training ground for AI.</p>
<h3 id="single-player-games">Single Player¬†Games<a class="headerlink" href="#single-player-games" title="Permanent link">¬∂</a></h3>
<p>Some of the key environments for AI training today are single agent games where A.I. agents accomplish a per-determined goal for a reward (an actual reward or simply higher accuracy).</p>
<p><a href="https://gym.openai.com/">Open AI‚Äôs Gym</a> or <a href="http://www.mujoco.org/">MuJoCo</a> is an excellent representation of many such environments where you can train your algorithm to perform for high accuracy on these games. There are some other open source implementations of these environments, for example, <a href="https://github.com/araffin/rl-baselines-zoo">RL-Baselines-Zoo</a> or <a href="https://openai.com/blog/roboschool/">RoboSchool</a> or <a href="https://github.com/benelot/pybullet-gym">PyBullet Gym</a>.</p>
<p>Though, using A.I. in Games isn‚Äôt a new phenomenon caused by the A.I. <strong>outbreak</strong> (<em>thanks Alex Net!</em>). A.I. has been long explored in the game domain. Among the key uses and implementations for A.I. in games earlier is developing new terrains, environments (esp. in rogue games) or defining the behaviors of NPCs (Non-Player Characters). Today, people in the game industry have now taken it a notch further by using AI to develop new game engines like <a href="https://twitter.com/angelinasgames">Angelina</a>.</p>
<h3 id="key-algorithms-for-ai-in-single-player-games">Key Algorithms for AI in Single Player¬†Games<a class="headerlink" href="#key-algorithms-for-ai-in-single-player-games" title="Permanent link">¬∂</a></h3>
<p>While it would be beyond the scope of this article to define all the algorithms in detail, some excellent resources on this topic are <a href="https://www.amazon.com/Artificial-Intelligence-Games-Ian-Millington/dp/0123747317">Ian Millington‚Äôs Book on Artificial Intelligence for Games</a> and <a href="https://www.amazon.com/Artificial-Intelligence-Games-Georgios-Yannakakis/dp/3319635182">Julian Togelius‚Äô book on Artificial Intelligence and Games</a>.</p>
<p>On the big picture level, these algorithms can be classified into three broad categories-</p>
<ol>
<li>Movement Algorithms: This includes some of the most commonly known path-finding algorithms including Dijkstra or A* or Near-Optimal Hierarchical Path-finding (HPA) etc.</li>
<li>Decision Making Algorithms: These include, but aren‚Äôt limited to, decision trees, finite state machines (FSM), behavior trees, fuzzy state machines and Markov Systems.</li>
</ol>
<p>The third category is the advanced State of the Art A.I. algorithms that we aren‚Äôt often seen in typical gaming industry implementations, though there are some exceptions we will discuss later.</p>
<p>Some of these key unexploited algorithms include-</p>
<ul>
<li>Monte Carlo Tree Search (MCTS)</li>
<li>Evolutionary Algorithms (Random-Key Genetic Algorithms, Differential Evolution Algorithms etc)</li>
<li>Deep Reinforcement Learning‚Ää‚Äî‚ÄäPolicy Gradient (PPO/TRPO), Q-Learning(DQN/HER) or Mixed Policy Optimization (DDPG/SAC)</li>
</ul>
<p>However, mainstream tech companies and academic labs for AI and Robotics, have been developing, exploiting and extending various AI algorithms and implementing them on simpler custom-built environments.</p>
<p>However, one of the key limitations of using single-player games to model and develop A.I. in single-agent environments is the inability of these trained agents to achieve goals in different multi-agent environments, thus refuting Shane and Hutter‚Äôs definition of intelligence.</p>
<blockquote>
<p>Intelligence measures an agent‚Äôs ability to achieve goals in a wide range of environments.</p>
</blockquote>
<h3 id="multiplayer-games">Multiplayer Games<a class="headerlink" href="#multiplayer-games" title="Permanent link">¬∂</a></h3>
<p>Some of the most prominent and recent success of our A.I. advancement can be captured by their performance on multiplayer games including <a href="https://openai.com/five/">Dota2</a> and <a href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/">StarCraft</a>.</p>
<p>This kind of environments often extend the capabilities of a single-player-environment-agent (SEA) to a multiplayer-environment-agent (MEA) by adding something extra to the mix. This extra is the tactical and strategic A.I. that includes way-point tactics, coordinated action and learning.</p>
<p>Multi-player games come in many flavors‚Ää‚Äî‚Ääsmall (Warframe), huge (Super Mario) or Open World Games (Sims 4, The Elder Scrolls V: Skyrim) and thus present a huge play-field for the research and development for A.I. algorithms.</p>
<h3 id="the-world-is-a-simulation">The World is a Simulation<a class="headerlink" href="#the-world-is-a-simulation" title="Permanent link">¬∂</a></h3>
<p>While potentially a contrarian view, the key fascination for A.I. researchers in games stems from the ability to closely simulate the real world to understand and imitate human behaviors and actions.</p>
<p>But isn‚Äôt the goal to create machine learning algorithms that could compete and (or) cooperate with human beings? Yes, precisely.</p>
<p>While playing a game, you are inside a carefully simulated world where your actions and reactions have been pre-planned by the game designer and are hard-coded in advance. Thus, one could argue that most games are boxes of complete (and also constantly evolving for open-world games) information. Thus, it‚Äôs fair to say that if you can learn the rules you could win the game. However, the real world ain‚Äôt much different.</p>
<p>For years, the so-called capitalist companies have been studying human behaviors to influence human decisions. Companies like Apple and Zara use several variations of warehouses to study the effect of the music playing in their stores to the lighting and positioning of items in the store. Researchers from various backgrounds (business, psychology, economics, politics, finance, journalism‚Ää‚Äî‚Ääyou name it) have for years studied how predictable humans are‚Ää‚Äî‚Ääincluding Jonah Berger, Dan &amp; Chip Heath, Fiery Cushman, Dan Ariely, Charles Duhigg, Daniel Kahneman, Richard H. Thaler, Daniel M. Oppenheimer.</p>
<p>In fact, some of the most fascinating work in the field of complex human patterns can be contributed to the man who shocked the world, Stanley Milgram, who devised a simple Obedience experiment to answer the following question about Nazi Germany and Holocaust:</p>
<blockquote>
<p>Could it be that Eichmann and his million accomplices in the Holocaust were just following orders? Could we call them all merely accomplices?‚Äù</p>
</blockquote>
<p>To someone who has never thought of his and other humans‚Äô behaviours as predictable, manipulable patterns, questions like these might seem like philosophical questions. However, based on Milgram‚Äôs experiment, he reached a conclusive answer in his article <a href="https://is.muni.cz/el/1423/podzim2013/PSY268/um/43422262/Milgram_-_perils_of_obediance.pdf">Perils of Obedience</a></p>
<blockquote>
<p>Ordinary people are likely to follow orders given by an authority figure, even to the extent of killing an innocent human being. Obedience to authority is ingrained in us all from the way we are brought up. People tend to obey orders from other people if they recognize their authority as morally right and/or legally based.</p>
</blockquote>
<p>This led him to the development of Milgram‚Äôs Agency Theory. Milgram‚Äôs Agency Theory suggests humans have two mental states:</p>
<ol>
<li>Autonomous: In the Autonomous State we perceive ourselves to be responsible for our own behaviour so we feel guilty for what we do</li>
<li>Agentic: In the Agentic State we perceive ourselves to be the agent of someone else‚Äôs will; the authority figure commanding us is responsible for what we do so we feel no guilt.</li>
</ol>
<p>As such, humans are predictable beings who live in a carefully planned predictable open-world. According to <a href="https://royalsocietypublishing.org/doi/10.1098/rsif.2018.0395">Seanoe et. al.</a>, the world we live in can also be interpreted as a biological (and, technological) evolution strongly tied to the generative potential tied through combinatorics that allows the system to grow and expand their available state spaces. Thus, many complex systems that presumably display Open-Ended Evolution, from language to proteins, share a common statistical property: the presence of <a href="https://www.wikiwand.com/en/Zipf%27s_law">Zipf‚Äôs Law</a>.</p>
<p>Though, according to another <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029796">study</a> conducted by Thurner et. al. on human behavioural sequences in an online world, it suggested that on a collective societal level the time-series of particular actions per day can be understood by a simple mean-reverting log-normal model which explains the rarity of absolute autonomy.</p>
<p>To conclude, it won‚Äôt be wrong to say that we are merely agents with the two agencies (autonomous and agentic) in a multi-agent world.</p>
<p>There are two essential ingredients when simulating human-like multi-agent societies-</p>
<ol>
<li>EvolutionThe environment or agent should be open-ended and constantly evolving.</li>
<li>CreativityThe idea of seeing <a href="https://www.researchgate.net/profile/Margaret_Boden/publication/209436199_Creativity_in_a_nutshell/links/5424477c0cf26120b7a732d4/Creativity-in-a-nutshell.pdf">creativity as a search</a> in a space of potential search-space is not new; it has been discussed at length by, for example, the British philosopher Margaret Boden. In fact, J. Schmidhuber has also worked tirelessly to capture the idea of curiosity and creativity to devise a <a href="http://people.idsia.ch/~juergen/creativity.html">Formal Theory of Creativity</a></li>
</ol>
<h3 id="but-training-ai-on-games-isnt-a-good-idea-in-the-long-run">But training AI on Games isn‚Äôt a good idea in the long¬†run<a class="headerlink" href="#but-training-ai-on-games-isnt-a-good-idea-in-the-long-run" title="Permanent link">¬∂</a></h3>
<p>Let us substitute the word ‚Äúgame‚Äù for the environment especially since we have established that the two mean the same in an agents‚Äô world.</p>
<p>Now, the games or training environments for AI can be broadly classified into two categories which can be further divided into two sub-categories-</p>
<ol>
<li>Static Environments</li>
<li>Dynamic Environments
    - Dynamic Determinate Environments
    - Dynamic Indeterminate Environments</li>
</ol>
<p>While training and developing AI algorithms to perform well on static environments is a near-finish goal however developing A.I. that can achieve goals in dynamic environments is an area of research where we quickly hit various roadblocks, esp in dynamic indeterminate environments. In my opinion, there are three key contributors to the same-</p>
<p>First would be our inadequate understanding of complexity theory and chaos theory. One of the most interesting efforts that is being pursued in this direction is <a href="https://arxiv.org/abs/1803.05049">Fractal AI</a>.</p>
<p>Second is the lack of models. Most of our current AI models are data-hungry beasts that need huge computational resources. Some of the most interesting works in this direction pursued by the Game AI community are <a href="http://www.possibilityspace.org/papers/iccc19.pdf">Framing for Computational Creativity</a> and by the causal inference community <a href="https://twitter.com/yudapearl">The Book of Why</a></p>
<p>The third is the problem of learning that includes transfer learning and lifelong learning. Some of the interesting works in this direction are <a href="https://www.automl.org/">AutoML Research</a>, <a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html">Meta-Learning</a> and Differential Evolution (check <a href="https://github.com/uber-research/differentiable-plasticity">Differential Plasticity</a>)</p>
<h3 id="open-endedness">Open-Endedness<a class="headerlink" href="#open-endedness" title="Permanent link">¬∂</a></h3>
<p>One of the classical properties of Dynamic Indeterminate Spaces is their <a href="https://www.oreilly.com/ideas/open-endedness-the-last-grand-challenge-youve-never-heard-of">open-endedness</a>. Open-ended problems are hard to define and model thus are often abandoned due to the lack of definite metrics.</p>
<p>Another way of looking at this problem is through the lens of algorithmic complexity namely <a href="https://www.wikiwand.com/en/Kolmogorov_complexity">Kolmogorov Complexity</a>, <a href="https://www.wikiwand.com/en/Attribute_substitution">Attribute Substitution</a>, <a href="https://www.wikiwand.com/en/Occam%27s_razora">Occam‚Äôs Razor</a> and <a href="http://people.idsia.ch/~juergen/mljssalevin/node5.html">Adaptive Levin Search</a>.</p>
<p>However, a few researchers who dabbled into this space have created quite a perspective shift. One such work is the paper <a href="http://eplex.cs.ucf.edu/papers/lehman_alife08.pdf">Exploiting Open-Endedness to Solve Problems Through the Search for Novelty</a>. Later, another excellent work in novelty-search which a few years later led to the development of <a href="https://www.frontiersin.org/articles/10.3389/frobt.2016.00040/full">quality-diversity algorithms</a>.</p>
<p>According to one of the first <a href="https://www.frontiersin.org/articles/10.3389/frobt.2016.00040/full">papers</a> in Quality diversity (QDA) algorithms by Kenneth Stanley et. al., QDAs is a kind of learning algorithm (for example, novelty search with local competition and <a href="http://jeffclune.com/publications/2019-06-20-ReWork-POET.pdf">MAP-Elites</a>) that try to balance the great dilemma in machine learning: <strong>exploration vs. exploitation</strong>. They aim to optimize for both quality and diversity simultaneously while aiming to fill a space of possibilities with the best possible example of each type of achievable behaviour. The result is this new class of algorithms that return an archive of diverse, high-quality behaviours in a single run.</p>
<p>Though, because of the digital nature of inheritance, there are inherent limits on the kinds of questions that can be answered using such an approach. In particular, according to an excellent paper by <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3284140/">Troy Day</a>, even in extremely simple evolutionary systems, a complete theory accounting for the potential open-endedness of evolution is unattainable unless evolution is progressive.</p>
<p>However, in 1936, Turing adapted Godel‚Äôs Theory of Open-Endedness (also called Incompleteness) to computability to show that a general algorithm to solve the <a href="https://www.wikiwand.com/en/Halting_problem">halting problem</a> for all possible program-input pairs cannot exist.</p>
<p>One of the rebuttals of Godel‚Äôs open-endedness problem has been presented in <a href="https://www.semanticscholar.org/paper/Monads-and-Sets%3A-On-G%C3%B6del%2C-Leibniz%2C-and-the-Atten/1bc646d1c6b0494f8b65477f0dd4e36e54253948">Monads and Sets: On G√∂del, Leibniz, and the Reflection Principle</a>. In complexity theory, it can also be interpreted as the Reflection Principle applied to the <a href="https://www.wikiwand.com/en/Reflection_principle_%28Wiener_process%29">Wiener Process</a>.</p>
<p>So, if we were to agree with Godel and his theory of open-endedness, then how do we develop progressive environments thus leading to progressive search spaces and thus hopefully progressive quality-diversity search algorithms?</p>
<h3 id="introducing-pcg">Introducing PCG<a class="headerlink" href="#introducing-pcg" title="Permanent link">¬∂</a></h3>
<p>Procedural content generation (PCG) is the programmatic generation of game content using a random or pseudo-random process that results in an unpredictable range of possible gameplay spaces.</p>
<p>But why games again? Didn‚Äôt we just establish that games are inadequate training environments?</p>
<p>Games make it easy to comparatively test and measure for novelty, delight and desirability in the newly created content.</p>
<p>Some of the interesting applications of procedural content generation for training A.I. agents have been seen in the games like <a href="https://arxiv.org/abs/1902.01378">Obstacle Tower</a>, <a href="https://deepmind.com/blog/capture-the-flag-science/">Capture the Flag</a> etc. Though it would be interesting to see A.I. agents trained on games like The Shadow of Mordor, a game where NPCs remember their encounters with you and refer back to them in future fights to more accurately represent the real world.</p>
<p>Thus, PCG presents huge opportunities for the field of A.I. in general. However, one of the big problems would be while games present excellent training environments for training our A.I. agents in decision making for competition and cooperation. But one of the biggest challenges for these advanced A.I. algorithms would be coordination with humans due to the differences in evolution.</p>
<p>Despite our common biological roots and parallel evolutionary process, it has taken animals and humans only 200,000 years to adapt and evolve alongside each other.</p>
<p>Through these thousands of years of shared evolution, many animal species esp cats and dogs have become inseparable from human societies, not only terrestrially but also emotionally‚Ää‚Äî‚Ääa magnificent feat especially given the lack of a common language of expression and entirely different cognitive and physical abilities.</p>
<h3 id="the-way-ahead-mixed-initiative-games">The Way Ahead: Mixed-Initiative Games<a class="headerlink" href="#the-way-ahead-mixed-initiative-games" title="Permanent link">¬∂</a></h3>
<p>This suggests that we could build A.I. systems where A.I. algorithms can collaborate with humans such that the AI can learn, provide suggestions and guess the intent of the human user to help him achieve common goals.</p>
<p>Some interesting works in this direction are <a href="https://link.springer.com/chapter/10.1007/978-3-319-42716-4_11">Mixed Initiative Game Design Platform Tangara</a>, <a href="https://deepmind.com/blog/capture-the-flag-science/">Quake III Arena</a>, <a href="http://www.sentientsketchbook.com/">Sentinent Sketchbook</a> which is used by AI to second guess the designer‚Äôs intention, <a href="http://www.cse.msu.edu/~jchai/">Collaborative Language Grounding with Robots</a>, <a href="https://slideslive.com/38916832/beyond-demonstrations-learning-behavior-from-higherlevel-supervision">Hierarchical Imitation</a> and <a href="https://www.media.mit.edu/publications/intrinsic-social-motivation-via-causal-influence-in-multi-agent-rl/">Intrinsic Social Motivation via Causal Influence in Multi-Agent Reinforcement Learning</a></p>
<p><sup> * Acknowledgements: I would like to thank <a href="https://twitter.com/iandanforth">Ian Danforth</a> for proofreading the article and providing useful feedback.</sup></p>
<hr/>
<div class="md-source-file">
<small>
    
      Last update:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">January 6, 2023</span>
<br/>
        Created:
        <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">January 6, 2023</span>
</small>
</div>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<nav aria-label="Footer" class="md-footer__inner md-grid">
<a aria-label="Previous: Why TensorLayer?" class="md-footer__link md-footer__link--prev" href="../../../ml-engineering/posts/2019-05-10-why-tensorlayer/" rel="prev">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</div>
<div class="md-footer__title">
<div class="md-ellipsis">
<span class="md-footer__direction">
                Previous
              </span>
              Why TensorLayer?
            </div>
</div>
</a>
<a aria-label="Next: Dense Associative Memory for Pattern Recognition" class="md-footer__link md-footer__link--next" href="../2017-09-10-associative-memory/" rel="next">
<div class="md-footer__title">
<div class="md-ellipsis">
<span class="md-footer__direction">
                Next
              </span>
              Dense Associative Memory for Pattern Recognition
            </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
<div class="md-copyright__highlight">
      Copyright ¬© 2022 Abi Aryan
    </div>
</div>
<div class="md-social">
<a class="md-social__link" href="https://masto.ai/@abi" rel="noopener" target="_blank" title="masto.ai">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.54 102.54 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5zm-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"></path></svg>
</a>
<a class="md-social__link" href="https://twitter.com/goabiaryan" rel="noopener" target="_blank" title="twitter.com">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg>
</a>
<a class="md-social__link" href="https://www.linkedin.com/in/goabiaryan/" rel="noopener" target="_blank" title="www.linkedin.com">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg>
</a>
<a class="md-social__link" href="mailto:&lt;abi@abiaryan.com&gt;" rel="noopener" target="_blank" title="">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6l-126.3-52.5-40.1 74.5c-5.2 9.7-16.3 14.6-27 11.9S192 499 192 488v-96c0-5.3 1.8-10.5 5.1-14.7l165.3-212.6c2.5-7.1-6.5-14.3-13-8.4l-179 161.9-32 28.9c-9.2 8.3-22.3 10.6-33.8 5.8l-85-35.4C8.4 312.8.8 302.2.1 290s5.5-23.7 16.1-29.8l448-256c10.7-6.1 23.9-5.5 34 1.4z"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.sections", "navigation.tabs"], "search": "../../../assets/javascripts/workers/search.5bf1dace.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
<script src="../../../assets/javascripts/bundle.078830c0.min.js"></script>
<script src="../../../javascripts/mathjax.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
<script src="../../../javascripts/tablesort.js"></script>
<script src="../../../js/image-carousel.js"></script>
<script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom"});})</script></body>
</html>