{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"My name is Abi. I am a Computer Scientist working extensively in machine learning to make the software systems smarter. Over the past six years, my focus has been building machine learning models for various applications including recommendation systems, voice-tech, geopositional apps etc. Prior to that, I also attended Insight in Toronto as a Data Science Fellow and was a Visiting Research Scholar at UCLA under Dr. Judea Pearl where I worked in AutoML, MultiAgent Systems and Emotion Recognition. At school, I majored in mathematics with minors in statistics and computer science with some finance and mangement courses loosely sprinkled in. I am deeply fascinated by deep tech and its applications in various industries. To that end, I have been lucky enough to be a VC Fellow with Laconia Capital. deepening my expertise in private-equity, fund-raising and due-diligence for AI/ML startups. I am also passionate about communities and helping self-taught engineers get into technical careers. In my free time, you would find me working behind the scenes obsessing over content strategy and analytics at the MLOps Community . I do also occassionally appear as a co-host on some of the podcast episodes - talking about Foundational models with Alex Ratner, Founder and CEO of Snorkel , Managing Machine Learning Projects with Simon Thompson, Head of Data Science, GFT Technologies , HCI for MLOps Infra Companies with Murtuza Shergadwala, Senior Data Scientist, Fidder.ai , What is MLOps with Niklas K\u00fchl, Managing Consultant for Data Science at IBM and many more. On another note , I am also working on some self-study courses that I wish were there when I was starting out. As someone who has taught python and machine learning at over 40 events for Women Who Code, WiMLDS and PyData conferences, I have often felt that software engineers tend to make simple things un-necessarily hard with unintuitive terminologies. As a self-taught programmer, it took me years to figure the best frameworks to learn anything quickly and build a unique learning style that spoke to my dopamine-hungry brain. Tying technical concepts with fun story-telling and intuitive analogies, I am now slowly trying to package my hard-learnt lessons and learnings into courses that would allow more people to have the power to make cool things to solve real-world problems by breaking down initimidating concepts into simple, easy-to-follow and obvious explanations. I am currently working on an MLOps Course: Deploying ML Models in Production with an edu-tech platform. Note What is MLOps? I've asked this question to over 100 DS, DE, and MLEs out there. \ud83d\ude0c Conclusion: \ud83d\udd8a\ufe0fMLOps is a best-practices engineering framework that combines #ML, #DevOps, and #DataEngineering, to deploy and maintain ML systems in production reliably and efficiently. \ud83d\ude15 And why should you care? You shouldn't \ud83d\ude24 \u2049\ufe0f unless... \ud83e\udd14 \u2757\ufe0f you believe machine learning is going to revolutionize your industry and you want to be at the helm of that development. Plz explain \ud83e\udde0 1\ufe0f. Adopting MLOps practices gives you faster time-to-market for ML projects deployed at scale. 2. To ensure that ML models are consistent and all business requirements are met at scale; a logical, easy-to-follow policy for data and model management is essential \u2234 MLOps\ud83e\udd37 What do you need to know to get started? \ud83c\udfce\ufe0f Machine Learning Fundamentals of Software Engineering DevOps Data Engineering System Design .. you don't need to know all of it to get started \u23f3 BUT you need a combination of at least three of the five to be successful \ud83c\udf7b Here are some snippets from the Table of Contents: While a registration link isn't available until the full course material is ready for publishing , but if you are already on my substack, you'll be the first to know when it releases:) I am best reached via email . I\u2019m always open to interesting conversations and collaboration. I am, foremost, driven by the goal to create personalized humane-agents that can influence and alter human decision making, but subsconsciously, also the nagging desire to leave the world a slightly better place than I found it. A tad bit geeky and a gadget lover at heart, I am equally fascinated by large-scale engineering application design as well as low level computer hardware engineering including microprocessors, crafting compilers, integrated chip design etc. Interested in hearing about Data Infrastructure, ML Systems, and new startups/research papers/tools in the MLOps space? \u00b6 I send out Data Driven Babe. Feel free to sub. No spamming. Just one letter a month. This month's newsletter will be about Building Reliable Data Pipelines.","title":"About"},{"location":"#interested-in-hearing-about-data-infrastructure-ml-systems-and-new-startupsresearch-paperstools-in-the-mlops-space","text":"I send out Data Driven Babe. Feel free to sub. No spamming. Just one letter a month. This month's newsletter will be about Building Reliable Data Pipelines.","title":"Interested in hearing about Data Infrastructure, ML Systems, and new startups/research papers/tools in the MLOps space?"},{"location":"tags/","text":"PCG \u00b6 Developing A.I. using Games annotated papers \u00b6 Dense Associative Memory for Pattern Recognition automl \u00b6 Problems and Challenges in AutoML best practices \u00b6 Best Engineering Practices for Data Scientists What's new in MLOps? content creation \u00b6 So you want to start a podcast? Why you shouldn't be a content creator? deep learning \u00b6 Dense Associative Memory for Pattern Recognition Developing A.I. using Games Why TensorLayer? fundraising \u00b6 How to Create a Winning Pitchdeck How to carry out due dilligence as an A.I. VC Fund? hopfield networks \u00b6 Dense Associative Memory for Pattern Recognition human behaviour \u00b6 Why you shouldn't be a content creator? large scale ML systems \u00b6 Why TensorLayer? machine learning \u00b6 Problems and Challenges in AutoML Best Engineering Practices for Data Scientists What's new in MLOps? meta-learning \u00b6 Problems and Challenges in AutoML multiagent systems \u00b6 Developing A.I. using Games neural architecture search \u00b6 Problems and Challenges in AutoML open source \u00b6 Why TensorLayer? podcasting \u00b6 So you want to start a podcast? programming \u00b6 Why Re-Learn Your Core Skills? software engineering \u00b6 Why Re-Learn Your Core Skills? Best Engineering Practices for Data Scientists What's new in MLOps? startup essays \u00b6 How to Create a Winning Pitchdeck How to carry out due dilligence as an A.I. VC Fund? venture capital \u00b6 How to Create a Winning Pitchdeck How to carry out due dilligence as an A.I. VC Fund?","title":"Sections"},{"location":"tags/#pcg","text":"Developing A.I. using Games","title":"PCG"},{"location":"tags/#annotated-papers","text":"Dense Associative Memory for Pattern Recognition","title":"annotated papers"},{"location":"tags/#automl","text":"Problems and Challenges in AutoML","title":"automl"},{"location":"tags/#best-practices","text":"Best Engineering Practices for Data Scientists What's new in MLOps?","title":"best practices"},{"location":"tags/#content-creation","text":"So you want to start a podcast? Why you shouldn't be a content creator?","title":"content creation"},{"location":"tags/#deep-learning","text":"Dense Associative Memory for Pattern Recognition Developing A.I. using Games Why TensorLayer?","title":"deep learning"},{"location":"tags/#fundraising","text":"How to Create a Winning Pitchdeck How to carry out due dilligence as an A.I. VC Fund?","title":"fundraising"},{"location":"tags/#hopfield-networks","text":"Dense Associative Memory for Pattern Recognition","title":"hopfield networks"},{"location":"tags/#human-behaviour","text":"Why you shouldn't be a content creator?","title":"human behaviour"},{"location":"tags/#large-scale-ml-systems","text":"Why TensorLayer?","title":"large scale ML systems"},{"location":"tags/#machine-learning","text":"Problems and Challenges in AutoML Best Engineering Practices for Data Scientists What's new in MLOps?","title":"machine learning"},{"location":"tags/#meta-learning","text":"Problems and Challenges in AutoML","title":"meta-learning"},{"location":"tags/#multiagent-systems","text":"Developing A.I. using Games","title":"multiagent systems"},{"location":"tags/#neural-architecture-search","text":"Problems and Challenges in AutoML","title":"neural architecture search"},{"location":"tags/#open-source","text":"Why TensorLayer?","title":"open source"},{"location":"tags/#podcasting","text":"So you want to start a podcast?","title":"podcasting"},{"location":"tags/#programming","text":"Why Re-Learn Your Core Skills?","title":"programming"},{"location":"tags/#software-engineering","text":"Why Re-Learn Your Core Skills? Best Engineering Practices for Data Scientists What's new in MLOps?","title":"software engineering"},{"location":"tags/#startup-essays","text":"How to Create a Winning Pitchdeck How to carry out due dilligence as an A.I. VC Fund?","title":"startup essays"},{"location":"tags/#venture-capital","text":"How to Create a Winning Pitchdeck How to carry out due dilligence as an A.I. VC Fund?","title":"venture capital"},{"location":"book/chapters/chapter-1/","text":"Learning to Code \u00b6","title":"Learning to Code"},{"location":"book/chapters/chapter-1/#learning-to-code","text":"","title":"Learning to Code"},{"location":"book/chapters/chapter-2/","text":"Software Engineering Basics \u00b6","title":"Software Engineering Basics"},{"location":"book/chapters/chapter-2/#software-engineering-basics","text":"","title":"Software Engineering Basics"},{"location":"book/chapters/preface/","text":"Who is this book for? \u00b6 Objective \u00b6 Learn how to take a machine learning model and deploy it in production from core development to deployment to monitoring and automation. Target audience \u00b6 Software engineers who are interested in a career as ML Engineer or Data Scientists who are looking to develop full-stack skills to productionize their ML models.","title":"Who is this book for?"},{"location":"book/chapters/preface/#who-is-this-book-for","text":"","title":"Who is this book for?"},{"location":"book/chapters/preface/#objective","text":"Learn how to take a machine learning model and deploy it in production from core development to deployment to monitoring and automation.","title":"Objective"},{"location":"book/chapters/preface/#target-audience","text":"Software engineers who are interested in a career as ML Engineer or Data Scientists who are looking to develop full-stack skills to productionize their ML models.","title":"Target audience"},{"location":"content-creation/posts/2022-10-12-how-to-podcast/","tags":["content creation","podcasting"],"text":"October 21, 2022 DRAFT Don't.","title":"So you want to start a podcast?"},{"location":"content-creation/posts/content-creation-stalking/","tags":["content creation","human behaviour"],"text":"October 21, 2022 DRAFT Humans have always generated content for various purposes. Sometimes education, sometimes entertainment and other times purely a self-indulgent creative outlet. But, never before YouTube has creating and distributing content as an individual publisher been so easy. It is this ease that baits several professions out of their nine-to-five jobs and into a new role called \"Youtuber\" or \"TikToker\" or \"Twitch Streamer\". My fascination with content has been as long as my existence as a way to educate, inform, and a medium to influence and transform people and culture at scale. Until one day, stalkers show up on your door and it gets scary.","title":"Why you shouldn't be a content creator?"},{"location":"fun-zone/posts/bucket-list/","text":"Bucket List \u00b6 Books I have read and I want to - - [x] The Motivation Myth - [ ] Creativity, Inc.","title":"Bucket list"},{"location":"fun-zone/posts/bucket-list/#bucket-list","text":"Books I have read and I want to - - [x] The Motivation Myth - [ ] Creativity, Inc.","title":"Bucket List"},{"location":"fun-zone/posts/news/","text":"News \u00b6 2022 (yet to update) \u00b6 September: Reviewer for PyData New York 2021 \u00b6 March: Delivered a workshop on Full Stack Deep Learning at Women Who Code Los Angeles 2020 \u00b6 August: Presented a technical workshop on Natural Language Processing (NLP) Hands On! at MLNerdie Los Angeles Our paper Arena: A General Evaluation Platform and Building Toolkit for Multi-Agent Intelligence was accepted for poster presentation at AAAI 2020 2019 \u00b6 Awarded full scholarship to attend International Conference on Machine Learning (ICML) 2019 , Long Beach, CA 2018 \u00b6 Delivered a workshop on A Hands-On Application of Causal Methods in Python at PyData Los Angeles Presented a talk on Big Problems at the Heart of Machine Learning at PyData Los Angeles Served as Committee Co-chair for PyData Los Angeles","title":"News"},{"location":"fun-zone/posts/news/#news","text":"","title":"News"},{"location":"fun-zone/posts/news/#2022-yet-to-update","text":"September: Reviewer for PyData New York","title":"2022 (yet to update)"},{"location":"fun-zone/posts/news/#2021","text":"March: Delivered a workshop on Full Stack Deep Learning at Women Who Code Los Angeles","title":"2021"},{"location":"fun-zone/posts/news/#2020","text":"August: Presented a technical workshop on Natural Language Processing (NLP) Hands On! at MLNerdie Los Angeles Our paper Arena: A General Evaluation Platform and Building Toolkit for Multi-Agent Intelligence was accepted for poster presentation at AAAI 2020","title":"2020"},{"location":"fun-zone/posts/news/#2019","text":"Awarded full scholarship to attend International Conference on Machine Learning (ICML) 2019 , Long Beach, CA","title":"2019"},{"location":"fun-zone/posts/news/#2018","text":"Delivered a workshop on A Hands-On Application of Causal Methods in Python at PyData Los Angeles Presented a talk on Big Problems at the Heart of Machine Learning at PyData Los Angeles Served as Committee Co-chair for PyData Los Angeles","title":"2018"},{"location":"fun-zone/posts/re-learn/","tags":["programming","software engineering"],"text":"December 11, 2022 DRAFT I was just talking to a friend who takes time every few years to Learn, Unlearn & Relearn their fundamentals and how it has helped them massively in their career. So,I am starting #relearn365 to commit to unlearn and re-learn my core skills.Plus, it will be nice to sort of have a written record of what I learnt and look back at the progress an year from now. If I could go back and do one thing right, it would be to not worry about learning everything right away but to build fun projects and learn along the way. Re-Learn Week 1: Building a Python Adventure Game \u00b6 12/12/2022 - building the basic program \u00b6","title":"Why Re-Learn Your Core Skills?"},{"location":"fun-zone/posts/re-learn/#re-learn-week-1-building-a-python-adventure-game","text":"","title":"Re-Learn Week 1: Building a Python Adventure Game"},{"location":"fun-zone/posts/re-learn/#12122022-building-the-basic-program","text":"","title":"12/12/2022 - building the basic program"},{"location":"fun-zone/posts/reading/","text":"\ud83d\udcc5 November 2022 \u00b6 I haven't had a lot of time to read this month. However, I've added three new books to my reading list. Dr Christian Busch - The Serendipity Mindset Michael J. Mauboussin - The Success Equation: Untangling Skill and Luck in Business, Sports, and Investing Richard Wiseman - The Luck Factor: The Scientific Study of the Lucky Mind-Arrow Recently, I was inspired to check out this book, When by Daniel Pink after learning about your time of the day chronotype. Here's my interepretation of the book- \ud83d\udcd1 WHEN by Daniel Pink \u00b6 It is normal to feel a mid-day slump sometime between 2 pm to 3 pm when nothing productive gets done for an average person who goes to sleep at 11 am and wakes up at 7 am It is, thus much, better to instead allow yourself the room to recharge during this time. There are multiple ways to recharge. However, two core ways to recharge would be. First, Taking a nappucino. Consuming a cappuchino and hitting a 25 minute nap so you wake up feeling that caffeine rush inside your body and more alert than before. Second, is to do organizing tasks that allow you to re-align your priorities for the rest of the day. This could be sorting, cleaning, updating stuff or running errands. Make sure you're following a step by step checklist to avoid making costly mistakes There are two kinds of problems we deal with through the day i.e. Logical Problems eg coding, research, editing and Insight Problems eg brainstorming, creative writing All of your jobs in today's day and time require us to wear multiple hats. Thus, you aren't always working on a single type of problems. But in any job, you are constantly working on both kinds of problems. For eg. for a content creator they have to be doing content research, filming, editing as well as content writing. While content research, filming and editing is best done in your first half of the day, content writing or anything that requires you to pause and reflect is best addressed towards the later half of the day. That said, it doesn't hold true for everyone. There are about 75% people who fall into this early chronotype but still a substantial 25% who fall into the late chronotype. The late chronotype people are able to accomplish the insight/creative work (writing, brainstorming, designing) right after waking up, and performing best on the logical work (researching, programming, reasoning) towards the later half of the day. I fall under the late chronotype where I find writing or learning something new almost a herculian task if I schedule it in the evening. However, coding in the evening is almost addictive, if not, incredibly relaxing for me but stressful thing to do if scheduled as my to-do right after waking up. As a content creator, I am also expected to film content and edit it. Filming content in the morning is easier for me as I tend not to overthink but editing it is much more peaceful when done in the evening as pausing feels more natural, a few hours before bed. That said, I don't always schedule things this way and I feel like different tasks may incite different emotions in people and it may not be as simple as that. For eg. as an OCD, I find coding pretty relaxing as an activity because it is partly repetitive in nature and requires you to pause and think. This may not be true for someone else who may find coding a very strenous and high-strung task. Also, what may seem like an artistic task, say writing, might not be artistic for a professional writer who may have come to develop their unique systems around it. So, my hypothesis is that the way we choose to classify a certain activity always does depends on our mastery of that task and that changes the game when you think of it with respect to the categories Daniel has defined in his book. Howeever, with that said, there is something to be taken away from the book, if not a lot and that is the awareness of the nature of our tasks, the need to batch similar tasks together so you aren't constantly context switching from one mode to another and also understanding our different capacities to handle chaos and uncertainity at different times during the day. Some people are able to do more uncertain tasks in the morning when their decision making abilities are still at peak leaving only more structured tasks for the evening whereas others prefer to accomplish more structured tasks in the morning where there is a clearly defined set of guidelines around what is expected and how to go about it.","title":"Books I am Reading"},{"location":"fun-zone/posts/reading/#november-2022","text":"I haven't had a lot of time to read this month. However, I've added three new books to my reading list. Dr Christian Busch - The Serendipity Mindset Michael J. Mauboussin - The Success Equation: Untangling Skill and Luck in Business, Sports, and Investing Richard Wiseman - The Luck Factor: The Scientific Study of the Lucky Mind-Arrow Recently, I was inspired to check out this book, When by Daniel Pink after learning about your time of the day chronotype. Here's my interepretation of the book-","title":"\ud83d\udcc5 November 2022"},{"location":"fun-zone/posts/reading/#when-by-daniel-pink","text":"It is normal to feel a mid-day slump sometime between 2 pm to 3 pm when nothing productive gets done for an average person who goes to sleep at 11 am and wakes up at 7 am It is, thus much, better to instead allow yourself the room to recharge during this time. There are multiple ways to recharge. However, two core ways to recharge would be. First, Taking a nappucino. Consuming a cappuchino and hitting a 25 minute nap so you wake up feeling that caffeine rush inside your body and more alert than before. Second, is to do organizing tasks that allow you to re-align your priorities for the rest of the day. This could be sorting, cleaning, updating stuff or running errands. Make sure you're following a step by step checklist to avoid making costly mistakes There are two kinds of problems we deal with through the day i.e. Logical Problems eg coding, research, editing and Insight Problems eg brainstorming, creative writing All of your jobs in today's day and time require us to wear multiple hats. Thus, you aren't always working on a single type of problems. But in any job, you are constantly working on both kinds of problems. For eg. for a content creator they have to be doing content research, filming, editing as well as content writing. While content research, filming and editing is best done in your first half of the day, content writing or anything that requires you to pause and reflect is best addressed towards the later half of the day. That said, it doesn't hold true for everyone. There are about 75% people who fall into this early chronotype but still a substantial 25% who fall into the late chronotype. The late chronotype people are able to accomplish the insight/creative work (writing, brainstorming, designing) right after waking up, and performing best on the logical work (researching, programming, reasoning) towards the later half of the day. I fall under the late chronotype where I find writing or learning something new almost a herculian task if I schedule it in the evening. However, coding in the evening is almost addictive, if not, incredibly relaxing for me but stressful thing to do if scheduled as my to-do right after waking up. As a content creator, I am also expected to film content and edit it. Filming content in the morning is easier for me as I tend not to overthink but editing it is much more peaceful when done in the evening as pausing feels more natural, a few hours before bed. That said, I don't always schedule things this way and I feel like different tasks may incite different emotions in people and it may not be as simple as that. For eg. as an OCD, I find coding pretty relaxing as an activity because it is partly repetitive in nature and requires you to pause and think. This may not be true for someone else who may find coding a very strenous and high-strung task. Also, what may seem like an artistic task, say writing, might not be artistic for a professional writer who may have come to develop their unique systems around it. So, my hypothesis is that the way we choose to classify a certain activity always does depends on our mastery of that task and that changes the game when you think of it with respect to the categories Daniel has defined in his book. Howeever, with that said, there is something to be taken away from the book, if not a lot and that is the awareness of the nature of our tasks, the need to batch similar tasks together so you aren't constantly context switching from one mode to another and also understanding our different capacities to handle chaos and uncertainity at different times during the day. Some people are able to do more uncertain tasks in the morning when their decision making abilities are still at peak leaving only more structured tasks for the evening whereas others prefer to accomplish more structured tasks in the morning where there is a clearly defined set of guidelines around what is expected and how to go about it.","title":"\ud83d\udcd1 WHEN by Daniel Pink"},{"location":"machine-learning/posts/2017-09-10-associative-memory/","tags":["hopfield networks","deep learning","annotated papers"],"text":"September 10, 2017 PUBLISHED This post is based on paper that explores the duality between Associative Memory and Feed-forward Neural Nets \u2014 two methods of deep learning. Dense Associative Memory for Pattern Recognition \u2014 joint work by Dmitry Krotov and John Hopfield. Presented at NIPS 2016, this paper is an effort to bring together these two distinct theories and be able to do pattern recognition more effectively. Let\u2019s use an analogy to explain what duality means With the advent of the nineteenth century, many physicists became enamoured by a strange fundamental question. Including them were \u2014 Albert Einstein, Thomas Young, Max Planck, Niels Bohr, Erwin Schr\u00f6dinger (yup, the man with the cat) and Werner Heisenberg \u2014 the geniuses who laid the foundation for Quantum Physics. The question was \u2014 \u201cHow does the light travel from one place to another?\u201d. A few experiments and some abstractions later, two complementary theories were proposed to explain how light behaves and how it transmits \u2014 The Particle Theory and The Wave Theory. However, the results observed suggested that neither of these theories managed to explain the complete behaviour of light \u2014 neither was fully right or was fully wrong. Light, to this day, exhibits this strange unexplained \u201cduality\u201d. For the sake of simplicity. we can roughly think of all the deep neural networks as distributed in two categories: Feed-Forward Networks (includes CNN, Auto-Encoders, GAN) and Cyclic Networks (includes RNN, Hopfield*, Markov Chain, Boltzmann Machine). Two goals of this paper: \u00b6 Study two models of pattern recognition \u2014 feature classification and prototype recognition. Establish a duality between Associative Memory Neural Networks and Feed Forward Neural Networks. Associative memory functions are represented by two major functions \u2014 an energy function and a dynamical update function (or state function that just like a gradient descent rule decreases the energy at every update). The idea of associative memory is that local minima of the energy function corresponds to some meaningful patterns of images and by going down the energy surface, one can recover the nearest local minimum which corresponds to some pattern from the training set. Generally associative memory network are represented by an energy function that looks something like this- and an update rule that looks something like this- where ( do not worry what these complex looking functions mean as of now \u2014 trust me they are not so complicated and aren\u2019t useful to build a general understanding of this paper ) These conventional associative nets with these functions can store approximately 0.14N memories. If one tries to store more patterns, several memories would converge together to create new patterns making the classification harder (there is even a term for that called Spurious Networks). However, we know that the size of the network and the number of memories stored hold a non-linear scaling relationship (proved in the paper). Thus, assuming this rule, we come up with a new class of associative models of higher order polynomial defined by the functions - and the update rule mentioned above. This new associative memory model was tested on XOR and it was found to be capable of solving the problem for higher order odd values of n (the interaction vertex of two or more nuerons). Then, they used another test case : MNIST (50000 training images, 10000 test images) and results proved that it works for n>=2 and interestingly the training time was faster than that in the case of ReLU on feed forward networks thus making it more efficient to train large training sets such as ImageNet. Additionally, this model has an exponentially increasing capacity that allows one to store N^(n-1) models (>0.14N) which can mean better and faster image classification due to higher number of stored patterns. how does the computation change as n varies? \u00b6 The authors selected 25 randomly selected memories and trained them using rectified polynomial energy function of orders n= 2,3,20, 30. So it looks like this - As we can see, as n increases the memories have transitioned from a set of features to a set of prototypes at approximately n=20. Depending on your particular case, it might be easier to classify an image based on features and other better as a prototype. For the sake of an example, the exact species of canine (Labrador vs. German Shepherd) be classified using prototype based classification and cats vs dogs can use a feature based classification (though it must be a bit harder than that given dogs do come in all sizes and forms). So far this has only been applied to simple models like MNIST, but in general this computational regime remains largely unexplored. Any extension outside of simple MNIST experiments would be very interesting to confirm the proposition. Now, it only makes sense to extend the this feature-prototype based classification to feed forward networks to see if feed forward networks too can benefit from such an application. So the authors made a few assumptions to deduce a relation between the two theories ( loosely like string theory in physics ) such that a computational model has two different descriptions : one in terms of associative memory and other in terms of a feedforward network - The feedforward network has only one hidden layer The associative memory model performs only a one step update The labels contain far less information that the data itself The classifications neurons of an associative model are all initialized in the state -\u03b5 Using the above assumptions, the authors were able to create a construction such that f(x) = F\u2019(x) Where f(x) is the activation function from the visible layer to the hidden layer in a feedforward network and F(x) is the energy function of associative memory model. The result suggests that the activation function of feedforward network is equal to the derivative of the energy function of the associative model i.e. for every small change in x, F(x) changes by f(x). Thus, for various n the activation and energy function are related such that - Where RePn represents rectified polynomial of order n. As we concluded previously, the speed of learning should increase as n increases given the higher number of stored patterns. Thus according to the table the ReLU function should be faster than tan hyperbolic function or logistic functions (used in 1980s and 90s) which is consistent with our knowledge of ReLU and thus we can also suggest that these rectified higher polynomial activation functions (ReP) might have even better computational properties than ReLU. Thus, at this point we can derive two things - ReP activation functions might have as good if not substantially better results than ReLU along with a few interesting attributes. These attributes can be inherited to FNN to probably utilize both prototype and feature classification methods into deep learning Some interesting future extensions of this work for readers to explore and experiment would be- \u00b6 Extension of this result for multilayer network architectures. Application of RePU activation functions for feedforward neural networks Application of the dense associative memory models (with RePU) to problems outside MNIST. Studying how regularization like weight decay, dropout etc affect the construction and duality. Limitations: The following results are only true when the stored patterns are stable.","title":"Dense Associative Memory for Pattern Recognition"},{"location":"machine-learning/posts/2017-09-10-associative-memory/#two-goals-of-this-paper","text":"Study two models of pattern recognition \u2014 feature classification and prototype recognition. Establish a duality between Associative Memory Neural Networks and Feed Forward Neural Networks. Associative memory functions are represented by two major functions \u2014 an energy function and a dynamical update function (or state function that just like a gradient descent rule decreases the energy at every update). The idea of associative memory is that local minima of the energy function corresponds to some meaningful patterns of images and by going down the energy surface, one can recover the nearest local minimum which corresponds to some pattern from the training set. Generally associative memory network are represented by an energy function that looks something like this- and an update rule that looks something like this- where ( do not worry what these complex looking functions mean as of now \u2014 trust me they are not so complicated and aren\u2019t useful to build a general understanding of this paper ) These conventional associative nets with these functions can store approximately 0.14N memories. If one tries to store more patterns, several memories would converge together to create new patterns making the classification harder (there is even a term for that called Spurious Networks). However, we know that the size of the network and the number of memories stored hold a non-linear scaling relationship (proved in the paper). Thus, assuming this rule, we come up with a new class of associative models of higher order polynomial defined by the functions - and the update rule mentioned above. This new associative memory model was tested on XOR and it was found to be capable of solving the problem for higher order odd values of n (the interaction vertex of two or more nuerons). Then, they used another test case : MNIST (50000 training images, 10000 test images) and results proved that it works for n>=2 and interestingly the training time was faster than that in the case of ReLU on feed forward networks thus making it more efficient to train large training sets such as ImageNet. Additionally, this model has an exponentially increasing capacity that allows one to store N^(n-1) models (>0.14N) which can mean better and faster image classification due to higher number of stored patterns.","title":"Two goals of this paper:"},{"location":"machine-learning/posts/2017-09-10-associative-memory/#how-does-the-computation-change-as-n-varies","text":"The authors selected 25 randomly selected memories and trained them using rectified polynomial energy function of orders n= 2,3,20, 30. So it looks like this - As we can see, as n increases the memories have transitioned from a set of features to a set of prototypes at approximately n=20. Depending on your particular case, it might be easier to classify an image based on features and other better as a prototype. For the sake of an example, the exact species of canine (Labrador vs. German Shepherd) be classified using prototype based classification and cats vs dogs can use a feature based classification (though it must be a bit harder than that given dogs do come in all sizes and forms). So far this has only been applied to simple models like MNIST, but in general this computational regime remains largely unexplored. Any extension outside of simple MNIST experiments would be very interesting to confirm the proposition. Now, it only makes sense to extend the this feature-prototype based classification to feed forward networks to see if feed forward networks too can benefit from such an application. So the authors made a few assumptions to deduce a relation between the two theories ( loosely like string theory in physics ) such that a computational model has two different descriptions : one in terms of associative memory and other in terms of a feedforward network - The feedforward network has only one hidden layer The associative memory model performs only a one step update The labels contain far less information that the data itself The classifications neurons of an associative model are all initialized in the state -\u03b5 Using the above assumptions, the authors were able to create a construction such that f(x) = F\u2019(x) Where f(x) is the activation function from the visible layer to the hidden layer in a feedforward network and F(x) is the energy function of associative memory model. The result suggests that the activation function of feedforward network is equal to the derivative of the energy function of the associative model i.e. for every small change in x, F(x) changes by f(x). Thus, for various n the activation and energy function are related such that - Where RePn represents rectified polynomial of order n. As we concluded previously, the speed of learning should increase as n increases given the higher number of stored patterns. Thus according to the table the ReLU function should be faster than tan hyperbolic function or logistic functions (used in 1980s and 90s) which is consistent with our knowledge of ReLU and thus we can also suggest that these rectified higher polynomial activation functions (ReP) might have even better computational properties than ReLU. Thus, at this point we can derive two things - ReP activation functions might have as good if not substantially better results than ReLU along with a few interesting attributes. These attributes can be inherited to FNN to probably utilize both prototype and feature classification methods into deep learning","title":"how does the computation change as n varies?"},{"location":"machine-learning/posts/2017-09-10-associative-memory/#some-interesting-future-extensions-of-this-work-for-readers-to-explore-and-experiment-would-be-","text":"Extension of this result for multilayer network architectures. Application of RePU activation functions for feedforward neural networks Application of the dense associative memory models (with RePU) to problems outside MNIST. Studying how regularization like weight decay, dropout etc affect the construction and duality. Limitations: The following results are only true when the stored patterns are stable.","title":"Some interesting future extensions of this work for readers to explore and experiment would be-"},{"location":"machine-learning/posts/2019-09-09-game-ai/","tags":["PCG","deep learning","multiagent systems"],"text":"September 9, 2019 PUBLISHED Intelligence is a highly abstract term that could mean several different things to several different people. In 2006, Shane Legg and Marcus Hutter decided to conduct a comprehensive literature review for the definition of intelligence in various disciplines. And thus, it paved the way for them to create a \u201ccomplete\u201d definition of intelligence finally presented in their 2007 paper Universal Intelligence: A Definition of Machine Intelligence . They defined it as - Intelligence measures an agent\u2019s ability to achieve goals in a wide range of environments Various Kinds of Intelligence \u00b6 While the above definition sound pretty general and obviously so. A goal can be anything\u200a\u2014\u200afrom moving from place A to place B\u200a\u2014\u200ato\u200a\u2014\u200arecognizing cat breeds from dog breeds\u200a\u2014\u200ato\u200a\u2014\u200aplanning a birthday party. While most earlier intelligence tests only tested for cognitive intelligence, for artificial intelligence- we have another equally important field of work to develop, often dealt in robotics, called Physical Intelligence. One can easily remark that physical intelligence can, in fact, be broken into a sequence of decisions combined with motor abilities for a machine. All right, let\u2019s take another example to mark the distinction- when we touch a hot saucepan we instantly pull our hand away, what kind of intelligence would you call it since it isn\u2019t a goal-oriented intelligence? Maybe consciousness?! But then how do you define consciousness? Unfortunately given our own limited understanding of human cognition, defining it as consciousness is likely to send us spiraling down a never-ending black hole of what it means for something to be conscious or to have free will. Big, important terms. Poorly understood. But beyond the scope of this article. So, for the sake of simplicity, let us call it physical intelligence i.e. our tendency to protect ourselves. Bodies by pulling our hand back or reproducing\u200a\u2014\u200a minds by saying I give up when the going gets hard\u200a\u2014\u200a emotions by distancing ourselves from the source of pain. But, neither kind, cognitive or physical intelligence has yet been fully developed in only but biological organisms, and in varying degrees. Coming to the next term in that definition, the environment . It\u2019s obvious that goals can only be measured in a particular environment. Environments, thus, act as the deterministic bounds for these goals. One of the most common domains where we explicitly seek, use and create environments is \u201cGames\u201d thus games become an excellent training ground for AI. Single Player Games \u00b6 Some of the key environments for AI training today are single agent games where A.I. agents accomplish a per-determined goal for a reward (an actual reward or simply higher accuracy). Open AI\u2019s Gym or MuJoCo is an excellent representation of many such environments where you can train your algorithm to perform for high accuracy on these games. There are some other open source implementations of these environments, for example, RL-Baselines-Zoo or RoboSchool or PyBullet Gym . Though, using A.I. in Games isn\u2019t a new phenomenon caused by the A.I. outbreak ( thanks Alex Net! ). A.I. has been long explored in the game domain. Among the key uses and implementations for A.I. in games earlier is developing new terrains, environments (esp. in rogue games) or defining the behaviors of NPCs (Non-Player Characters). Today, people in the game industry have now taken it a notch further by using AI to develop new game engines like Angelina . Key Algorithms for AI in Single Player Games \u00b6 While it would be beyond the scope of this article to define all the algorithms in detail, some excellent resources on this topic are Ian Millington\u2019s Book on Artificial Intelligence for Games and Julian Togelius\u2019 book on Artificial Intelligence and Games . On the big picture level, these algorithms can be classified into three broad categories- Movement Algorithms: This includes some of the most commonly known path-finding algorithms including Dijkstra or A* or Near-Optimal Hierarchical Path-finding (HPA) etc. Decision Making Algorithms: These include, but aren\u2019t limited to, decision trees, finite state machines (FSM), behavior trees, fuzzy state machines and Markov Systems. The third category is the advanced State of the Art A.I. algorithms that we aren\u2019t often seen in typical gaming industry implementations, though there are some exceptions we will discuss later. Some of these key unexploited algorithms include- Monte Carlo Tree Search (MCTS) Evolutionary Algorithms (Random-Key Genetic Algorithms, Differential Evolution Algorithms etc) Deep Reinforcement Learning\u200a\u2014\u200aPolicy Gradient (PPO/TRPO), Q-Learning(DQN/HER) or Mixed Policy Optimization (DDPG/SAC) However, mainstream tech companies and academic labs for AI and Robotics, have been developing, exploiting and extending various AI algorithms and implementing them on simpler custom-built environments. However, one of the key limitations of using single-player games to model and develop A.I. in single-agent environments is the inability of these trained agents to achieve goals in different multi-agent environments, thus refuting Shane and Hutter\u2019s definition of intelligence. Intelligence measures an agent\u2019s ability to achieve goals in a wide range of environments. Multiplayer Games \u00b6 Some of the most prominent and recent success of our A.I. advancement can be captured by their performance on multiplayer games including Dota2 and StarCraft . This kind of environments often extend the capabilities of a single-player-environment-agent (SEA) to a multiplayer-environment-agent (MEA) by adding something extra to the mix. This extra is the tactical and strategic A.I. that includes way-point tactics, coordinated action and learning. Multi-player games come in many flavors\u200a\u2014\u200asmall (Warframe), huge (Super Mario) or Open World Games (Sims 4, The Elder Scrolls V: Skyrim) and thus present a huge play-field for the research and development for A.I. algorithms. The World is a Simulation \u00b6 While potentially a contrarian view, the key fascination for A.I. researchers in games stems from the ability to closely simulate the real world to understand and imitate human behaviors and actions. But isn\u2019t the goal to create machine learning algorithms that could compete and (or) cooperate with human beings? Yes, precisely. While playing a game, you are inside a carefully simulated world where your actions and reactions have been pre-planned by the game designer and are hard-coded in advance. Thus, one could argue that most games are boxes of complete (and also constantly evolving for open-world games) information. Thus, it\u2019s fair to say that if you can learn the rules you could win the game. However, the real world ain\u2019t much different. For years, the so-called capitalist companies have been studying human behaviors to influence human decisions. Companies like Apple and Zara use several variations of warehouses to study the effect of the music playing in their stores to the lighting and positioning of items in the store. Researchers from various backgrounds (business, psychology, economics, politics, finance, journalism\u200a\u2014\u200ayou name it) have for years studied how predictable humans are\u200a\u2014\u200aincluding Jonah Berger, Dan & Chip Heath, Fiery Cushman, Dan Ariely, Charles Duhigg, Daniel Kahneman, Richard H. Thaler, Daniel M. Oppenheimer. In fact, some of the most fascinating work in the field of complex human patterns can be contributed to the man who shocked the world, Stanley Milgram, who devised a simple Obedience experiment to answer the following question about Nazi Germany and Holocaust: Could it be that Eichmann and his million accomplices in the Holocaust were just following orders? Could we call them all merely accomplices?\u201d To someone who has never thought of his and other humans\u2019 behaviours as predictable, manipulable patterns, questions like these might seem like philosophical questions. However, based on Milgram\u2019s experiment, he reached a conclusive answer in his article Perils of Obedience Ordinary people are likely to follow orders given by an authority figure, even to the extent of killing an innocent human being. Obedience to authority is ingrained in us all from the way we are brought up. People tend to obey orders from other people if they recognize their authority as morally right and/or legally based. This led him to the development of Milgram\u2019s Agency Theory. Milgram\u2019s Agency Theory suggests humans have two mental states: Autonomous: In the Autonomous State we perceive ourselves to be responsible for our own behaviour so we feel guilty for what we do Agentic: In the Agentic State we perceive ourselves to be the agent of someone else\u2019s will; the authority figure commanding us is responsible for what we do so we feel no guilt. As such, humans are predictable beings who live in a carefully planned predictable open-world. According to Seanoe et. al. , the world we live in can also be interpreted as a biological (and, technological) evolution strongly tied to the generative potential tied through combinatorics that allows the system to grow and expand their available state spaces. Thus, many complex systems that presumably display Open-Ended Evolution, from language to proteins, share a common statistical property: the presence of Zipf\u2019s Law . Though, according to another study conducted by Thurner et. al. on human behavioural sequences in an online world, it suggested that on a collective societal level the time-series of particular actions per day can be understood by a simple mean-reverting log-normal model which explains the rarity of absolute autonomy. To conclude, it won\u2019t be wrong to say that we are merely agents with the two agencies (autonomous and agentic) in a multi-agent world. There are two essential ingredients when simulating human-like multi-agent societies- EvolutionThe environment or agent should be open-ended and constantly evolving. CreativityThe idea of seeing creativity as a search in a space of potential search-space is not new; it has been discussed at length by, for example, the British philosopher Margaret Boden. In fact, J. Schmidhuber has also worked tirelessly to capture the idea of curiosity and creativity to devise a Formal Theory of Creativity But training AI on Games isn\u2019t a good idea in the long run \u00b6 Let us substitute the word \u201cgame\u201d for the environment especially since we have established that the two mean the same in an agents\u2019 world. Now, the games or training environments for AI can be broadly classified into two categories which can be further divided into two sub-categories- Static Environments Dynamic Environments - Dynamic Determinate Environments - Dynamic Indeterminate Environments While training and developing AI algorithms to perform well on static environments is a near-finish goal however developing A.I. that can achieve goals in dynamic environments is an area of research where we quickly hit various roadblocks, esp in dynamic indeterminate environments. In my opinion, there are three key contributors to the same- First would be our inadequate understanding of complexity theory and chaos theory. One of the most interesting efforts that is being pursued in this direction is Fractal AI . Second is the lack of models. Most of our current AI models are data-hungry beasts that need huge computational resources. Some of the most interesting works in this direction pursued by the Game AI community are Framing for Computational Creativity and by the causal inference community The Book of Why The third is the problem of learning that includes transfer learning and lifelong learning. Some of the interesting works in this direction are AutoML Research , Meta-Learning and Differential Evolution (check Differential Plasticity ) Open-Endedness \u00b6 One of the classical properties of Dynamic Indeterminate Spaces is their open-endedness . Open-ended problems are hard to define and model thus are often abandoned due to the lack of definite metrics. Another way of looking at this problem is through the lens of algorithmic complexity namely Kolmogorov Complexity , Attribute Substitution , Occam\u2019s Razor and Adaptive Levin Search . However, a few researchers who dabbled into this space have created quite a perspective shift. One such work is the paper Exploiting Open-Endedness to Solve Problems Through the Search for Novelty . Later, another excellent work in novelty-search which a few years later led to the development of quality-diversity algorithms . According to one of the first papers in Quality diversity (QDA) algorithms by Kenneth Stanley et. al., QDAs is a kind of learning algorithm (for example, novelty search with local competition and MAP-Elites ) that try to balance the great dilemma in machine learning: exploration vs. exploitation . They aim to optimize for both quality and diversity simultaneously while aiming to fill a space of possibilities with the best possible example of each type of achievable behaviour. The result is this new class of algorithms that return an archive of diverse, high-quality behaviours in a single run. Though, because of the digital nature of inheritance, there are inherent limits on the kinds of questions that can be answered using such an approach. In particular, according to an excellent paper by Troy Day , even in extremely simple evolutionary systems, a complete theory accounting for the potential open-endedness of evolution is unattainable unless evolution is progressive. However, in 1936, Turing adapted Godel\u2019s Theory of Open-Endedness (also called Incompleteness) to computability to show that a general algorithm to solve the halting problem for all possible program-input pairs cannot exist. One of the rebuttals of Godel\u2019s open-endedness problem has been presented in Monads and Sets: On G\u00f6del, Leibniz, and the Reflection Principle . In complexity theory, it can also be interpreted as the Reflection Principle applied to the Wiener Process . So, if we were to agree with Godel and his theory of open-endedness, then how do we develop progressive environments thus leading to progressive search spaces and thus hopefully progressive quality-diversity search algorithms? Introducing PCG \u00b6 Procedural content generation (PCG) is the programmatic generation of game content using a random or pseudo-random process that results in an unpredictable range of possible gameplay spaces. But why games again? Didn\u2019t we just establish that games are inadequate training environments? Games make it easy to comparatively test and measure for novelty, delight and desirability in the newly created content. Some of the interesting applications of procedural content generation for training A.I. agents have been seen in the games like Obstacle Tower , Capture the Flag etc. Though it would be interesting to see A.I. agents trained on games like The Shadow of Mordor, a game where NPCs remember their encounters with you and refer back to them in future fights to more accurately represent the real world. Thus, PCG presents huge opportunities for the field of A.I. in general. However, one of the big problems would be while games present excellent training environments for training our A.I. agents in decision making for competition and cooperation. But one of the biggest challenges for these advanced A.I. algorithms would be coordination with humans due to the differences in evolution. Despite our common biological roots and parallel evolutionary process, it has taken animals and humans only 200,000 years to adapt and evolve alongside each other. Through these thousands of years of shared evolution, many animal species esp cats and dogs have become inseparable from human societies, not only terrestrially but also emotionally\u200a\u2014\u200aa magnificent feat especially given the lack of a common language of expression and entirely different cognitive and physical abilities. The Way Ahead: Mixed-Initiative Games \u00b6 This suggests that we could build A.I. systems where A.I. algorithms can collaborate with humans such that the AI can learn, provide suggestions and guess the intent of the human user to help him achieve common goals. Some interesting works in this direction are Mixed Initiative Game Design Platform Tangara , Quake III Arena , Sentinent Sketchbook which is used by AI to second guess the designer\u2019s intention, Collaborative Language Grounding with Robots , Hierarchical Imitation and Intrinsic Social Motivation via Causal Influence in Multi-Agent Reinforcement Learning * Acknowledgements: I would like to thank Ian Danforth for proofreading the article and providing useful feedback.","title":"Developing A.I. using Games"},{"location":"machine-learning/posts/2019-09-09-game-ai/#various-kinds-of-intelligence","text":"While the above definition sound pretty general and obviously so. A goal can be anything\u200a\u2014\u200afrom moving from place A to place B\u200a\u2014\u200ato\u200a\u2014\u200arecognizing cat breeds from dog breeds\u200a\u2014\u200ato\u200a\u2014\u200aplanning a birthday party. While most earlier intelligence tests only tested for cognitive intelligence, for artificial intelligence- we have another equally important field of work to develop, often dealt in robotics, called Physical Intelligence. One can easily remark that physical intelligence can, in fact, be broken into a sequence of decisions combined with motor abilities for a machine. All right, let\u2019s take another example to mark the distinction- when we touch a hot saucepan we instantly pull our hand away, what kind of intelligence would you call it since it isn\u2019t a goal-oriented intelligence? Maybe consciousness?! But then how do you define consciousness? Unfortunately given our own limited understanding of human cognition, defining it as consciousness is likely to send us spiraling down a never-ending black hole of what it means for something to be conscious or to have free will. Big, important terms. Poorly understood. But beyond the scope of this article. So, for the sake of simplicity, let us call it physical intelligence i.e. our tendency to protect ourselves. Bodies by pulling our hand back or reproducing\u200a\u2014\u200a minds by saying I give up when the going gets hard\u200a\u2014\u200a emotions by distancing ourselves from the source of pain. But, neither kind, cognitive or physical intelligence has yet been fully developed in only but biological organisms, and in varying degrees. Coming to the next term in that definition, the environment . It\u2019s obvious that goals can only be measured in a particular environment. Environments, thus, act as the deterministic bounds for these goals. One of the most common domains where we explicitly seek, use and create environments is \u201cGames\u201d thus games become an excellent training ground for AI.","title":"Various Kinds of Intelligence"},{"location":"machine-learning/posts/2019-09-09-game-ai/#single-player-games","text":"Some of the key environments for AI training today are single agent games where A.I. agents accomplish a per-determined goal for a reward (an actual reward or simply higher accuracy). Open AI\u2019s Gym or MuJoCo is an excellent representation of many such environments where you can train your algorithm to perform for high accuracy on these games. There are some other open source implementations of these environments, for example, RL-Baselines-Zoo or RoboSchool or PyBullet Gym . Though, using A.I. in Games isn\u2019t a new phenomenon caused by the A.I. outbreak ( thanks Alex Net! ). A.I. has been long explored in the game domain. Among the key uses and implementations for A.I. in games earlier is developing new terrains, environments (esp. in rogue games) or defining the behaviors of NPCs (Non-Player Characters). Today, people in the game industry have now taken it a notch further by using AI to develop new game engines like Angelina .","title":"Single Player\u00a0Games"},{"location":"machine-learning/posts/2019-09-09-game-ai/#key-algorithms-for-ai-in-single-player-games","text":"While it would be beyond the scope of this article to define all the algorithms in detail, some excellent resources on this topic are Ian Millington\u2019s Book on Artificial Intelligence for Games and Julian Togelius\u2019 book on Artificial Intelligence and Games . On the big picture level, these algorithms can be classified into three broad categories- Movement Algorithms: This includes some of the most commonly known path-finding algorithms including Dijkstra or A* or Near-Optimal Hierarchical Path-finding (HPA) etc. Decision Making Algorithms: These include, but aren\u2019t limited to, decision trees, finite state machines (FSM), behavior trees, fuzzy state machines and Markov Systems. The third category is the advanced State of the Art A.I. algorithms that we aren\u2019t often seen in typical gaming industry implementations, though there are some exceptions we will discuss later. Some of these key unexploited algorithms include- Monte Carlo Tree Search (MCTS) Evolutionary Algorithms (Random-Key Genetic Algorithms, Differential Evolution Algorithms etc) Deep Reinforcement Learning\u200a\u2014\u200aPolicy Gradient (PPO/TRPO), Q-Learning(DQN/HER) or Mixed Policy Optimization (DDPG/SAC) However, mainstream tech companies and academic labs for AI and Robotics, have been developing, exploiting and extending various AI algorithms and implementing them on simpler custom-built environments. However, one of the key limitations of using single-player games to model and develop A.I. in single-agent environments is the inability of these trained agents to achieve goals in different multi-agent environments, thus refuting Shane and Hutter\u2019s definition of intelligence. Intelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.","title":"Key Algorithms for AI in Single Player\u00a0Games"},{"location":"machine-learning/posts/2019-09-09-game-ai/#multiplayer-games","text":"Some of the most prominent and recent success of our A.I. advancement can be captured by their performance on multiplayer games including Dota2 and StarCraft . This kind of environments often extend the capabilities of a single-player-environment-agent (SEA) to a multiplayer-environment-agent (MEA) by adding something extra to the mix. This extra is the tactical and strategic A.I. that includes way-point tactics, coordinated action and learning. Multi-player games come in many flavors\u200a\u2014\u200asmall (Warframe), huge (Super Mario) or Open World Games (Sims 4, The Elder Scrolls V: Skyrim) and thus present a huge play-field for the research and development for A.I. algorithms.","title":"Multiplayer Games"},{"location":"machine-learning/posts/2019-09-09-game-ai/#the-world-is-a-simulation","text":"While potentially a contrarian view, the key fascination for A.I. researchers in games stems from the ability to closely simulate the real world to understand and imitate human behaviors and actions. But isn\u2019t the goal to create machine learning algorithms that could compete and (or) cooperate with human beings? Yes, precisely. While playing a game, you are inside a carefully simulated world where your actions and reactions have been pre-planned by the game designer and are hard-coded in advance. Thus, one could argue that most games are boxes of complete (and also constantly evolving for open-world games) information. Thus, it\u2019s fair to say that if you can learn the rules you could win the game. However, the real world ain\u2019t much different. For years, the so-called capitalist companies have been studying human behaviors to influence human decisions. Companies like Apple and Zara use several variations of warehouses to study the effect of the music playing in their stores to the lighting and positioning of items in the store. Researchers from various backgrounds (business, psychology, economics, politics, finance, journalism\u200a\u2014\u200ayou name it) have for years studied how predictable humans are\u200a\u2014\u200aincluding Jonah Berger, Dan & Chip Heath, Fiery Cushman, Dan Ariely, Charles Duhigg, Daniel Kahneman, Richard H. Thaler, Daniel M. Oppenheimer. In fact, some of the most fascinating work in the field of complex human patterns can be contributed to the man who shocked the world, Stanley Milgram, who devised a simple Obedience experiment to answer the following question about Nazi Germany and Holocaust: Could it be that Eichmann and his million accomplices in the Holocaust were just following orders? Could we call them all merely accomplices?\u201d To someone who has never thought of his and other humans\u2019 behaviours as predictable, manipulable patterns, questions like these might seem like philosophical questions. However, based on Milgram\u2019s experiment, he reached a conclusive answer in his article Perils of Obedience Ordinary people are likely to follow orders given by an authority figure, even to the extent of killing an innocent human being. Obedience to authority is ingrained in us all from the way we are brought up. People tend to obey orders from other people if they recognize their authority as morally right and/or legally based. This led him to the development of Milgram\u2019s Agency Theory. Milgram\u2019s Agency Theory suggests humans have two mental states: Autonomous: In the Autonomous State we perceive ourselves to be responsible for our own behaviour so we feel guilty for what we do Agentic: In the Agentic State we perceive ourselves to be the agent of someone else\u2019s will; the authority figure commanding us is responsible for what we do so we feel no guilt. As such, humans are predictable beings who live in a carefully planned predictable open-world. According to Seanoe et. al. , the world we live in can also be interpreted as a biological (and, technological) evolution strongly tied to the generative potential tied through combinatorics that allows the system to grow and expand their available state spaces. Thus, many complex systems that presumably display Open-Ended Evolution, from language to proteins, share a common statistical property: the presence of Zipf\u2019s Law . Though, according to another study conducted by Thurner et. al. on human behavioural sequences in an online world, it suggested that on a collective societal level the time-series of particular actions per day can be understood by a simple mean-reverting log-normal model which explains the rarity of absolute autonomy. To conclude, it won\u2019t be wrong to say that we are merely agents with the two agencies (autonomous and agentic) in a multi-agent world. There are two essential ingredients when simulating human-like multi-agent societies- EvolutionThe environment or agent should be open-ended and constantly evolving. CreativityThe idea of seeing creativity as a search in a space of potential search-space is not new; it has been discussed at length by, for example, the British philosopher Margaret Boden. In fact, J. Schmidhuber has also worked tirelessly to capture the idea of curiosity and creativity to devise a Formal Theory of Creativity","title":"The World is a Simulation"},{"location":"machine-learning/posts/2019-09-09-game-ai/#but-training-ai-on-games-isnt-a-good-idea-in-the-long-run","text":"Let us substitute the word \u201cgame\u201d for the environment especially since we have established that the two mean the same in an agents\u2019 world. Now, the games or training environments for AI can be broadly classified into two categories which can be further divided into two sub-categories- Static Environments Dynamic Environments - Dynamic Determinate Environments - Dynamic Indeterminate Environments While training and developing AI algorithms to perform well on static environments is a near-finish goal however developing A.I. that can achieve goals in dynamic environments is an area of research where we quickly hit various roadblocks, esp in dynamic indeterminate environments. In my opinion, there are three key contributors to the same- First would be our inadequate understanding of complexity theory and chaos theory. One of the most interesting efforts that is being pursued in this direction is Fractal AI . Second is the lack of models. Most of our current AI models are data-hungry beasts that need huge computational resources. Some of the most interesting works in this direction pursued by the Game AI community are Framing for Computational Creativity and by the causal inference community The Book of Why The third is the problem of learning that includes transfer learning and lifelong learning. Some of the interesting works in this direction are AutoML Research , Meta-Learning and Differential Evolution (check Differential Plasticity )","title":"But training AI on Games isn\u2019t a good idea in the long\u00a0run"},{"location":"machine-learning/posts/2019-09-09-game-ai/#open-endedness","text":"One of the classical properties of Dynamic Indeterminate Spaces is their open-endedness . Open-ended problems are hard to define and model thus are often abandoned due to the lack of definite metrics. Another way of looking at this problem is through the lens of algorithmic complexity namely Kolmogorov Complexity , Attribute Substitution , Occam\u2019s Razor and Adaptive Levin Search . However, a few researchers who dabbled into this space have created quite a perspective shift. One such work is the paper Exploiting Open-Endedness to Solve Problems Through the Search for Novelty . Later, another excellent work in novelty-search which a few years later led to the development of quality-diversity algorithms . According to one of the first papers in Quality diversity (QDA) algorithms by Kenneth Stanley et. al., QDAs is a kind of learning algorithm (for example, novelty search with local competition and MAP-Elites ) that try to balance the great dilemma in machine learning: exploration vs. exploitation . They aim to optimize for both quality and diversity simultaneously while aiming to fill a space of possibilities with the best possible example of each type of achievable behaviour. The result is this new class of algorithms that return an archive of diverse, high-quality behaviours in a single run. Though, because of the digital nature of inheritance, there are inherent limits on the kinds of questions that can be answered using such an approach. In particular, according to an excellent paper by Troy Day , even in extremely simple evolutionary systems, a complete theory accounting for the potential open-endedness of evolution is unattainable unless evolution is progressive. However, in 1936, Turing adapted Godel\u2019s Theory of Open-Endedness (also called Incompleteness) to computability to show that a general algorithm to solve the halting problem for all possible program-input pairs cannot exist. One of the rebuttals of Godel\u2019s open-endedness problem has been presented in Monads and Sets: On G\u00f6del, Leibniz, and the Reflection Principle . In complexity theory, it can also be interpreted as the Reflection Principle applied to the Wiener Process . So, if we were to agree with Godel and his theory of open-endedness, then how do we develop progressive environments thus leading to progressive search spaces and thus hopefully progressive quality-diversity search algorithms?","title":"Open-Endedness"},{"location":"machine-learning/posts/2019-09-09-game-ai/#introducing-pcg","text":"Procedural content generation (PCG) is the programmatic generation of game content using a random or pseudo-random process that results in an unpredictable range of possible gameplay spaces. But why games again? Didn\u2019t we just establish that games are inadequate training environments? Games make it easy to comparatively test and measure for novelty, delight and desirability in the newly created content. Some of the interesting applications of procedural content generation for training A.I. agents have been seen in the games like Obstacle Tower , Capture the Flag etc. Though it would be interesting to see A.I. agents trained on games like The Shadow of Mordor, a game where NPCs remember their encounters with you and refer back to them in future fights to more accurately represent the real world. Thus, PCG presents huge opportunities for the field of A.I. in general. However, one of the big problems would be while games present excellent training environments for training our A.I. agents in decision making for competition and cooperation. But one of the biggest challenges for these advanced A.I. algorithms would be coordination with humans due to the differences in evolution. Despite our common biological roots and parallel evolutionary process, it has taken animals and humans only 200,000 years to adapt and evolve alongside each other. Through these thousands of years of shared evolution, many animal species esp cats and dogs have become inseparable from human societies, not only terrestrially but also emotionally\u200a\u2014\u200aa magnificent feat especially given the lack of a common language of expression and entirely different cognitive and physical abilities.","title":"Introducing PCG"},{"location":"machine-learning/posts/2019-09-09-game-ai/#the-way-ahead-mixed-initiative-games","text":"This suggests that we could build A.I. systems where A.I. algorithms can collaborate with humans such that the AI can learn, provide suggestions and guess the intent of the human user to help him achieve common goals. Some interesting works in this direction are Mixed Initiative Game Design Platform Tangara , Quake III Arena , Sentinent Sketchbook which is used by AI to second guess the designer\u2019s intention, Collaborative Language Grounding with Robots , Hierarchical Imitation and Intrinsic Social Motivation via Causal Influence in Multi-Agent Reinforcement Learning * Acknowledgements: I would like to thank Ian Danforth for proofreading the article and providing useful feedback.","title":"The Way Ahead: Mixed-Initiative Games"},{"location":"machine-learning/posts/automl-problems-2018/","tags":["machine learning","automl","meta-learning","neural architecture search"],"text":"March 16, 2019 BASED ON MY RESEARCH PAPER ON THE SAME | CO-AUTHOR: SHUBHI SAREEN The constantly growing amount and complexity of data involved in brain studies makes manual parameterization of data processing and modelling algorithms a tedious and time-consuming task. One of the key focuses for the machine learning community has been to develop autonomous agents, by automating the entire production pipeline for selecting and training the learning algorithms, so as to take humans out of the loop. This automation has several benefits including reducing human effort and the associated time cost involved, improving the performance of these algorithms by tailoring them to the problem at hand, reducing human induced bias as well as improving the improving the reproducibility and fairness of the scientific studie s. In this paper, we will be discussing some of the key problems and challenges in AutoML. Our survey will cover central algorithms in AutoML including NAS, etc. In parallel, we also highlight the unique advantages of deep neural network, focusing on developing algorithms that can generalize of different tasks via AutoML. To conclude, we describe several current areas of research within the field. While neural networks were not the first choice for designing machine intelligence, it was in 2012 that neural networks began outperforming competing techniques for tasks that required complex feature engineering [Hinton, Sutskever 2012]. This advent of deep learning has had a significant impact on many areas in machine learning, dramatically improving the state-of-the-art in tasks such as object detection, speech recognition, and language translation. Why Deep Learning for AutoML? \u00b6 While AutoML is not simply limited to deep learning methods, classical machine learning becomes limited by the need for simpler rules and features. The key foundation for deep learning is compositionality, which means it is possible to construct a task from a series of sub-tasks. For most of the tasks we can divide them into two categories - classification problems and regression problems. Deep Learning has shown excellent promise on classification problems. And one could argue that one of the key contributors for this success on classification problem is the convolutional neural networks. Convolutional Neural Networks consist of multiple layers and are designed to require relatively little pre-processing compared to other classification algorithms. They learn by using filters and applying them to the images. The algorithm takes a small square (or \u2018window\u2019) and starts applying it over the image. This mathematical process is called convolution. Each filter allows the CNN to identify certain patterns in the image. The CNN looks for parts of the image where a filter matches the contents of the image. The first few layers of the network may detect simple features like lines, circles, edges. In each layer, the network is able to combine these findings and continually learn more complex concepts as we go deeper and deeper into the layers of the Neural Network. Thus, deep learning essentially allows us to map complex features from inputs to a classification. For this complex feature engineering, the number of available choices of algorithms, learning methods and packages for machine learning have quadrupled in the past decade. The algorithm choice space ranges from CNNs, LSTMs to GANs. Learning methods ranging from K-means clustering, PCA (Principal Component Analysis), Bayesian Optimization to Q-Learning. The number of packages ranging from Tensorflow, Scikit-Learn, PyTorch, etc. When doing AutoML these days, the performance is hugely dependent on the design decision of the model. These design decisions include - algorithm selection and hyperparameter optimization. Usually, this optimization process for any selected algorithm is repeated via hit and trial until we find a good set of choices for each dataset. This problem has been dubbed as CASH (Combined Algorithm Selection and Hyperparameter Optimization) and can present a combinatorial number of choices for any user. For any non-expert, this often results in choosing an algorithm based on either reputation or intuitive appeal and more often than not, leaving the hyperparameter settings to default thus resulting in less than optimal performance for that particular selection of models. Lately, Tensorflow has helped make this process slightly easier by allowing the users a visual representation of the process and an almost plug and play like interface for optimization settings, however the choice of algorithm selection and learning method selection is still heavily user-dependent. History \u00b6 The goal of AutoML is largely to build autonomous agents that can take human out of the loop for choosing and designing learning algorithms. This can potentially mean huge gains in the performance of machine learning algorithms by tailoring them to the problem at hand, reducing human induced bias as well as improving the improving the reproducibility and fairness of the scientific studies. We roughly divide this section in two parts- the Model Selection where we discuss the problem of architecture design (also known as Search Space) and Learning Policy Selection, and Model Optimization where we discuss the problem of optimization choices made while training the model including hyperparameters, ensembles and regularizers. In this section, we will go through the reasoning behind some of these decisions. For image classification, various architectures have been proposed over the years. The convolution neural network architecture was first proposed in LeNet-5, which consisted of 6 layers consisting of convolution, average pooling and hidden layers. However, because of low computational resources, convolutional architectures next came into attention in 2012 with the first fast implementation of CNNs on GPU, AlexNet. AlexNet employed 8 layers, including 5 convolutional and 3 fully connected layers using the ReLU activation function. The authors highlighted the gains in performance using ReLU over tanh and sigmoid activation functions. Dropout was used in the fully connected layers to prevent overfitting. In 2014, VGGNet was proposed which consists of 16 convolution and three fully connected layers employing only 3x3 convolutions. Next, GoogleNet was proposed which consisted of 22 layers. The main intuition behind the model was to make it compatible to be used on a smartphone with 12 times lesser parameters than AlexNet. It employed inception modules, which convolve parallely using different sizes of filters. Next, in 2015, ResNet was proposed which introduced skip-connections between the layers. These skip connections allowed a 152 layer architecture to be trained, while still having complexity lesser than that of VGGNet. Thus the advances in image classifications can be attributed to two main features- Increasing the number of layers Introduction of operations/connections which reduces the amount of parameters to be trained. However, the process of defining the number of layers is still controlled majorly by human beings and the hyperparameters are hand-tuned, therefore requiring a lot of time to be invested. The next step was to model these techniques automatically, thereby leading to the introduction of AutoML. What is AutoML? \u00b6 To define the problem of AutoML, In the past five years, various AutoML methods have been proposed. Introduced in 2013, Auto-WEKA was the first AutoML method and used Bayesian Optimization to automatically instantiate a highly parametric machine learning framework. Parametric models assume some finite set of parameters \\(\\theta\\) . Given the parameters \\(\\theta\\) , future predictions, x, are independent of the observed data, D \\[ P(x|\\theta, D) = P(x|\\theta) \\] It is based upon the assumption that \\(\\theta\\) captures everything there is to know about the data and thus the complexity of the model is bounded even if the amount of data is unbounded. While these parametric methods are simpler, faster, interpretable and can be trained with less data but they pose flexibility constraints while struggling with complex data generating a poor fit for the real world problems. It was further extended as a package called Hyperopt-sklearn in 2014 to non-parametric methods like probability calibration, density estimation provided by Scikit-Learn. One of the key achievements of Hyperopt-sklearn is an ability to remove redundant features, or reduce data dimensionality using SVM or PCA so the classification task can become simpler. While one possible direction post this could be extending the framework to support more machine learning algorithms or extending the number of pre-processing methods however the ability to select a good algorithm and a combination of the right hyperparameters is a problem that lies at the heart of AutoML. Introduced in 2015, Auto-sklearn approached the problem by taking into account past performance on similar datasets, or constructing ensembles from the models evaluated during the optimization. While budget and computational power being the obvious constraints for such methods, one not-so-obvious limitation is that testing hundreds of hyperparameter optimization configuration increases the risk of overfitting to the validation set. Aside from this, several incremental developments have been made in the field of architecture selection, hyperparameter optimization and meta-learning using both gradient based as well as gradient-free methods. Gradient Free Techniques \u00b6 The most intuitive gradient-free technique is random search i.e. randomly searching the state-space domain to find optimal values. However, since the state space is multi-dimensional, therefore, leading to suboptimal results using random search. Population based methods such as Genetic Algorithms and Particle Swarm Optimization have been employed in order to search the state-space more effectively. Other gradient free techniques include Bayesian Optimization by modelling the loss of a configuration, where the improvement of a configuration is proportional to the distance between the best-so-far loss and the predicted mean or using Tree Based Estimator like MCTS or using surrogate models for instance Gaussian processes. Q-learning has also been used in order to learn desirable architectures, where a reinforcement agent chooses components of an architecture sequentially with the reward equal to the validation accuracy. Gradient Based Techniques \u00b6 In the past few years, deep learning methods have shown immense potential in the area of AutoML. One such key project is Auto-Net which uses Bayesian Optimization method SMAC and the deep learning library Lasagne, later followed by Auto-Net 2.0 that uses a combination of Bayesian Optimization and HyperBand, called BHOB, and uses PyTorch as the key deep learning library. Auto-Net provides automatically-tuned feed-forward neural networks without any human intervention. BHOB uses repeated runs of successive halving to invest most runtime in promising neural networks and stops training neural networks with poor performance early and using kernel density estimator to describe regions of high performance in the space of neural network. One of its key advantages is that this method is highly parallelizable achieving almost linear speedups with an increasing number of workers. Auto-Net performance on three different datasets - As observed, AutoNet performs best twice and comparatively well for yolanda as well. As the No Free Lunch theorem goes, any one algorithm that searches for an optimal cost or fitness solution is not universally superior to any other algorithm given that we are optimizing for fit on a limited class of problems thus it may or may not generalize in an infinite possibilities dataset. Thus gradient based neural networks started to focus on building a model from existing class of models instead of random search on a particular number of configurations. In 2016, the focus shifted from searching within specified parameters to instead the use of modular approach to find the best model and combination of hyperparameters. Model Selection - Architecture Design and Learning Policy Selection \u00b6 In 2016, MetaQNN was introduced to generate CNN Architectures for a given task using Q-Learning, where the agent chooses the architecture layers sequentially, via the epsilon-greedy strategy, with reward equal to the validation accuracy. Next, the problem of generating a high-performing architecture was dealt by using a meta-controller. A meta controller models state information and directs how the search space is to be navigated in order to find the optimal architecture. Traditionally, RNNs and LSTMs are used as metacontrollers, encoding the state space information. The state space consists of various blocks performing operations like convolution and pooling, and the meta-controller, say RNN, arranges these blocks in an end-to-end fashion. The controller is then updated based upon the validation accuracies to produce a better performing architecture in the next iterations. Another new approach in 2016, was the creation of NAS. The intuition behind this was since a single model does not generalize well for different datasets. Let\u2019s try to automate the process of building a system that can adapt to different datasets. This was done by using smaller pieces from the architecture to build a new architecture using a technique called convolutional cells. However, limitations of this approach are it does limit itself to using the same type of controller and same kind of model for every image, which might or might not be optimal. Also, it happens to be computationally expensive. Progressive Nets however were the next iteration. Progressive Nets employ sequential-based model optimization to further optimize the search process. It learns the architecture in increasing complexity, with first learning the cell structure and then stacking those together to form a CNN. The next in line was ENAS i..e efficient neural architecture search. In 2017, Efficient Architecture Search was proposed to reuse the weights of pre-existing architectures using a Reinforcement Agent as a meta-controller. The architecture is encoded into a low dimensional representation using a bidirectional RNN with Embedding Layer. The network transformations are produced by an independent actor network which takes in as input the encoded representation. Efficient Neural Architecture Search is the next step to reduce the dependence on computational resources for generating high performance architectures. A policy gradient approach is used to train a controller which can select optimal subgraphs from within a larger graph. It allows the children graphs to share weights, therefore allowing faster convergence. ENAS employs LSTM as a controller. The training procedure consists of alternating between learning the parameters shared by the children and that of the LSTM controller. In addition to the requirement of high computational costs, the performance of network architectures learnt on a specific dataset don\u2019t transfer to other tasks. In order to overcome that, in BlockQNN the authors proposed to generate the architecture block-wise, rather than layer-wise. The optimal block structure is searched using Q-Learning with \\(\\epsilon\\) -greedy strategy and experience relay. Lately, there has been a influx of various AutoML methods like Elastic Architecture Transfer for Accelerating Large-scale Neural Architecture Search (EAT-NAS), Graph HyperNetworks for Neural Architecture Search, Stochastic Neural Architecture Search (SNAS), Instance-aware Neural Architecture Search (Insta-NAS) etc that use several approaches that we will discuss in further sections. One of the major drawbacks of these approaches is the fact that each architecture is trained from scratch, thereby basing their success on external factors like availability of computational resources. However, In the limited computational resource scenario, Bayesian Optimization techniques have emerged to be very successful for their ability to work with combinatorial space problems with limited training time as well as their flexibility with high-dimensional problems and transfer learning problems. Architecture Search via Bayesian Optimization \u00b6 Bayesian optimization has emerged as an exciting subfield of machine learning that is concerned with the global optimization of noisy, black-box functions using probabilistic methods. Essentially, Bayesian Optimization is a sequential model-based approach to machine learning. Mathematically, the problem of finding a global maximizer (or minimizer) of an unknown objective function f can be represented as x* = arg max f(x) for x in X\u2019 where X\u2019 is some design space of interest. One of the key advantages of Bayesian Optimization framework is that it can be applied to unusual search spaces that involve conditional or categorical inputs, or even combinatorial search spaces with multiple categorical inputs. The Bayesian Optimization framework has two key ingredients - first, a probabilistic surrogate model which consists of a prior distribution that captures our beliefs about the behaviour of the unknown objective function and an observation model that describes the data generation mechanism and secondly, the loss function that describes how optimal a sequence of queries are. In practice, these loss functions are often expresses as a form of regret, either simply or cumulative which is then minimized to select an optimal sequence of queries. If f is cheap to evaluate we could sample at many points e.g. via grid search, random search or numeric gradient estimation. However, if function evaluation is expensive e.g. tuning hyperparameters of a deep neural network, probe drilling for oil at given geographic coordinates or evaluating the effectiveness of a drug candidate taken from a chemical search space then it is important to minimize the number of samples drawn from the black box function f. This is the domain where Bayesian optimization techniques are most useful. They attempt to find the global optimum in a minimum number of steps. A popular surrogate model for Bayesian optimization are Gaussian processes (GPs). The success of these methods has led to the discovery and development of several acquisition functions including Thompson Sampling, Probability of Improvement, Expected Improvement, Upper Confidence Bounds, Entropy Search etc. These acquisition functions trade off exploration and exploitation such that their optima is located where the uncertainty in the surrogate model is large (exploration) and/or the prediction is high (exploitation). In terms of AutoML, it has been proved through various experiments that often the careful choice of statistical model is often far more important than the choice of acquisition functions. While Bayesian Optimization is already one of the preferred methods for combinatorial search spaces and policy search for both parametric as well as non-parametric models, one of the major applications in AutoML has been the exploration of Bayesian Optimization for Automating Hyperparameter Tuning. In architectural search spaces, Bayesian Optimization methods have received great attention in tuning deep belief networks, Markov Chain Monte Carlo Methods (MCMC), Convolutional Neural Networks and its application in one of the most primitive neural architecture search methods such as SMAC, Auto-SkLearn, AutoWeka, BHOB, REMBO etc. Another key application of Bayesian Optimization has been on multi-task learning and transfer learning, esp in a constrained budget. There have been several attempts to exploit this property within the Bayesian Optimization framework including Collaborative Hyperparameter Tuning, Initializing Bayesian Hyperparameter Optimization via Meta-Learning, Sequential Model-Based Optimization for General Algorithm Configuration, Gaussian Process Bandit Optimization, Freeze-Thaw Optimization, Efficient Transfer Learning via Bayesian Optimization. The key idea here is that there are several correlated functions, \\(\\tau\\) = {1, 2, \u2026.,M}, called tasks and that we are interested in optimizing some subset of these tasks, thus using the data from one task to provide information about another task. One such method of sharing information between tasks has been implementing by modifying the Bayesian Optimization Routine to modify the underlying the Gaussian Process (GP) model called Multi-Output GPs. We define a valid covariance over input and task pairs, k((x,m), (x\u2019, m\u2019)). One of the explored methods to do this is using coregionalization that utilizes the product kernel - \\[ k((x,m), (x\u2019, m\u2019)) = k_{\\chi}(x,x')k_{\\tau}(m,m') \\] Where m, m\u2019 \\(\\in\\) \\(\\tau\\) . The problem of defining a Multi-Output GP can then be viewed as learning a latent function, or a set of latent functions, that can be transformed to produce the output tasks. This can be done by introducing a latent ranking function across different pairs of observations on various tasks such that the tasks are jointly embedded in a single latent space that in variant to potentially different output scales across tasks. The key idea here is to build a joint model with a joint probability distribution function which can then be trained via various methods like Random Forest to learn the similarities between different tasks based on the task features. The best inputs from the most familiar tasks (or models) are then used as the initial design for the new task (or new model) thus creating an automatic architecture space pipeline for working on cross domain machine learning models. Architecture Search via Evolutionary Methods \u00b6 An evolutionary algorithm based approach is used to evolve AmoebaNet-A, an architecture for image classification at par with human crafted designs. The proposed approach biases tournament selection with age, thereby preferring younger genomes. The search space consists of directed graphs where hidden states correspond to vertices and ordinary operations like convolution, pooling etc. are associated with edges. Mutation is performed either by random reconnection of edges to other vertices or relabeling randomly selected edges. After initializing the parent population with randomly generated architectures which are trained and evaluated, children architectures are generated using mutation. These architectures are then trained and evaluated, whereas the oldest ones are discarded. The ageing evolution used is analogous to regularization, as the only way good architectures can be retained are when they are inherited by the children. Since at each step, children models are retrained, architectures which constantly perform good, are the ones which are eventually retained, thereby preventing overfitting. In GeneticCNN, Genetic Algorithms are used to come up with Convolutional Neural Network Architectures. A binary string is used to represent each possible network. 0.5 \\(\\Sigma\\) sKs(Ks-1) bits are needed to represent the network with S stages, and each stage having Ks nodes. The bit corresponding to (vs,i, vs,j) is set if there is a connection between the associated nodes. A default input and output node are present at every stage. Isolated nodes do not have a connection to the default nodes. In case of absence of connections at a stage, i.e. all bits are unset, convolution is applied only once. Only convolution and pooling operations are taken into consideration. The initial population consists of randomized strings, where each bit is sampled from Bernoulli Distribution. Individuals are selected using Russian roulette process, where fittest individual has maximum probability of being selected. In mutation, a randomly selected bit is flipped with mutation probability. In crossover between two individuals, corresponding stage pairs are exchanged with crossover probability. Fitness is obtained by training the architectures represented by individuals on a preselected reference dataset. Cartesian Genetic Programming is used to design architecture for Convolutional Neural Network which provides the benefit of flexibility to incorporate skip connections and variable depth networks in the representation. The network is defined as a directed acyclic graph represented using s two dimensional grid consisting of computational vertices. For a grid of Nr rows and Nc columns with fixed number of inputs and outputs, nodes in the nth column have connections from nodes in the range of (n-l) to (n-1) columns. The architecture representation in the genotype have fixed length integers, each chromosome consisting of type and connectivity information regarding that node. The phenotype consists of variable number of nodes because all nodes don\u2019t have a connection to the output. Computational nodes comprises of convolution operation, average pooling, max pooling, summation, concatenation and ResBlock (convolution processing, batch normalization, ReLU, and tensor summation). A (1+ \\(\\lambda\\) ) evolution strategy is used to find the appropropriate architecture where the most fit of the \\(\\lambda\\) children generated at each generation become the parents for the next generation. Children architectures are trained in parallel and fitness is defined as the validation accuracy. The mutation strategy used is to change the type (size and number of filters) or the edge connections at random depending upon the mutation probability. In case of stagnation in fitness of children, mutation is applied to the non functioning nodes in the parent architectures. The proposed approach has an inherent drawback of high computational cost. Another work analyzed the performance of gradient-free Genetic Algorithms on Reinforcement Learning Problems. The population of N individuals comprise of vector parameters for neural networks denoted by \\(\\theta\\) i. On evaluating \\(\\theta\\) i in every generation, a fitness score F( \\(\\theta\\) i) is associated with each individual which is analogous to reward. The most fit individuals become the parents in the next generation. Mutation is performed on randomly selected parent by addition of Gaussian noise in the parameter vector: \\(\\theta'\\) = \\(\\theta\\) + \\(\\sigma_\\epsilon\\) , where \\(\\epsilon\\) ~ N(0,1) and value of \\(\\epsilon\\) is empirically determined in every experiment. The best individual from each generation is retained which is determined by evaluation of the most fit 10 individuals for additional 30 episodes and selecting the one with the highest mean score as the elite individual to be retained. Crossover is not included for simplicity. The process is then repeated for the new generation of individuals. The hyperparameters for all Atari games were selected from a collection of 36 hyperparameters evaluated on 6 games. In order to efficiently implement distributed deep GA, parameter vectors are stored as combination of the initial seed value and the random seeds producing the sequence of mutations from which it is possibly to reconstruct \\(\\theta\\) . This compressed representation is independent of the network size and linearly dependent on the number of generations. It was observed that all the final Atari networks were 8000-50000 times compressible. Another advantage provided by the use of GAs over traditional RL methods is that of increased speed. Another variant of GA which uses Novelty Search in place of Fitness Function was implemented to evaluate the performance of its combination with DNNs on deceptive image based Reinforcement Learning problems. Novelty Search rewards agents for novel behaviour, thereby avoiding local optima. A policy\u2019s novelty is determined by calculating the average behavioral distance to its \\(k(25)\\) nearest neighbours. This fitness function is inspired by Darwin\u2019s Fitness function which suggests that the genetic contribution of an individual to the next generation's gene pool relative to the average for the population, usually measured by the number of offspring or close kin that survive to reproductive age. the ability of a population to maintain or increase its numbers in succeeding generations. Also, another related work derived a new approach where the children are pre-trained from parent networks. This technique is called Lakhmarian Evolution. Architecture Search via Reinforcement Learning \u00b6 Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice. Despite their success, these methods are still limited in that they only search models from a fixed-length space. In other words, it is difficult to ask them to generate a variable-length configuration that specifies the structure and connectivity of a network. And then as discussed above, there are Bayesian optimization methods that allow one to search non fixed length architectures, but they are less general and less flexible than reinforcement learning methods which provide a general framework for machine learning. Modern neuro-evolution algorithms, on the other hand, are much more flexible for composing novel models, yet they are usually less practical at a large scale. Their limitations lie in the fact that they are search-based methods,thus they are slow or require many heuristics to work well. One of the key advantages of reinforcement learning methods for architectural space design is that they can learn directly from the reward signal without any supervised bootstrapping, which is an immensely attractive characteristic for learning new tasks without much human interference. The key idea here is to use a neural network to learn the gradient descent updates for another network and then using reinforcement learning to find update policies for other networks. Under this framework, any particular optimization algorithm simply corresponds to a policy. Then, we reward optimization algorithms that converge quickly and penalize those that do not. Learning an optimization algorithm then reduces to finding an optimal policy, which can be solved using any reinforcement learning method. To differentiate the algorithm that performs learning from the algorithm that is learned, we refer to the former as the \u201clearning algorithm\u201d or \u201clearner\u201d and the latter as the \u201cautonomous algorithm\u201d or \u201cpolicy\u201d and use an off-the-shelf reinforcement learning algorithm known as guided policy search. A reinforcement learning problem is typically formally represented as an Markov decision process(MDP). We consider a finite-horizon MDP with continuous state and action spaces defined by the tuple (S,A,p0,p,c, \\(\\gamma\\) ), where S is the set of states, A is the set of actions, \\(p0:S \\rightarrow R+\\) is the probability density over initial states, \\(p:S \\times A \\times S \\rightarrow R+\\) is the transition probability density, that is, the conditional probability density over successor states given the current state and action, \\(c:S \\rightarrow R\\) is a function that maps state to cost and \\(\\gamma \\in (0,1]\\) is the discount factor.The objective is to learn a stochastic policy \\(\\Pi^*\\) : $S \\times A \\rightarrow R+ $, which is a conditional probability density over actions given the current state, such that the expected cumulative cost is minimized. That is, where the expectation is taken with respect to the joint distribution over the sequence of states and actions, often referred to as a trajectory, which has the density This problem of finding the cost-minimizing policy is known as the policy search problem. To enable generalization to unseen states, the policy is typically parameterized and minimization is performed over representable policies. Solving this problem exactly is intractable in all but selected special cases. Therefore, policy search methods generally tackle this problem by solving it approximately.In many practical settings,p, which characterizes the dynamics, is unknown and must therefore be estimated. Additionally, because it is often equally important to minimize cost at earlier and later time steps, we will henceforth focus on the undiscounted setting, i.e. the setting where \\(\\gamma\\) = 1. Guided policy search is a method for performing policy search in continuous state and action spaces under possibly unknown dynamics. It works by alternating between computing a target distribution over trajectories that is encouraged to minimize cost and agree with the current policy and learning parameters of the policy in a standard supervised fashion so that sample trajectories from executing the policy are close to sample trajectories drawn from the target distribution. The target trajectory distribution is computed by iteratively fitting local time-varying linear and quadratic approximations to the (estimated) dynamics and cost respectively and optimizing over a restricted class of linear-Gaussian policies subject to a trust region constraint, which can be solved efficiently in closed form using a dynamic programming algorithm known as linear-quadratic-Gaussian (LQG). At the point of writing, we are not aware of any model-based reinforcement learning methods used to find the optimal search space for automatic neural architecture design. Architecture Search via Tree Based Methods \u00b6 Another alternative approach for finding optimal architectures is also what is sometimes referred as Programming by Demonstration. In these methods, the strategy/policy is based upon inputs and outputs which are recorded at very instance which we do guided search using MCTS or Monte-Carlo Tree Search. Also another popular approach used in Markov Decision Monte Carlo Tree Search. While highly interpretable, but this approach has multiple limitations. MCTS works by exploring several single method and optimizing for global optimum which can be computationally expensive. This was explored by studying the performance of MCTS methods in the 2013 paper Investigating the Limits of Monte-Carlo Tree Search Methods in Computer Go. Model Optimization- HPO and Regularization \u00b6 While the progress in the field of AutoML has followed the step-wise progressive direction of from doing effective feature designing to manual tuning of various hyper-parameters and implementing regularization techniques to architecture designing one of the key contributions of this paper is to instead look at the problem of automatic model creation and hyperparameter tuning in a top-down fashion instead of a bottom-up approach that explores various hacks to produce substantial predictive and computational gains over our datasets. We believe that the problem of automatic model generation lies at the heart of AutoML and other various optimization techniques can instead be used to further the performance of such automatically generated models. One of the key advantages of this approach is the focus on general-use architectures that can adapt well to non-stationary and cross-domain data i.e. transfer learning. In the past two years, transfer learning has managed to effectively adapt to new domains including in Video Classification to Sentiment Analysis in NLP. The biggest advantage of adopting this line of thinking is the idea of reducing human time and effort required in creating ML model pipelines that lies at the heart of AutoML. We believe the bottom-up approach is rather contrarian in increasing the amount of manual human effort and computational efforts required in testing various approaches since designing architectures still requires a lot of expert knowledge and takes ample time. Various ways in which the performance of state-of-the-art architectures can be improved includes inclusion of more diverse data for training, or trying various possibly architectures, both being expensive tasks. In order to reduce model overfitting, regularization is used to penalize the model loss for complex models. The basic intuition is that smaller magnitude of model weights will produce a simpler model, thereby reducing overfitting. Normalization techniques can fasten the training process of deep neural networks. The input distribution for each layer in a network changes with updation in parameters of the previous layer, referred to as Internal Covariance Shift. Batch Normalization reduces this shift by taking mean and variance of each layer as a constant, and integrating normalization in the neural network architecture, thereby accelerating the end-to-end training process. The dependency if gradients on input values and scale is considerably reduced, allowing larger learning rates. It also acts as a regularizer, reducing the need of Dropout. However, simply normalizing inputs of each layer can change the meaning represented by each layer. This is tackled by introducing parameters which scale and shift each activation, and learnt during the training process. Secondly, this model uses mini-batches of the training dataset to estimate mean and variances. Weight Normalization, on the other hand aims to reparametrize weight vectors into a parameterized vector and a scalar parameter, in such a way that the Euclidean norm of the weight vector is equal to the scalar parameter. Thus acceleration in the training process can be attributed to decoupling the weight vector\u2019s norm from its direction. Weight normalization reduces the required overheads and is not dependent on batch size. Another way could be to find the optimal values of hyperparameters in the hyperparameter space which minimizes the loss. The naive approach is to incorporate grid search, which evaluates all possible combinations of hyperparameters. However, this isn\u2019t feasible in real-time, because hyperparameter space is multi-dimensional with infinitely many possible values for the numerical hyperparameters. The next step to perform random search which tests random combinations of hyperparameters within the limits of resources or until a desirable performance is achieved. The success of random search lies in the fact that there are only a few hyperparameters which affect the model accuracy, thereby reducing the apparent dimensionality of the search space. However, the importance of hyperparameters isn\u2019t known beforehand. The next step, intuitively, is to learn the impact of the hyperparameters and accordingly pay attention to those. Bayesian Optimization, thus builds a probabilistic loss-model, updating it sequentially with the newer combinations being tried out, which are selected by the acquisition function. Other sequential model based algorithms employs Tree Parzen Estimators and Random Forests. However these approaches lack a method of early termination for poorly performing hyperparameter combinations. Bertrand H. et. al. proposed an approach to fine tune hyperparameters for a machine learning model using an ensemble of two complementary approaches - Bayesian Optimization and Hyperband. Hyperband selects and trains a range of models in the hyperparameter domain, while removing the models with the worst performing models after a certain time spent in training. This step is repeated until only a few combinations are left. Bayesian Optimization invests time on training bad models, while Hyperband doesn\u2019t result in improvement in the quality of selected model with time. Combining these two approaches can overcome both of these problems. Initial configurations are selected uniformly and evaluated using Hyperband, after which subsequent selections are a result of training Gaussian Process. After this, the next set of configurations is sampled from the probability distribution resulting from the normalized expected improvement of un-evaluated configurations. An alternative approach to hyperparameter optimization is to use gradient based approaches. It uses two contrasting approaches to calculate hypergradient in Reverse and Forward Mode. In the reverse mode formulation, the computation is similar to that of backpropagation through time. The forward mode of hypergradient computation relies on chain rule for computing derivative in order to compute the gradient w.r.t hyperparameters. Forward mode calculation is particularly efficient when the hyperparameter space is much smaller than that of the parameters. In another approach, we aim to overcome the constraint of limited memory when computing gradient w.r.t hyperparameters while optimizing the same on validation set. This is achieved by a procedure which is exactly the reverse of stochastic gradient descent with momentum, which allows to compute gradient of the function of the final trained weights w.r.t. learning rate, initial weight configuration, momentum schedule and any other hyperparameter which may be involved with the time complexity equal to that of forward SGD. However, because of finite precision, this computation results in loss of important information as momentum schedule \\(\\gamma < 1\\) , therefore discarding bits at every multiplication step. This can be overcome by saving the discarded information at each step, and with \\(\\gamma = 0.9\\) , less than one bit per average needs to be stored. Pipeline Optimization for AutoML \u00b6 While machine learning is devised to become a tool that could generate classifications or predictions directly from the data itself however as of now deploying these machine learning models end-to-end is still a challenge that\u2019s currently tackled in part with a lot of intervention from humans at various stages. The key steps in training a machine learning algorithm range from pre-processing to feature selection to prediction. While still an unsolved problem but in 2016, TPOT was created as an automated pipeline design optimization method for AutoML. TPOT works as an architecture with three kinds of operators where each operator corresponds to a machine learning algorithm. Multiple copies of the entire dataset enter the pipeline for analysis. These various copies are then passed over to various types of feature processing operators are available in TPOT including StandardScalar, RobustScalar, RandomizedPCA, Polynomial Features etc that modify the dataset in some way thus returning the modified dataset. Later, these all features from multiple copies are then combined and best features are selected via Feature Selection Operators including SelectKBest, SelectFwe, Recursive Feature Elimination etc. The goal of this step is to reduce the number of features so as pick only the best features. However, this can also cause leaks and overfitting in automated pipelines. Finally, the selected features are sent to Supervised Classification Operators that include Decision Tree, Random Forest, Extreme Gradient Boosting (XGBoost), Logistic Regression and K-Nearest Neighbour Classifier(KNN) which stores the classifier's predictions as a new feature as well as produce classifications for the pipeline. PROBLEMS AND CHALLENGES in AutoML \u00b6 Cost Sensitivity \u00b6 Building an AutoML system that looks for all the variations of hyperparameter settings, all learning policies can be a very costly endeavour. Also, this does not favour how natural intelligence works. Argument 1: During meta-learning, the model is trained to learn tasks in the meta-training set. There are two optimizations at play \u2013 the learner, which learns new tasks, and the meta-learner, which trains the learner. Methods for meta-learning have typically fallen into one of three categories: recurrent models, metric learning and learning optimizers. Model Based Methods \u00b6 The basic idea behind model based methods is fast learning through rapid parameter updation. One way is to feed the input sequentially to a recurrent model which learns the sequence representation to adapt to newer tasks. These RNN models ingest their own weights as additional inputs to identify their errors and update their own weights for better generalization on newer tasks. Memory Augmented Neural Networks are a family of architectures which learn how to quickly encode previously unseen information and thus learn new tasks after seeing limited samples. A Neural Turing Machine combines a controller with external storage buffer and learns to perform read and write operations in memory using soft attention. The label for an input was presented with an offset of 1 timestamp, which allowed the network to learn to keep the data in memory until the label is presented later and retrieval of earlier information for making predictions. The reading mechanism incorporates content similarity where the cosine similarity between the input feature and the memory rows is computed to produce a read vector which is the weighted sum of memory rows. For the write operation, Least Recently Used Access (LRUA) writer is designed in which the write weight vector is an interpolation between previous least used weight vector and previous read weight vector. The intuition lies in the idea of preserving frequently used information and the fact that recently retrieved information won\u2019t be used again for a while. MetaNets are used for rapid generalization across various tasks. One neural network is used to predict weights of another neural network and these weights are known as fast weights. Weights learnt using stochastic gradient descent, on the other hand are termed as slow weights. MetaNets combine slow and fast weights. MetaNets consist of an embedding function for learning the input representation and a base learner model which performs the actual learning task. A LSTM network is then used to learn the fast weights for embedding function and a neural network is incorporated to learn the fast weights for base learner from their loss gradients. Thus, here we can see that MetaNets effectively have to learn 4 sets of model parameters. Thus model based methods effectively design a fast learning mechanism, where the models rapidly update their parameters using their own architecture or by incorporating additional meta learning model. Metric Learning \u00b6 The metric learning based approaches are analogous to kernel density estimation and nearest neighbours estimation, where the predicted probability for an output label for a data point is equal to the weighted sum of labels for other data points in training sample, where the weight signifies the similarity between the 2 samples. Koch, Zemel & Salakhutdinov used Siamese Networks to learn embeddings for a pair of images to check if they belong to the same output class. The advantage of this approach lies in its ability to measure similarity between images of unknown classes, but its utility decreases with divergence in tasks from the original training perspective. Relation Networks are similar to Siamese networks but they captured the correlation between an image and support set by incorporating a CNN classifier rather than using a distance metric. It also used MSE Loss in place of cross entropy, because intuitively, it outputs a relationship score, rather than a classification label. In another approach - Matching Networks, incorporated attention networks which measured cosine similarity between the embedding for a test sample and corresponding support set images. Since the performance of Matching Networks depends upon the quality of the embeddings learnt, a Bidirectional LSTM was trained over the entire support set for a class label to learn the embeddings. For the test sample, an LSTM incorporating attention with the support set as hidden state was used to learn effective embeddings. Prototypical Networks learns a prototype feature vector for each class, which is defined as mean over feature vectors learnt for each sample in the support set. For a test sample, the output class can be found by taking softmax over inverse-distances with the prototype vectors. The major task, therefore, in Metric based methods, is therefore to learn the kernel function, which in turn depends upon learning the embedding function for the test sample and the samples in support set. Optimization Based Learning \u00b6 Gradient based methods aren\u2019t suitable for learning from a small training dataset or converging in a few steps. One idea is to use an additional meta learner to update the weights of the model at hand in a small number of steps. An LSTM Meta Learning provides the following advantages: Similarity between cell state update and gradient based update Analogy between storing history of gradients in momentum based update and memory in LSTM Parameter sharing across coordinates is incorporated to compress the parameter space for the meta learner In Model Agnostic Meta-Learning, the parameters for the new task are calculated by performing a single or multiple gradient descent operations on the original parameters. The original model parameters are then updated using stochastic gradient descent using the loss over new parameters for the new problems sampled from a distribution of tasks. The basic idea behind this approach is to learn features which are applicable to a distribution of tasks, rather than to that of a single task. Meta Learning Approaches, therefore aim at increasing the generalization of a model across tasks, without re-training the entire network, and therefore reducing the cost of learning. Model Based Methods utilize inherent or additional memory for fast updation of model parameters. Metric Learning, on the other hand rely majorly on effective representation learning. Optimization Methods replace gradient based methods to generalize the model across tasks with a smaller dataset in hand or in a fewer number of steps. Curse of Dimensionality \u00b6 Curse of Dimensionality refers to the problem of exponentially increasing data configurations with a linear increase in dimensions. The statistical challenge lies in the small number of available training tuples available compared to the large number of possible configurations. With increasing dimensions, there is a possibility for the lack of training examples for a given configuration of dimensional values. Machine Learning Algorithms using the nearest neighbour information to perform a task will fail in such a case. Natural intelligence is multidimensional (Gardner, 2011) and given the complexity of the world, generalized artificial intelligence will necessarily be multi-dimensional as well. The most important property of deep learning is that deep neural networks can automatically find compact low-dimensional representations(features) of high-dimensional data (e.g., images, text and audio). Through crafting inductive biases into neural network architectures, particularly that of hierarchical representations,machine learning practitioners have made effective progress in addressing the curse of dimensionality [Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation Learning: A Review and New Perspective] Principal Component Analysis, the most commonly used dimensionality reduction technique maps the data to a lower dimension, such that the resulting variance of the data is maximized. The basic objective is to capture most of the variance in the original data, while reducing the number of dimensions used to represent it. Autoencoders can also be used to perform dimensionality reduction, where they learn a reduced representation in the reduction phase, and then goes on to reconstruct the original data tuple in the reconstruction phase. Interpretability \u00b6 Deep neural networks tend to perform better than the shallow networks, but is it certain that the improved performance can be attributed to the increased depth of a network? In a related paper, the authors mimicked deep networks with shallow ones by training shallow networks not on the original labels, but on the ones predicted by the deep network. The authors observed that shallow networks at par with the deep networks on CIFAR-10 image recognition and TIMIT phoneme recognition, allowing us to ponder upon the correlation between the training procedures used and the success of deep neural networks. It also opens up room to come up with novel approaches to train a shallow network more accurately. According to the No Free Lunch Theorem for Optimisation: If an algorithm performs better than random search on some class of problems then in must perform worse than random search on the remaining problems. In other words, the NFLT states that no black box strategy is superior to another one. Another point to consider is the tradeoff between generality and specificity of an approach. The need for defining interpretability arises from the gap between objectives in a supervised learning environment and the actual deployment costs. The training procedure only requires the predicted and actual labels for optimization. The problem arises when the real world problem cannot be translated into a mathematical function or because of the difference between the deployment and training settings. Interpretations thus cater to the important but difficult to formalize objectives. The objectives of interpretability are: Trust: Does the model mimic humans, i.e. performing accurately where humans perform accurately and committing mistakes in areas where humans also struggle? Are we comfortable giving the model the entire control without human supervision? Causality: Can the associations generated by models be tested experimentally? Transferability: While supervised learning algorithms are evaluated based upon their performance on the test set, independent of the training data, humans have far more capabilities of generalizing their learnings. This is evident from the failure of machine learning models in adversarial settings. Informativeness: To provide more information to human decision makers, rather than simply outputting a single value. Fair and Ethical Decision-Making: Conventional evaluation metrics do not ensure acceptability. Algorithmic decisions must be supported with reasoning and should offer a way to be contested and modified if proven to be wrong. The paper also expanded on the properties of an interpretable model grouping it under 2 categories: transparency and post-hoc explanations. Transparency : It can be defined at 3 levels:- At the level of the entire model - Simulatability: Allowing a human to combine input data, along with model parameters and be able to carry out all the computations to produce the output in reasonable time. But how much is \u201creasonable\u201d amount of time? Are high dimensional linear models, cumbersome rule based systems or huge decision trees interpretable - No. What about a compact neural network? At the level of individual components - Decomposability: Offering an intuitive explanation to each input, parameter and calculation. Associating descriptions with each node in decision trees or using parameters of a linear model to represent the association strength between features and labels. At the level of algorithm used - Algorithmic Transparency: A linear model will converge to a unique point even for a new dataset. This however cannot be guaranteed for deep learning mechanisms. Interpretability : Offer useful information to humans in the form of: Text Explanations: Train a model to produce an output and a separate model to produce an explanation, mimicking the verbal descriptions of humans. Visualizations: Qualitatively represent the model\u2019s learnings in form of visualizations. Local Explanations: Offering local explanations to the decisions of a neural network. A popular example is that of a saliency map which highlights important pixels in an image, which when changed will maximize the effect on the output. In other work, the authors trained a sparse model in locality of a particular point to offer explanation to the decisions of the original model. Explanation by Examples: Reporting examples which the model consider similar to one under observation. This can be achieved by finding the nearest neighbours in the learned space of the model. Linear models might seem to be more interpretable than deep learning ones, however they lose simulatability and decomposability with increasing number of dimensions or complexity of features. Deep architectures work on raw features, and thus offer better and more intuitive post-hoc explanations. Limited Data \u00b6 One of the key arguments here is that humans can learn from very small amount of data. However, deep learning method based AutoML methods and libraries require huge amounts of training data to compute the results. Thus, one of the more apparent limitations of the key AutoML libraries is scenario modelling for the performance of existing models on these particular tasks depend upon the availability of relevant data. This limitation can also be explained by No-Free Lunch Theorem that suggests that no single algorithm can model and provide good predictions on all kinds of data. One of the methods used in the cases of limited data is probablistic inferencing models that have used to explore compositionality. While compostitional models are yet to match the performance of deep learning based models however one of the AutoML projects developed using this core idea of compositionality is called Automatic Statistician. The main problem for such methods would be computational complexity and the high amount of computational resources required to make predictions. Thus, although theoretically robust, however are also non-scalable. Linear Regression particularly suffers from lack of data. Relationships learnt from limited data may not portray the accurate trends. For example, the model may learn faster or slower growth in trends than what it actually should be because of lack of sufficient data points. In the paper Linear Regression with Limited Observation, compared the performance of various regression variants when only a subset of attributes were available. They discovered that while Lasso and Ridge regression required the same number of total attributes to achieve a certain accuracy, SVM Regressors could achieve that with exponentially less attributes. Many real world scenarios (social media, citation graphs, the Internet etc.) can be visualized in the form of graphs. Two challenges in performing tasks like classification on a graph includes - 1. Absence of labels for many nodes 2. Large computation time for the graph convolutions involved in spectral graph theory. A graph G = (V, E) can be represented using an adjacency matrix A. The input features for the graph can be represented as a N x D matrix, where N are the number of Nodes, and each node had D input features. Each node is associated with an output Z. Thus, a neural network of the graph G can be summarized as: H(l+1) = f(H(l), A) where input to the first row is the feature matrix and output corresponds to the labels. Kipf & Welling proposed a fast layer wise propagation rule which ensured the inclusion of the feature vector for the node in consideration, as well as normalized the adjacency matrix. The proposed rule is also analogous to differentiable, parameterized form of Weisfeiler-Lehman algorithm, therefore assigning features which captures the role of nodes in the graph. Training this model on a graph, where only a single node is labelled corresponding to each class, the proposed model is able to linearly separate nodes into various classes. The drawback of this approach is the possibility of the proposed approach to underperform on regular graphs. Missing data poses another problem to learning models which do not overfit the training data. Missing data can be categorized into 3 classes - Missing Completely at Random (MCAR), Missing Not at Random (MNAR) and Missing at Random (MAR). In MAR, the missing data distribution depends on the observed values. MCAR, is a special case where the missing value distribution depends on both observed and missing data. In MNAR, the distribution is independent of observed as well as missing values. Some basic approaches for missing data imputation include Mean Imputation, Gaussian Random Imputation and Expected Maximization Imputation. As their name suggests, Mean Imputation replaces the missing value of the attribute with the mean of available values and Expected Maximization Imputation estimates the missing variable sum and maximizes their mean and covariance using the estimated sum. Gaussian Random Imputation replaces missing values with \\(\\sigma * z + \\nu\\) where z is a uniformly distributed random standard normal variate and \\(\\sigma\\) and \\(\\nu\\) are standard deviation and mean of the attribute with missing values. In another work, ensemble methods were used to improve missing value imputation. In Bagging Simple Method, the dataset was resampled and missing values in each dataset was imputed using average of values generated by multiple imputation. After that, a decision tree was constructed for each sample, and their majority vote was used as the classification prediction. In Bagging Multiple Method, an individual decision tree was built for result of each of the dataset generated by multiple imputation. In MI Ensemble, Multiple Imputation is applied on the dataset multiple times, and majority vote of the classifier built on the top of each complete dataset is used. These methods proved to be robust even in scenarios with high percentage of missing data. For handling image datasets with missing values, variations of GANs and VAEs have been proposed. CollaGAN was proposed which uses a single generator and discriminator network to estimate missing data by looking at the problem of image imputation as multi domain image translation task, and therefore being more memory efficient than other image translation GAN architectures such as CycleGAN. Comparing the performance of modified Generative Adversarial Networks (GAIN) and Variational Autoencoders for missing data imputation, it is observed that GAIN performs better than VAE for smaller datasets with missing values of numerical attributes but worse on larger ones. VAE tend to be more stable than GAIN, even in cases where they perform worse. In another approach, the missing values weren\u2019t directly imputed. Instead the missing data was fed to a neural network, and the missing data uncertainty is modeled using probabilistic functions with parametric density, like GMMs, which is learnt with the remaining neural network parameters. Such an approach can work with incomplete datasets, without the need of accurate output for training. Catastrophic Forgetting \u00b6 Human beings are capable of learning sequentially, where knowledge regarding a new domain doesn't wipe out previously acquired information. Neural Networks, on the other hand, traditionally suffer from a phenomenon called catastrophic forgetting where adaptation over a new task completely wipes out the \"memory\" of the problem they were previously solving. Lately, there has been some work which promises to overcome catastrophic forgetting and allow networks to learn sequentially. Intuitively, the most basic way is to keep predictions over old task unchanged when training for newer ones. This, however limits the task distributions. Other way is to ensure that the model parameters always remain in the neighbourhood of optimal parameters achieved for previous iterations. This requires a large number of parameters to be stored. Later, we overcame these disadvantages by training undercomplete autoencoders to preserve feature information important to various tasks. Deepmind came up with Elastic Weight Consolidation where knowledge is preserved by reducing weight plasticity, where parameter values are drawn back to their older values proportional to their degree of importance for the previous task. EWC, however is sensitive to diagonal approximation of Fischer Information Matrix. Another related work proposed reparametrization of the layers by rotating FIM in the directions which are less prone to forgetting. This resulted in more compact and diagonal FIM. SupportNet uses SVMs to recognize \"support data\" which, along with the data for the newer tasks is fed to the neural networks for adapting to newer tasks while retaining information about the older ones. In a related work, the authors used incremental moment matching along with transfer learning techniques like weight transfer and a variant of dropout which boosted the performance of IMM. Some researchers later used an application oriented view for incremental learning to evaluate the performance of various approaches to cater to catastrophic forgetting. They concluded that dropout doesn't particularly help and EWC, though partly effective still doesn't perform up to the mark. Towards AGI \u00b6 Current Machine Learning paradigms have achieved human-liker performaces in various domains such as games like Go. However, a very important domain where these models lack the human-like performance is to perform sufficiently well in multiple tasks. Meta Learning, or learning to learn focuses on bridging this gap in Artificial Intelligence. Such models are introduced to numerous tasks, and then they are evaluated on how efficiently they can learn a new one. Meta Learning tasks can be classified into 3 approaches -: Using RNNs as base learners, where dataset is processed sequentially and using gradient descent to train the meta learner. Using a meta-learned metric space, where the base learner follows a comparison scheme and meta learner uses gradient descent. To learn an optimizer which updates the base learner to efficiently learn a new task. Such a meta-learner is usually an RNN, trained using supervised learning or reinforcement learning which remembers how the base learner was previously updated. In a different approach Model Agnostic Meta Learning, the authors proposed a model and task independent mechanism which tunes a model\u2019s parameters in order to quickly adapt to a new problem statement with the help of few gradient updates. The parameters for the new task are calculated by performing a single or multiple gradient descent operations on the original parameters. The original model parameters are then updated using stochastic gradient descent using the loss over new parameters for the new problems sampled from a distribution of tasks. The basic idea behind this approach is to learn features which are applicable to a distribution of tasks, rather than to that of a single task. In spite of the simplicity of MAML,it outperformed various few-shot image classification benchmarks, and therefore brought us one step closer to building more versatile agents. While given the advances in GPUs as well as parallel distribution, several systems are now able to not only compute but also infer the results from past training results. However, one of the key limitations of the widely-accepted methods are their lack of causal inference given these models are trained in a model-blind manner. While there have been several model-based approaches been used in reinforcement learning, however their application to AutoML methods yet remains to be seen and compared against that of model-free methods. One of the key limitations for these methods in AutoML is the ability to capture abstraction in the data. Several approaches in the past have been thus explored to deal with abstraction and hierarchy in the data namely Fuzzy Set Theory, Rough Set Theory and Quotient Space Based Models. Christoph Molnar emphasized on understanding how a machine learning model comes up with a prediction, in addition to what the prediction was. The basis of this hypothesis lies closely with human curiosity and our quest to find meaning in life. The study is divided into 2 parts - Interpretable Models and Model Agnostic Methods for interpretability. Linear models like Linear Regression and Logistic Regression are inherently interpretable models. However, these models suffer from dependence on handcrafted non-linearities and lower predictive performance. Other models including Decision Trees, Decision Rules etc. also fall under the category of interpretable models where importance of each feature can be computed. However these models struggle to accurately model linear correlations. Model agnostic methods focus on developing stand-alone techniques to interpret the results of a machine learning model. The model is treated as a black box, whose parameters, weights or even architecture is unknown. Certain types of agnostic methods try to model the relationship between the features and the output of the ML model. These methods are easily understandable and intuitive to human beings. However, these methods rely on the assumption of independence among the input features, which is not true for many real world scenarios. Another class of methods focuses on the input features. These methods include the study the meaningful but computationally expensive interaction among the features or studying how the model error changes with permutation in the input feature values. The latter method provides a global overview of the model, but is prone to outliers. The third class of model agnostic methods comprise of surrogate models. The global surrogate method uses the predictions of a black box model for the input data to train an inherently interpretable model, which is then used to carry out the interpretability study. The Local Surrogate Method (LIME) generates permuted data samples for the sample of interest, along with the black-box model\u2019s outcomes for these new data points. It then trains an interpretable model over this newly generate dataset where each input sample\u2019s weight is based upon its distance from the original sample. These methods are flexible and can be used over any type of dataset. However the performance of these methods depend upon the choice and performance of the surrogate interpretable models.","title":"AutoML - An Overview"},{"location":"machine-learning/posts/automl-problems-2018/#why-deep-learning-for-automl","text":"While AutoML is not simply limited to deep learning methods, classical machine learning becomes limited by the need for simpler rules and features. The key foundation for deep learning is compositionality, which means it is possible to construct a task from a series of sub-tasks. For most of the tasks we can divide them into two categories - classification problems and regression problems. Deep Learning has shown excellent promise on classification problems. And one could argue that one of the key contributors for this success on classification problem is the convolutional neural networks. Convolutional Neural Networks consist of multiple layers and are designed to require relatively little pre-processing compared to other classification algorithms. They learn by using filters and applying them to the images. The algorithm takes a small square (or \u2018window\u2019) and starts applying it over the image. This mathematical process is called convolution. Each filter allows the CNN to identify certain patterns in the image. The CNN looks for parts of the image where a filter matches the contents of the image. The first few layers of the network may detect simple features like lines, circles, edges. In each layer, the network is able to combine these findings and continually learn more complex concepts as we go deeper and deeper into the layers of the Neural Network. Thus, deep learning essentially allows us to map complex features from inputs to a classification. For this complex feature engineering, the number of available choices of algorithms, learning methods and packages for machine learning have quadrupled in the past decade. The algorithm choice space ranges from CNNs, LSTMs to GANs. Learning methods ranging from K-means clustering, PCA (Principal Component Analysis), Bayesian Optimization to Q-Learning. The number of packages ranging from Tensorflow, Scikit-Learn, PyTorch, etc. When doing AutoML these days, the performance is hugely dependent on the design decision of the model. These design decisions include - algorithm selection and hyperparameter optimization. Usually, this optimization process for any selected algorithm is repeated via hit and trial until we find a good set of choices for each dataset. This problem has been dubbed as CASH (Combined Algorithm Selection and Hyperparameter Optimization) and can present a combinatorial number of choices for any user. For any non-expert, this often results in choosing an algorithm based on either reputation or intuitive appeal and more often than not, leaving the hyperparameter settings to default thus resulting in less than optimal performance for that particular selection of models. Lately, Tensorflow has helped make this process slightly easier by allowing the users a visual representation of the process and an almost plug and play like interface for optimization settings, however the choice of algorithm selection and learning method selection is still heavily user-dependent.","title":"Why Deep Learning for AutoML?"},{"location":"machine-learning/posts/automl-problems-2018/#history","text":"The goal of AutoML is largely to build autonomous agents that can take human out of the loop for choosing and designing learning algorithms. This can potentially mean huge gains in the performance of machine learning algorithms by tailoring them to the problem at hand, reducing human induced bias as well as improving the improving the reproducibility and fairness of the scientific studies. We roughly divide this section in two parts- the Model Selection where we discuss the problem of architecture design (also known as Search Space) and Learning Policy Selection, and Model Optimization where we discuss the problem of optimization choices made while training the model including hyperparameters, ensembles and regularizers. In this section, we will go through the reasoning behind some of these decisions. For image classification, various architectures have been proposed over the years. The convolution neural network architecture was first proposed in LeNet-5, which consisted of 6 layers consisting of convolution, average pooling and hidden layers. However, because of low computational resources, convolutional architectures next came into attention in 2012 with the first fast implementation of CNNs on GPU, AlexNet. AlexNet employed 8 layers, including 5 convolutional and 3 fully connected layers using the ReLU activation function. The authors highlighted the gains in performance using ReLU over tanh and sigmoid activation functions. Dropout was used in the fully connected layers to prevent overfitting. In 2014, VGGNet was proposed which consists of 16 convolution and three fully connected layers employing only 3x3 convolutions. Next, GoogleNet was proposed which consisted of 22 layers. The main intuition behind the model was to make it compatible to be used on a smartphone with 12 times lesser parameters than AlexNet. It employed inception modules, which convolve parallely using different sizes of filters. Next, in 2015, ResNet was proposed which introduced skip-connections between the layers. These skip connections allowed a 152 layer architecture to be trained, while still having complexity lesser than that of VGGNet. Thus the advances in image classifications can be attributed to two main features- Increasing the number of layers Introduction of operations/connections which reduces the amount of parameters to be trained. However, the process of defining the number of layers is still controlled majorly by human beings and the hyperparameters are hand-tuned, therefore requiring a lot of time to be invested. The next step was to model these techniques automatically, thereby leading to the introduction of AutoML.","title":"History"},{"location":"machine-learning/posts/automl-problems-2018/#what-is-automl","text":"To define the problem of AutoML, In the past five years, various AutoML methods have been proposed. Introduced in 2013, Auto-WEKA was the first AutoML method and used Bayesian Optimization to automatically instantiate a highly parametric machine learning framework. Parametric models assume some finite set of parameters \\(\\theta\\) . Given the parameters \\(\\theta\\) , future predictions, x, are independent of the observed data, D \\[ P(x|\\theta, D) = P(x|\\theta) \\] It is based upon the assumption that \\(\\theta\\) captures everything there is to know about the data and thus the complexity of the model is bounded even if the amount of data is unbounded. While these parametric methods are simpler, faster, interpretable and can be trained with less data but they pose flexibility constraints while struggling with complex data generating a poor fit for the real world problems. It was further extended as a package called Hyperopt-sklearn in 2014 to non-parametric methods like probability calibration, density estimation provided by Scikit-Learn. One of the key achievements of Hyperopt-sklearn is an ability to remove redundant features, or reduce data dimensionality using SVM or PCA so the classification task can become simpler. While one possible direction post this could be extending the framework to support more machine learning algorithms or extending the number of pre-processing methods however the ability to select a good algorithm and a combination of the right hyperparameters is a problem that lies at the heart of AutoML. Introduced in 2015, Auto-sklearn approached the problem by taking into account past performance on similar datasets, or constructing ensembles from the models evaluated during the optimization. While budget and computational power being the obvious constraints for such methods, one not-so-obvious limitation is that testing hundreds of hyperparameter optimization configuration increases the risk of overfitting to the validation set. Aside from this, several incremental developments have been made in the field of architecture selection, hyperparameter optimization and meta-learning using both gradient based as well as gradient-free methods.","title":"What is AutoML?"},{"location":"machine-learning/posts/automl-problems-2018/#gradient-free-techniques","text":"The most intuitive gradient-free technique is random search i.e. randomly searching the state-space domain to find optimal values. However, since the state space is multi-dimensional, therefore, leading to suboptimal results using random search. Population based methods such as Genetic Algorithms and Particle Swarm Optimization have been employed in order to search the state-space more effectively. Other gradient free techniques include Bayesian Optimization by modelling the loss of a configuration, where the improvement of a configuration is proportional to the distance between the best-so-far loss and the predicted mean or using Tree Based Estimator like MCTS or using surrogate models for instance Gaussian processes. Q-learning has also been used in order to learn desirable architectures, where a reinforcement agent chooses components of an architecture sequentially with the reward equal to the validation accuracy.","title":"Gradient Free Techniques"},{"location":"machine-learning/posts/automl-problems-2018/#gradient-based-techniques","text":"In the past few years, deep learning methods have shown immense potential in the area of AutoML. One such key project is Auto-Net which uses Bayesian Optimization method SMAC and the deep learning library Lasagne, later followed by Auto-Net 2.0 that uses a combination of Bayesian Optimization and HyperBand, called BHOB, and uses PyTorch as the key deep learning library. Auto-Net provides automatically-tuned feed-forward neural networks without any human intervention. BHOB uses repeated runs of successive halving to invest most runtime in promising neural networks and stops training neural networks with poor performance early and using kernel density estimator to describe regions of high performance in the space of neural network. One of its key advantages is that this method is highly parallelizable achieving almost linear speedups with an increasing number of workers. Auto-Net performance on three different datasets - As observed, AutoNet performs best twice and comparatively well for yolanda as well. As the No Free Lunch theorem goes, any one algorithm that searches for an optimal cost or fitness solution is not universally superior to any other algorithm given that we are optimizing for fit on a limited class of problems thus it may or may not generalize in an infinite possibilities dataset. Thus gradient based neural networks started to focus on building a model from existing class of models instead of random search on a particular number of configurations. In 2016, the focus shifted from searching within specified parameters to instead the use of modular approach to find the best model and combination of hyperparameters.","title":"Gradient Based Techniques"},{"location":"machine-learning/posts/automl-problems-2018/#model-selection-architecture-design-and-learning-policy-selection","text":"In 2016, MetaQNN was introduced to generate CNN Architectures for a given task using Q-Learning, where the agent chooses the architecture layers sequentially, via the epsilon-greedy strategy, with reward equal to the validation accuracy. Next, the problem of generating a high-performing architecture was dealt by using a meta-controller. A meta controller models state information and directs how the search space is to be navigated in order to find the optimal architecture. Traditionally, RNNs and LSTMs are used as metacontrollers, encoding the state space information. The state space consists of various blocks performing operations like convolution and pooling, and the meta-controller, say RNN, arranges these blocks in an end-to-end fashion. The controller is then updated based upon the validation accuracies to produce a better performing architecture in the next iterations. Another new approach in 2016, was the creation of NAS. The intuition behind this was since a single model does not generalize well for different datasets. Let\u2019s try to automate the process of building a system that can adapt to different datasets. This was done by using smaller pieces from the architecture to build a new architecture using a technique called convolutional cells. However, limitations of this approach are it does limit itself to using the same type of controller and same kind of model for every image, which might or might not be optimal. Also, it happens to be computationally expensive. Progressive Nets however were the next iteration. Progressive Nets employ sequential-based model optimization to further optimize the search process. It learns the architecture in increasing complexity, with first learning the cell structure and then stacking those together to form a CNN. The next in line was ENAS i..e efficient neural architecture search. In 2017, Efficient Architecture Search was proposed to reuse the weights of pre-existing architectures using a Reinforcement Agent as a meta-controller. The architecture is encoded into a low dimensional representation using a bidirectional RNN with Embedding Layer. The network transformations are produced by an independent actor network which takes in as input the encoded representation. Efficient Neural Architecture Search is the next step to reduce the dependence on computational resources for generating high performance architectures. A policy gradient approach is used to train a controller which can select optimal subgraphs from within a larger graph. It allows the children graphs to share weights, therefore allowing faster convergence. ENAS employs LSTM as a controller. The training procedure consists of alternating between learning the parameters shared by the children and that of the LSTM controller. In addition to the requirement of high computational costs, the performance of network architectures learnt on a specific dataset don\u2019t transfer to other tasks. In order to overcome that, in BlockQNN the authors proposed to generate the architecture block-wise, rather than layer-wise. The optimal block structure is searched using Q-Learning with \\(\\epsilon\\) -greedy strategy and experience relay. Lately, there has been a influx of various AutoML methods like Elastic Architecture Transfer for Accelerating Large-scale Neural Architecture Search (EAT-NAS), Graph HyperNetworks for Neural Architecture Search, Stochastic Neural Architecture Search (SNAS), Instance-aware Neural Architecture Search (Insta-NAS) etc that use several approaches that we will discuss in further sections. One of the major drawbacks of these approaches is the fact that each architecture is trained from scratch, thereby basing their success on external factors like availability of computational resources. However, In the limited computational resource scenario, Bayesian Optimization techniques have emerged to be very successful for their ability to work with combinatorial space problems with limited training time as well as their flexibility with high-dimensional problems and transfer learning problems.","title":"Model Selection - Architecture Design and Learning Policy Selection"},{"location":"machine-learning/posts/automl-problems-2018/#architecture-search-via-bayesian-optimization","text":"Bayesian optimization has emerged as an exciting subfield of machine learning that is concerned with the global optimization of noisy, black-box functions using probabilistic methods. Essentially, Bayesian Optimization is a sequential model-based approach to machine learning. Mathematically, the problem of finding a global maximizer (or minimizer) of an unknown objective function f can be represented as x* = arg max f(x) for x in X\u2019 where X\u2019 is some design space of interest. One of the key advantages of Bayesian Optimization framework is that it can be applied to unusual search spaces that involve conditional or categorical inputs, or even combinatorial search spaces with multiple categorical inputs. The Bayesian Optimization framework has two key ingredients - first, a probabilistic surrogate model which consists of a prior distribution that captures our beliefs about the behaviour of the unknown objective function and an observation model that describes the data generation mechanism and secondly, the loss function that describes how optimal a sequence of queries are. In practice, these loss functions are often expresses as a form of regret, either simply or cumulative which is then minimized to select an optimal sequence of queries. If f is cheap to evaluate we could sample at many points e.g. via grid search, random search or numeric gradient estimation. However, if function evaluation is expensive e.g. tuning hyperparameters of a deep neural network, probe drilling for oil at given geographic coordinates or evaluating the effectiveness of a drug candidate taken from a chemical search space then it is important to minimize the number of samples drawn from the black box function f. This is the domain where Bayesian optimization techniques are most useful. They attempt to find the global optimum in a minimum number of steps. A popular surrogate model for Bayesian optimization are Gaussian processes (GPs). The success of these methods has led to the discovery and development of several acquisition functions including Thompson Sampling, Probability of Improvement, Expected Improvement, Upper Confidence Bounds, Entropy Search etc. These acquisition functions trade off exploration and exploitation such that their optima is located where the uncertainty in the surrogate model is large (exploration) and/or the prediction is high (exploitation). In terms of AutoML, it has been proved through various experiments that often the careful choice of statistical model is often far more important than the choice of acquisition functions. While Bayesian Optimization is already one of the preferred methods for combinatorial search spaces and policy search for both parametric as well as non-parametric models, one of the major applications in AutoML has been the exploration of Bayesian Optimization for Automating Hyperparameter Tuning. In architectural search spaces, Bayesian Optimization methods have received great attention in tuning deep belief networks, Markov Chain Monte Carlo Methods (MCMC), Convolutional Neural Networks and its application in one of the most primitive neural architecture search methods such as SMAC, Auto-SkLearn, AutoWeka, BHOB, REMBO etc. Another key application of Bayesian Optimization has been on multi-task learning and transfer learning, esp in a constrained budget. There have been several attempts to exploit this property within the Bayesian Optimization framework including Collaborative Hyperparameter Tuning, Initializing Bayesian Hyperparameter Optimization via Meta-Learning, Sequential Model-Based Optimization for General Algorithm Configuration, Gaussian Process Bandit Optimization, Freeze-Thaw Optimization, Efficient Transfer Learning via Bayesian Optimization. The key idea here is that there are several correlated functions, \\(\\tau\\) = {1, 2, \u2026.,M}, called tasks and that we are interested in optimizing some subset of these tasks, thus using the data from one task to provide information about another task. One such method of sharing information between tasks has been implementing by modifying the Bayesian Optimization Routine to modify the underlying the Gaussian Process (GP) model called Multi-Output GPs. We define a valid covariance over input and task pairs, k((x,m), (x\u2019, m\u2019)). One of the explored methods to do this is using coregionalization that utilizes the product kernel - \\[ k((x,m), (x\u2019, m\u2019)) = k_{\\chi}(x,x')k_{\\tau}(m,m') \\] Where m, m\u2019 \\(\\in\\) \\(\\tau\\) . The problem of defining a Multi-Output GP can then be viewed as learning a latent function, or a set of latent functions, that can be transformed to produce the output tasks. This can be done by introducing a latent ranking function across different pairs of observations on various tasks such that the tasks are jointly embedded in a single latent space that in variant to potentially different output scales across tasks. The key idea here is to build a joint model with a joint probability distribution function which can then be trained via various methods like Random Forest to learn the similarities between different tasks based on the task features. The best inputs from the most familiar tasks (or models) are then used as the initial design for the new task (or new model) thus creating an automatic architecture space pipeline for working on cross domain machine learning models.","title":"Architecture Search via Bayesian Optimization"},{"location":"machine-learning/posts/automl-problems-2018/#architecture-search-via-evolutionary-methods","text":"An evolutionary algorithm based approach is used to evolve AmoebaNet-A, an architecture for image classification at par with human crafted designs. The proposed approach biases tournament selection with age, thereby preferring younger genomes. The search space consists of directed graphs where hidden states correspond to vertices and ordinary operations like convolution, pooling etc. are associated with edges. Mutation is performed either by random reconnection of edges to other vertices or relabeling randomly selected edges. After initializing the parent population with randomly generated architectures which are trained and evaluated, children architectures are generated using mutation. These architectures are then trained and evaluated, whereas the oldest ones are discarded. The ageing evolution used is analogous to regularization, as the only way good architectures can be retained are when they are inherited by the children. Since at each step, children models are retrained, architectures which constantly perform good, are the ones which are eventually retained, thereby preventing overfitting. In GeneticCNN, Genetic Algorithms are used to come up with Convolutional Neural Network Architectures. A binary string is used to represent each possible network. 0.5 \\(\\Sigma\\) sKs(Ks-1) bits are needed to represent the network with S stages, and each stage having Ks nodes. The bit corresponding to (vs,i, vs,j) is set if there is a connection between the associated nodes. A default input and output node are present at every stage. Isolated nodes do not have a connection to the default nodes. In case of absence of connections at a stage, i.e. all bits are unset, convolution is applied only once. Only convolution and pooling operations are taken into consideration. The initial population consists of randomized strings, where each bit is sampled from Bernoulli Distribution. Individuals are selected using Russian roulette process, where fittest individual has maximum probability of being selected. In mutation, a randomly selected bit is flipped with mutation probability. In crossover between two individuals, corresponding stage pairs are exchanged with crossover probability. Fitness is obtained by training the architectures represented by individuals on a preselected reference dataset. Cartesian Genetic Programming is used to design architecture for Convolutional Neural Network which provides the benefit of flexibility to incorporate skip connections and variable depth networks in the representation. The network is defined as a directed acyclic graph represented using s two dimensional grid consisting of computational vertices. For a grid of Nr rows and Nc columns with fixed number of inputs and outputs, nodes in the nth column have connections from nodes in the range of (n-l) to (n-1) columns. The architecture representation in the genotype have fixed length integers, each chromosome consisting of type and connectivity information regarding that node. The phenotype consists of variable number of nodes because all nodes don\u2019t have a connection to the output. Computational nodes comprises of convolution operation, average pooling, max pooling, summation, concatenation and ResBlock (convolution processing, batch normalization, ReLU, and tensor summation). A (1+ \\(\\lambda\\) ) evolution strategy is used to find the appropropriate architecture where the most fit of the \\(\\lambda\\) children generated at each generation become the parents for the next generation. Children architectures are trained in parallel and fitness is defined as the validation accuracy. The mutation strategy used is to change the type (size and number of filters) or the edge connections at random depending upon the mutation probability. In case of stagnation in fitness of children, mutation is applied to the non functioning nodes in the parent architectures. The proposed approach has an inherent drawback of high computational cost. Another work analyzed the performance of gradient-free Genetic Algorithms on Reinforcement Learning Problems. The population of N individuals comprise of vector parameters for neural networks denoted by \\(\\theta\\) i. On evaluating \\(\\theta\\) i in every generation, a fitness score F( \\(\\theta\\) i) is associated with each individual which is analogous to reward. The most fit individuals become the parents in the next generation. Mutation is performed on randomly selected parent by addition of Gaussian noise in the parameter vector: \\(\\theta'\\) = \\(\\theta\\) + \\(\\sigma_\\epsilon\\) , where \\(\\epsilon\\) ~ N(0,1) and value of \\(\\epsilon\\) is empirically determined in every experiment. The best individual from each generation is retained which is determined by evaluation of the most fit 10 individuals for additional 30 episodes and selecting the one with the highest mean score as the elite individual to be retained. Crossover is not included for simplicity. The process is then repeated for the new generation of individuals. The hyperparameters for all Atari games were selected from a collection of 36 hyperparameters evaluated on 6 games. In order to efficiently implement distributed deep GA, parameter vectors are stored as combination of the initial seed value and the random seeds producing the sequence of mutations from which it is possibly to reconstruct \\(\\theta\\) . This compressed representation is independent of the network size and linearly dependent on the number of generations. It was observed that all the final Atari networks were 8000-50000 times compressible. Another advantage provided by the use of GAs over traditional RL methods is that of increased speed. Another variant of GA which uses Novelty Search in place of Fitness Function was implemented to evaluate the performance of its combination with DNNs on deceptive image based Reinforcement Learning problems. Novelty Search rewards agents for novel behaviour, thereby avoiding local optima. A policy\u2019s novelty is determined by calculating the average behavioral distance to its \\(k(25)\\) nearest neighbours. This fitness function is inspired by Darwin\u2019s Fitness function which suggests that the genetic contribution of an individual to the next generation's gene pool relative to the average for the population, usually measured by the number of offspring or close kin that survive to reproductive age. the ability of a population to maintain or increase its numbers in succeeding generations. Also, another related work derived a new approach where the children are pre-trained from parent networks. This technique is called Lakhmarian Evolution.","title":"Architecture Search via Evolutionary Methods"},{"location":"machine-learning/posts/automl-problems-2018/#architecture-search-via-reinforcement-learning","text":"Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice. Despite their success, these methods are still limited in that they only search models from a fixed-length space. In other words, it is difficult to ask them to generate a variable-length configuration that specifies the structure and connectivity of a network. And then as discussed above, there are Bayesian optimization methods that allow one to search non fixed length architectures, but they are less general and less flexible than reinforcement learning methods which provide a general framework for machine learning. Modern neuro-evolution algorithms, on the other hand, are much more flexible for composing novel models, yet they are usually less practical at a large scale. Their limitations lie in the fact that they are search-based methods,thus they are slow or require many heuristics to work well. One of the key advantages of reinforcement learning methods for architectural space design is that they can learn directly from the reward signal without any supervised bootstrapping, which is an immensely attractive characteristic for learning new tasks without much human interference. The key idea here is to use a neural network to learn the gradient descent updates for another network and then using reinforcement learning to find update policies for other networks. Under this framework, any particular optimization algorithm simply corresponds to a policy. Then, we reward optimization algorithms that converge quickly and penalize those that do not. Learning an optimization algorithm then reduces to finding an optimal policy, which can be solved using any reinforcement learning method. To differentiate the algorithm that performs learning from the algorithm that is learned, we refer to the former as the \u201clearning algorithm\u201d or \u201clearner\u201d and the latter as the \u201cautonomous algorithm\u201d or \u201cpolicy\u201d and use an off-the-shelf reinforcement learning algorithm known as guided policy search. A reinforcement learning problem is typically formally represented as an Markov decision process(MDP). We consider a finite-horizon MDP with continuous state and action spaces defined by the tuple (S,A,p0,p,c, \\(\\gamma\\) ), where S is the set of states, A is the set of actions, \\(p0:S \\rightarrow R+\\) is the probability density over initial states, \\(p:S \\times A \\times S \\rightarrow R+\\) is the transition probability density, that is, the conditional probability density over successor states given the current state and action, \\(c:S \\rightarrow R\\) is a function that maps state to cost and \\(\\gamma \\in (0,1]\\) is the discount factor.The objective is to learn a stochastic policy \\(\\Pi^*\\) : $S \\times A \\rightarrow R+ $, which is a conditional probability density over actions given the current state, such that the expected cumulative cost is minimized. That is, where the expectation is taken with respect to the joint distribution over the sequence of states and actions, often referred to as a trajectory, which has the density This problem of finding the cost-minimizing policy is known as the policy search problem. To enable generalization to unseen states, the policy is typically parameterized and minimization is performed over representable policies. Solving this problem exactly is intractable in all but selected special cases. Therefore, policy search methods generally tackle this problem by solving it approximately.In many practical settings,p, which characterizes the dynamics, is unknown and must therefore be estimated. Additionally, because it is often equally important to minimize cost at earlier and later time steps, we will henceforth focus on the undiscounted setting, i.e. the setting where \\(\\gamma\\) = 1. Guided policy search is a method for performing policy search in continuous state and action spaces under possibly unknown dynamics. It works by alternating between computing a target distribution over trajectories that is encouraged to minimize cost and agree with the current policy and learning parameters of the policy in a standard supervised fashion so that sample trajectories from executing the policy are close to sample trajectories drawn from the target distribution. The target trajectory distribution is computed by iteratively fitting local time-varying linear and quadratic approximations to the (estimated) dynamics and cost respectively and optimizing over a restricted class of linear-Gaussian policies subject to a trust region constraint, which can be solved efficiently in closed form using a dynamic programming algorithm known as linear-quadratic-Gaussian (LQG). At the point of writing, we are not aware of any model-based reinforcement learning methods used to find the optimal search space for automatic neural architecture design.","title":"Architecture Search via Reinforcement Learning"},{"location":"machine-learning/posts/automl-problems-2018/#architecture-search-via-tree-based-methods","text":"Another alternative approach for finding optimal architectures is also what is sometimes referred as Programming by Demonstration. In these methods, the strategy/policy is based upon inputs and outputs which are recorded at very instance which we do guided search using MCTS or Monte-Carlo Tree Search. Also another popular approach used in Markov Decision Monte Carlo Tree Search. While highly interpretable, but this approach has multiple limitations. MCTS works by exploring several single method and optimizing for global optimum which can be computationally expensive. This was explored by studying the performance of MCTS methods in the 2013 paper Investigating the Limits of Monte-Carlo Tree Search Methods in Computer Go.","title":"Architecture Search via Tree Based Methods"},{"location":"machine-learning/posts/automl-problems-2018/#model-optimization-hpo-and-regularization","text":"While the progress in the field of AutoML has followed the step-wise progressive direction of from doing effective feature designing to manual tuning of various hyper-parameters and implementing regularization techniques to architecture designing one of the key contributions of this paper is to instead look at the problem of automatic model creation and hyperparameter tuning in a top-down fashion instead of a bottom-up approach that explores various hacks to produce substantial predictive and computational gains over our datasets. We believe that the problem of automatic model generation lies at the heart of AutoML and other various optimization techniques can instead be used to further the performance of such automatically generated models. One of the key advantages of this approach is the focus on general-use architectures that can adapt well to non-stationary and cross-domain data i.e. transfer learning. In the past two years, transfer learning has managed to effectively adapt to new domains including in Video Classification to Sentiment Analysis in NLP. The biggest advantage of adopting this line of thinking is the idea of reducing human time and effort required in creating ML model pipelines that lies at the heart of AutoML. We believe the bottom-up approach is rather contrarian in increasing the amount of manual human effort and computational efforts required in testing various approaches since designing architectures still requires a lot of expert knowledge and takes ample time. Various ways in which the performance of state-of-the-art architectures can be improved includes inclusion of more diverse data for training, or trying various possibly architectures, both being expensive tasks. In order to reduce model overfitting, regularization is used to penalize the model loss for complex models. The basic intuition is that smaller magnitude of model weights will produce a simpler model, thereby reducing overfitting. Normalization techniques can fasten the training process of deep neural networks. The input distribution for each layer in a network changes with updation in parameters of the previous layer, referred to as Internal Covariance Shift. Batch Normalization reduces this shift by taking mean and variance of each layer as a constant, and integrating normalization in the neural network architecture, thereby accelerating the end-to-end training process. The dependency if gradients on input values and scale is considerably reduced, allowing larger learning rates. It also acts as a regularizer, reducing the need of Dropout. However, simply normalizing inputs of each layer can change the meaning represented by each layer. This is tackled by introducing parameters which scale and shift each activation, and learnt during the training process. Secondly, this model uses mini-batches of the training dataset to estimate mean and variances. Weight Normalization, on the other hand aims to reparametrize weight vectors into a parameterized vector and a scalar parameter, in such a way that the Euclidean norm of the weight vector is equal to the scalar parameter. Thus acceleration in the training process can be attributed to decoupling the weight vector\u2019s norm from its direction. Weight normalization reduces the required overheads and is not dependent on batch size. Another way could be to find the optimal values of hyperparameters in the hyperparameter space which minimizes the loss. The naive approach is to incorporate grid search, which evaluates all possible combinations of hyperparameters. However, this isn\u2019t feasible in real-time, because hyperparameter space is multi-dimensional with infinitely many possible values for the numerical hyperparameters. The next step to perform random search which tests random combinations of hyperparameters within the limits of resources or until a desirable performance is achieved. The success of random search lies in the fact that there are only a few hyperparameters which affect the model accuracy, thereby reducing the apparent dimensionality of the search space. However, the importance of hyperparameters isn\u2019t known beforehand. The next step, intuitively, is to learn the impact of the hyperparameters and accordingly pay attention to those. Bayesian Optimization, thus builds a probabilistic loss-model, updating it sequentially with the newer combinations being tried out, which are selected by the acquisition function. Other sequential model based algorithms employs Tree Parzen Estimators and Random Forests. However these approaches lack a method of early termination for poorly performing hyperparameter combinations. Bertrand H. et. al. proposed an approach to fine tune hyperparameters for a machine learning model using an ensemble of two complementary approaches - Bayesian Optimization and Hyperband. Hyperband selects and trains a range of models in the hyperparameter domain, while removing the models with the worst performing models after a certain time spent in training. This step is repeated until only a few combinations are left. Bayesian Optimization invests time on training bad models, while Hyperband doesn\u2019t result in improvement in the quality of selected model with time. Combining these two approaches can overcome both of these problems. Initial configurations are selected uniformly and evaluated using Hyperband, after which subsequent selections are a result of training Gaussian Process. After this, the next set of configurations is sampled from the probability distribution resulting from the normalized expected improvement of un-evaluated configurations. An alternative approach to hyperparameter optimization is to use gradient based approaches. It uses two contrasting approaches to calculate hypergradient in Reverse and Forward Mode. In the reverse mode formulation, the computation is similar to that of backpropagation through time. The forward mode of hypergradient computation relies on chain rule for computing derivative in order to compute the gradient w.r.t hyperparameters. Forward mode calculation is particularly efficient when the hyperparameter space is much smaller than that of the parameters. In another approach, we aim to overcome the constraint of limited memory when computing gradient w.r.t hyperparameters while optimizing the same on validation set. This is achieved by a procedure which is exactly the reverse of stochastic gradient descent with momentum, which allows to compute gradient of the function of the final trained weights w.r.t. learning rate, initial weight configuration, momentum schedule and any other hyperparameter which may be involved with the time complexity equal to that of forward SGD. However, because of finite precision, this computation results in loss of important information as momentum schedule \\(\\gamma < 1\\) , therefore discarding bits at every multiplication step. This can be overcome by saving the discarded information at each step, and with \\(\\gamma = 0.9\\) , less than one bit per average needs to be stored.","title":"Model Optimization- HPO and Regularization"},{"location":"machine-learning/posts/automl-problems-2018/#pipeline-optimization-for-automl","text":"While machine learning is devised to become a tool that could generate classifications or predictions directly from the data itself however as of now deploying these machine learning models end-to-end is still a challenge that\u2019s currently tackled in part with a lot of intervention from humans at various stages. The key steps in training a machine learning algorithm range from pre-processing to feature selection to prediction. While still an unsolved problem but in 2016, TPOT was created as an automated pipeline design optimization method for AutoML. TPOT works as an architecture with three kinds of operators where each operator corresponds to a machine learning algorithm. Multiple copies of the entire dataset enter the pipeline for analysis. These various copies are then passed over to various types of feature processing operators are available in TPOT including StandardScalar, RobustScalar, RandomizedPCA, Polynomial Features etc that modify the dataset in some way thus returning the modified dataset. Later, these all features from multiple copies are then combined and best features are selected via Feature Selection Operators including SelectKBest, SelectFwe, Recursive Feature Elimination etc. The goal of this step is to reduce the number of features so as pick only the best features. However, this can also cause leaks and overfitting in automated pipelines. Finally, the selected features are sent to Supervised Classification Operators that include Decision Tree, Random Forest, Extreme Gradient Boosting (XGBoost), Logistic Regression and K-Nearest Neighbour Classifier(KNN) which stores the classifier's predictions as a new feature as well as produce classifications for the pipeline.","title":"Pipeline Optimization for AutoML"},{"location":"machine-learning/posts/automl-problems-2018/#problems-and-challenges-in-automl","text":"","title":"PROBLEMS AND CHALLENGES in AutoML"},{"location":"machine-learning/posts/automl-problems-2018/#cost-sensitivity","text":"Building an AutoML system that looks for all the variations of hyperparameter settings, all learning policies can be a very costly endeavour. Also, this does not favour how natural intelligence works. Argument 1: During meta-learning, the model is trained to learn tasks in the meta-training set. There are two optimizations at play \u2013 the learner, which learns new tasks, and the meta-learner, which trains the learner. Methods for meta-learning have typically fallen into one of three categories: recurrent models, metric learning and learning optimizers.","title":"Cost Sensitivity"},{"location":"machine-learning/posts/automl-problems-2018/#model-based-methods","text":"The basic idea behind model based methods is fast learning through rapid parameter updation. One way is to feed the input sequentially to a recurrent model which learns the sequence representation to adapt to newer tasks. These RNN models ingest their own weights as additional inputs to identify their errors and update their own weights for better generalization on newer tasks. Memory Augmented Neural Networks are a family of architectures which learn how to quickly encode previously unseen information and thus learn new tasks after seeing limited samples. A Neural Turing Machine combines a controller with external storage buffer and learns to perform read and write operations in memory using soft attention. The label for an input was presented with an offset of 1 timestamp, which allowed the network to learn to keep the data in memory until the label is presented later and retrieval of earlier information for making predictions. The reading mechanism incorporates content similarity where the cosine similarity between the input feature and the memory rows is computed to produce a read vector which is the weighted sum of memory rows. For the write operation, Least Recently Used Access (LRUA) writer is designed in which the write weight vector is an interpolation between previous least used weight vector and previous read weight vector. The intuition lies in the idea of preserving frequently used information and the fact that recently retrieved information won\u2019t be used again for a while. MetaNets are used for rapid generalization across various tasks. One neural network is used to predict weights of another neural network and these weights are known as fast weights. Weights learnt using stochastic gradient descent, on the other hand are termed as slow weights. MetaNets combine slow and fast weights. MetaNets consist of an embedding function for learning the input representation and a base learner model which performs the actual learning task. A LSTM network is then used to learn the fast weights for embedding function and a neural network is incorporated to learn the fast weights for base learner from their loss gradients. Thus, here we can see that MetaNets effectively have to learn 4 sets of model parameters. Thus model based methods effectively design a fast learning mechanism, where the models rapidly update their parameters using their own architecture or by incorporating additional meta learning model.","title":"Model Based Methods"},{"location":"machine-learning/posts/automl-problems-2018/#metric-learning","text":"The metric learning based approaches are analogous to kernel density estimation and nearest neighbours estimation, where the predicted probability for an output label for a data point is equal to the weighted sum of labels for other data points in training sample, where the weight signifies the similarity between the 2 samples. Koch, Zemel & Salakhutdinov used Siamese Networks to learn embeddings for a pair of images to check if they belong to the same output class. The advantage of this approach lies in its ability to measure similarity between images of unknown classes, but its utility decreases with divergence in tasks from the original training perspective. Relation Networks are similar to Siamese networks but they captured the correlation between an image and support set by incorporating a CNN classifier rather than using a distance metric. It also used MSE Loss in place of cross entropy, because intuitively, it outputs a relationship score, rather than a classification label. In another approach - Matching Networks, incorporated attention networks which measured cosine similarity between the embedding for a test sample and corresponding support set images. Since the performance of Matching Networks depends upon the quality of the embeddings learnt, a Bidirectional LSTM was trained over the entire support set for a class label to learn the embeddings. For the test sample, an LSTM incorporating attention with the support set as hidden state was used to learn effective embeddings. Prototypical Networks learns a prototype feature vector for each class, which is defined as mean over feature vectors learnt for each sample in the support set. For a test sample, the output class can be found by taking softmax over inverse-distances with the prototype vectors. The major task, therefore, in Metric based methods, is therefore to learn the kernel function, which in turn depends upon learning the embedding function for the test sample and the samples in support set.","title":"Metric Learning"},{"location":"machine-learning/posts/automl-problems-2018/#optimization-based-learning","text":"Gradient based methods aren\u2019t suitable for learning from a small training dataset or converging in a few steps. One idea is to use an additional meta learner to update the weights of the model at hand in a small number of steps. An LSTM Meta Learning provides the following advantages: Similarity between cell state update and gradient based update Analogy between storing history of gradients in momentum based update and memory in LSTM Parameter sharing across coordinates is incorporated to compress the parameter space for the meta learner In Model Agnostic Meta-Learning, the parameters for the new task are calculated by performing a single or multiple gradient descent operations on the original parameters. The original model parameters are then updated using stochastic gradient descent using the loss over new parameters for the new problems sampled from a distribution of tasks. The basic idea behind this approach is to learn features which are applicable to a distribution of tasks, rather than to that of a single task. Meta Learning Approaches, therefore aim at increasing the generalization of a model across tasks, without re-training the entire network, and therefore reducing the cost of learning. Model Based Methods utilize inherent or additional memory for fast updation of model parameters. Metric Learning, on the other hand rely majorly on effective representation learning. Optimization Methods replace gradient based methods to generalize the model across tasks with a smaller dataset in hand or in a fewer number of steps.","title":"Optimization Based Learning"},{"location":"machine-learning/posts/automl-problems-2018/#curse-of-dimensionality","text":"Curse of Dimensionality refers to the problem of exponentially increasing data configurations with a linear increase in dimensions. The statistical challenge lies in the small number of available training tuples available compared to the large number of possible configurations. With increasing dimensions, there is a possibility for the lack of training examples for a given configuration of dimensional values. Machine Learning Algorithms using the nearest neighbour information to perform a task will fail in such a case. Natural intelligence is multidimensional (Gardner, 2011) and given the complexity of the world, generalized artificial intelligence will necessarily be multi-dimensional as well. The most important property of deep learning is that deep neural networks can automatically find compact low-dimensional representations(features) of high-dimensional data (e.g., images, text and audio). Through crafting inductive biases into neural network architectures, particularly that of hierarchical representations,machine learning practitioners have made effective progress in addressing the curse of dimensionality [Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation Learning: A Review and New Perspective] Principal Component Analysis, the most commonly used dimensionality reduction technique maps the data to a lower dimension, such that the resulting variance of the data is maximized. The basic objective is to capture most of the variance in the original data, while reducing the number of dimensions used to represent it. Autoencoders can also be used to perform dimensionality reduction, where they learn a reduced representation in the reduction phase, and then goes on to reconstruct the original data tuple in the reconstruction phase.","title":"Curse of Dimensionality"},{"location":"machine-learning/posts/automl-problems-2018/#interpretability","text":"Deep neural networks tend to perform better than the shallow networks, but is it certain that the improved performance can be attributed to the increased depth of a network? In a related paper, the authors mimicked deep networks with shallow ones by training shallow networks not on the original labels, but on the ones predicted by the deep network. The authors observed that shallow networks at par with the deep networks on CIFAR-10 image recognition and TIMIT phoneme recognition, allowing us to ponder upon the correlation between the training procedures used and the success of deep neural networks. It also opens up room to come up with novel approaches to train a shallow network more accurately. According to the No Free Lunch Theorem for Optimisation: If an algorithm performs better than random search on some class of problems then in must perform worse than random search on the remaining problems. In other words, the NFLT states that no black box strategy is superior to another one. Another point to consider is the tradeoff between generality and specificity of an approach. The need for defining interpretability arises from the gap between objectives in a supervised learning environment and the actual deployment costs. The training procedure only requires the predicted and actual labels for optimization. The problem arises when the real world problem cannot be translated into a mathematical function or because of the difference between the deployment and training settings. Interpretations thus cater to the important but difficult to formalize objectives. The objectives of interpretability are: Trust: Does the model mimic humans, i.e. performing accurately where humans perform accurately and committing mistakes in areas where humans also struggle? Are we comfortable giving the model the entire control without human supervision? Causality: Can the associations generated by models be tested experimentally? Transferability: While supervised learning algorithms are evaluated based upon their performance on the test set, independent of the training data, humans have far more capabilities of generalizing their learnings. This is evident from the failure of machine learning models in adversarial settings. Informativeness: To provide more information to human decision makers, rather than simply outputting a single value. Fair and Ethical Decision-Making: Conventional evaluation metrics do not ensure acceptability. Algorithmic decisions must be supported with reasoning and should offer a way to be contested and modified if proven to be wrong. The paper also expanded on the properties of an interpretable model grouping it under 2 categories: transparency and post-hoc explanations. Transparency : It can be defined at 3 levels:- At the level of the entire model - Simulatability: Allowing a human to combine input data, along with model parameters and be able to carry out all the computations to produce the output in reasonable time. But how much is \u201creasonable\u201d amount of time? Are high dimensional linear models, cumbersome rule based systems or huge decision trees interpretable - No. What about a compact neural network? At the level of individual components - Decomposability: Offering an intuitive explanation to each input, parameter and calculation. Associating descriptions with each node in decision trees or using parameters of a linear model to represent the association strength between features and labels. At the level of algorithm used - Algorithmic Transparency: A linear model will converge to a unique point even for a new dataset. This however cannot be guaranteed for deep learning mechanisms. Interpretability : Offer useful information to humans in the form of: Text Explanations: Train a model to produce an output and a separate model to produce an explanation, mimicking the verbal descriptions of humans. Visualizations: Qualitatively represent the model\u2019s learnings in form of visualizations. Local Explanations: Offering local explanations to the decisions of a neural network. A popular example is that of a saliency map which highlights important pixels in an image, which when changed will maximize the effect on the output. In other work, the authors trained a sparse model in locality of a particular point to offer explanation to the decisions of the original model. Explanation by Examples: Reporting examples which the model consider similar to one under observation. This can be achieved by finding the nearest neighbours in the learned space of the model. Linear models might seem to be more interpretable than deep learning ones, however they lose simulatability and decomposability with increasing number of dimensions or complexity of features. Deep architectures work on raw features, and thus offer better and more intuitive post-hoc explanations.","title":"Interpretability"},{"location":"machine-learning/posts/automl-problems-2018/#limited-data","text":"One of the key arguments here is that humans can learn from very small amount of data. However, deep learning method based AutoML methods and libraries require huge amounts of training data to compute the results. Thus, one of the more apparent limitations of the key AutoML libraries is scenario modelling for the performance of existing models on these particular tasks depend upon the availability of relevant data. This limitation can also be explained by No-Free Lunch Theorem that suggests that no single algorithm can model and provide good predictions on all kinds of data. One of the methods used in the cases of limited data is probablistic inferencing models that have used to explore compositionality. While compostitional models are yet to match the performance of deep learning based models however one of the AutoML projects developed using this core idea of compositionality is called Automatic Statistician. The main problem for such methods would be computational complexity and the high amount of computational resources required to make predictions. Thus, although theoretically robust, however are also non-scalable. Linear Regression particularly suffers from lack of data. Relationships learnt from limited data may not portray the accurate trends. For example, the model may learn faster or slower growth in trends than what it actually should be because of lack of sufficient data points. In the paper Linear Regression with Limited Observation, compared the performance of various regression variants when only a subset of attributes were available. They discovered that while Lasso and Ridge regression required the same number of total attributes to achieve a certain accuracy, SVM Regressors could achieve that with exponentially less attributes. Many real world scenarios (social media, citation graphs, the Internet etc.) can be visualized in the form of graphs. Two challenges in performing tasks like classification on a graph includes - 1. Absence of labels for many nodes 2. Large computation time for the graph convolutions involved in spectral graph theory. A graph G = (V, E) can be represented using an adjacency matrix A. The input features for the graph can be represented as a N x D matrix, where N are the number of Nodes, and each node had D input features. Each node is associated with an output Z. Thus, a neural network of the graph G can be summarized as: H(l+1) = f(H(l), A) where input to the first row is the feature matrix and output corresponds to the labels. Kipf & Welling proposed a fast layer wise propagation rule which ensured the inclusion of the feature vector for the node in consideration, as well as normalized the adjacency matrix. The proposed rule is also analogous to differentiable, parameterized form of Weisfeiler-Lehman algorithm, therefore assigning features which captures the role of nodes in the graph. Training this model on a graph, where only a single node is labelled corresponding to each class, the proposed model is able to linearly separate nodes into various classes. The drawback of this approach is the possibility of the proposed approach to underperform on regular graphs. Missing data poses another problem to learning models which do not overfit the training data. Missing data can be categorized into 3 classes - Missing Completely at Random (MCAR), Missing Not at Random (MNAR) and Missing at Random (MAR). In MAR, the missing data distribution depends on the observed values. MCAR, is a special case where the missing value distribution depends on both observed and missing data. In MNAR, the distribution is independent of observed as well as missing values. Some basic approaches for missing data imputation include Mean Imputation, Gaussian Random Imputation and Expected Maximization Imputation. As their name suggests, Mean Imputation replaces the missing value of the attribute with the mean of available values and Expected Maximization Imputation estimates the missing variable sum and maximizes their mean and covariance using the estimated sum. Gaussian Random Imputation replaces missing values with \\(\\sigma * z + \\nu\\) where z is a uniformly distributed random standard normal variate and \\(\\sigma\\) and \\(\\nu\\) are standard deviation and mean of the attribute with missing values. In another work, ensemble methods were used to improve missing value imputation. In Bagging Simple Method, the dataset was resampled and missing values in each dataset was imputed using average of values generated by multiple imputation. After that, a decision tree was constructed for each sample, and their majority vote was used as the classification prediction. In Bagging Multiple Method, an individual decision tree was built for result of each of the dataset generated by multiple imputation. In MI Ensemble, Multiple Imputation is applied on the dataset multiple times, and majority vote of the classifier built on the top of each complete dataset is used. These methods proved to be robust even in scenarios with high percentage of missing data. For handling image datasets with missing values, variations of GANs and VAEs have been proposed. CollaGAN was proposed which uses a single generator and discriminator network to estimate missing data by looking at the problem of image imputation as multi domain image translation task, and therefore being more memory efficient than other image translation GAN architectures such as CycleGAN. Comparing the performance of modified Generative Adversarial Networks (GAIN) and Variational Autoencoders for missing data imputation, it is observed that GAIN performs better than VAE for smaller datasets with missing values of numerical attributes but worse on larger ones. VAE tend to be more stable than GAIN, even in cases where they perform worse. In another approach, the missing values weren\u2019t directly imputed. Instead the missing data was fed to a neural network, and the missing data uncertainty is modeled using probabilistic functions with parametric density, like GMMs, which is learnt with the remaining neural network parameters. Such an approach can work with incomplete datasets, without the need of accurate output for training.","title":"Limited Data"},{"location":"machine-learning/posts/automl-problems-2018/#catastrophic-forgetting","text":"Human beings are capable of learning sequentially, where knowledge regarding a new domain doesn't wipe out previously acquired information. Neural Networks, on the other hand, traditionally suffer from a phenomenon called catastrophic forgetting where adaptation over a new task completely wipes out the \"memory\" of the problem they were previously solving. Lately, there has been some work which promises to overcome catastrophic forgetting and allow networks to learn sequentially. Intuitively, the most basic way is to keep predictions over old task unchanged when training for newer ones. This, however limits the task distributions. Other way is to ensure that the model parameters always remain in the neighbourhood of optimal parameters achieved for previous iterations. This requires a large number of parameters to be stored. Later, we overcame these disadvantages by training undercomplete autoencoders to preserve feature information important to various tasks. Deepmind came up with Elastic Weight Consolidation where knowledge is preserved by reducing weight plasticity, where parameter values are drawn back to their older values proportional to their degree of importance for the previous task. EWC, however is sensitive to diagonal approximation of Fischer Information Matrix. Another related work proposed reparametrization of the layers by rotating FIM in the directions which are less prone to forgetting. This resulted in more compact and diagonal FIM. SupportNet uses SVMs to recognize \"support data\" which, along with the data for the newer tasks is fed to the neural networks for adapting to newer tasks while retaining information about the older ones. In a related work, the authors used incremental moment matching along with transfer learning techniques like weight transfer and a variant of dropout which boosted the performance of IMM. Some researchers later used an application oriented view for incremental learning to evaluate the performance of various approaches to cater to catastrophic forgetting. They concluded that dropout doesn't particularly help and EWC, though partly effective still doesn't perform up to the mark.","title":"Catastrophic Forgetting"},{"location":"machine-learning/posts/automl-problems-2018/#towards-agi","text":"Current Machine Learning paradigms have achieved human-liker performaces in various domains such as games like Go. However, a very important domain where these models lack the human-like performance is to perform sufficiently well in multiple tasks. Meta Learning, or learning to learn focuses on bridging this gap in Artificial Intelligence. Such models are introduced to numerous tasks, and then they are evaluated on how efficiently they can learn a new one. Meta Learning tasks can be classified into 3 approaches -: Using RNNs as base learners, where dataset is processed sequentially and using gradient descent to train the meta learner. Using a meta-learned metric space, where the base learner follows a comparison scheme and meta learner uses gradient descent. To learn an optimizer which updates the base learner to efficiently learn a new task. Such a meta-learner is usually an RNN, trained using supervised learning or reinforcement learning which remembers how the base learner was previously updated. In a different approach Model Agnostic Meta Learning, the authors proposed a model and task independent mechanism which tunes a model\u2019s parameters in order to quickly adapt to a new problem statement with the help of few gradient updates. The parameters for the new task are calculated by performing a single or multiple gradient descent operations on the original parameters. The original model parameters are then updated using stochastic gradient descent using the loss over new parameters for the new problems sampled from a distribution of tasks. The basic idea behind this approach is to learn features which are applicable to a distribution of tasks, rather than to that of a single task. In spite of the simplicity of MAML,it outperformed various few-shot image classification benchmarks, and therefore brought us one step closer to building more versatile agents. While given the advances in GPUs as well as parallel distribution, several systems are now able to not only compute but also infer the results from past training results. However, one of the key limitations of the widely-accepted methods are their lack of causal inference given these models are trained in a model-blind manner. While there have been several model-based approaches been used in reinforcement learning, however their application to AutoML methods yet remains to be seen and compared against that of model-free methods. One of the key limitations for these methods in AutoML is the ability to capture abstraction in the data. Several approaches in the past have been thus explored to deal with abstraction and hierarchy in the data namely Fuzzy Set Theory, Rough Set Theory and Quotient Space Based Models. Christoph Molnar emphasized on understanding how a machine learning model comes up with a prediction, in addition to what the prediction was. The basis of this hypothesis lies closely with human curiosity and our quest to find meaning in life. The study is divided into 2 parts - Interpretable Models and Model Agnostic Methods for interpretability. Linear models like Linear Regression and Logistic Regression are inherently interpretable models. However, these models suffer from dependence on handcrafted non-linearities and lower predictive performance. Other models including Decision Trees, Decision Rules etc. also fall under the category of interpretable models where importance of each feature can be computed. However these models struggle to accurately model linear correlations. Model agnostic methods focus on developing stand-alone techniques to interpret the results of a machine learning model. The model is treated as a black box, whose parameters, weights or even architecture is unknown. Certain types of agnostic methods try to model the relationship between the features and the output of the ML model. These methods are easily understandable and intuitive to human beings. However, these methods rely on the assumption of independence among the input features, which is not true for many real world scenarios. Another class of methods focuses on the input features. These methods include the study the meaningful but computationally expensive interaction among the features or studying how the model error changes with permutation in the input feature values. The latter method provides a global overview of the model, but is prone to outliers. The third class of model agnostic methods comprise of surrogate models. The global surrogate method uses the predictions of a black box model for the input data to train an inherently interpretable model, which is then used to carry out the interpretability study. The Local Surrogate Method (LIME) generates permuted data samples for the sample of interest, along with the black-box model\u2019s outcomes for these new data points. It then trains an interpretable model over this newly generate dataset where each input sample\u2019s weight is based upon its distance from the original sample. These methods are flexible and can be used over any type of dataset. However the performance of these methods depend upon the choice and performance of the surrogate interpretable models.","title":"Towards AGI"},{"location":"ml-engineering/posts/2019-05-10-why-tensorlayer/","tags":["open source","large scale ML systems","deep learning"],"text":"10 May, 2019 ORIGINALLY PUBLISHED ON OFFICIAL TENSORLAYER BLOG As we are moving towards the next leap in deep learning, there has been a lot of progress on the engineering front, esp the framework front. Within the past few years, we have seen a huge number of machine learning libraries and frameworks to enable us to build machine learning models \u2014 faster, scalable and accessible. Two of the most prominent of these frameworks are \u2014 TensorFlow and PyTorch . While it is incredibly tempting to do a side by side comparison for both but I think, to sum it up, one can say that PyTorch is easier to use, lighter and flexible (dynamic computational graph) whereas TensorFlow is a complete production level tool. While it has more abstractions and a steep learning curve but it is extremely powerful and tops everything else when it comes to scalability for your deep learning models. When working with deep learning models however, this low-level architecture mastery for TensorFlow and the high-level architecture mastery for deep learning can seem like a ridiculously tall order. It leads to an abstraction gap that demands for some bridging tools. These bridging tools need to fulfill few key requirements in order to be effective: Simplicity Flexibility Functionality Portability Scalability Performance Thus, over the past few years a few excellent bridging tools and libraries have been developed. These include TensorLayer, Keras, TFLearn etc. But do they compare? So what is TensorLayer? \u00b6 TensorLayer , is a part of Google\u2019s popular machine learning and numerical computational framework TensorFlow . It provides popular Deep Learning and Reinforcement Learning modules that can be easily customized and assembled for tackling real-world machine learning problems. The idea behind this library was to facilitate a modular approach to Deep Learning as well as Reinforcement Learning to tackle complex as well as iterative tasks for when it comes to large neural networks and their interactions. How? \u00b6 1 . It provides high-level state-of-the-art deep learning modules 2 . It enables the users to build a model using native TensorFlow APIs Also, 3 . It enables the users to define their own computational operations 4 . It enables the users to define their own training logic 5 . The users can glue different modules together (e.g., connected with TF-Slim and Keras ). 6 . It provides zero-cost abstraction (or negligible overhead) 7 . It allows easy scaling of your models from laptops to clouds. In fact, it is because of these incredibly powerful features that TensorLayer was awarded the 2017 Best Open Source Software by the prestigious ACM Multimedia Society. Quick Recap: \u00b6 Speed: It is basically designed in order to speed-up experimentation and developments by providing a higher-level API to TensorFlow . Flexibility: TensorLayer APIs are transparent in nature hence leaving the user with massive hooks that support diverse low-level tuning. Customizable : TensorLayer is quite easily extensible as well as modifiable. Zero-cost Abstraction: TensorLayer has the ability to achieve the performance of TensorFlow at its fullest. Modular Reference Layers: The highlighting feature of TensorLayer lies in the IDE-like approach where the host of operations such as neural networks , their states, data and other parameters are assorted into easily accessible modules. When compared with TensorFlow , TensorLayer closely fares the same when classic ML models of neural networks are performed. Although it might look like a setback, it allows the layer module in TensorLayer to cater to customized models. But on the other hand, it uses indexing concepts for quicker row selection while handling datasets. Also, the cache is stored locally to handle workloads with larger data for optimal performance. In addition, it also helps the workflow module to implement Deep Learning models using \u2018asynchronous training \u2019. How does TensorLayer Work? \u00b6 TensorLayer relies on TensorFlow\u2019s computational engine for training using MongoDB as the storage backend. Citations: ArXiv Paper: TensorLayer: A Versatile Library for Efficient Deep Learning Development TensorLayer Documentation What Is TensorLayer And How Is It Different From TensorFlow\u2019s Other Machine Learning Libraries? Presenting TensorLayer for Researchers and Engineers: A transparent Deep Learning and Reinforcement Learning Library","title":"Why TensorLayer?"},{"location":"ml-engineering/posts/2019-05-10-why-tensorlayer/#so-what-is-tensorlayer","text":"TensorLayer , is a part of Google\u2019s popular machine learning and numerical computational framework TensorFlow . It provides popular Deep Learning and Reinforcement Learning modules that can be easily customized and assembled for tackling real-world machine learning problems. The idea behind this library was to facilitate a modular approach to Deep Learning as well as Reinforcement Learning to tackle complex as well as iterative tasks for when it comes to large neural networks and their interactions.","title":"So what is TensorLayer?"},{"location":"ml-engineering/posts/2019-05-10-why-tensorlayer/#how","text":"1 . It provides high-level state-of-the-art deep learning modules 2 . It enables the users to build a model using native TensorFlow APIs Also, 3 . It enables the users to define their own computational operations 4 . It enables the users to define their own training logic 5 . The users can glue different modules together (e.g., connected with TF-Slim and Keras ). 6 . It provides zero-cost abstraction (or negligible overhead) 7 . It allows easy scaling of your models from laptops to clouds. In fact, it is because of these incredibly powerful features that TensorLayer was awarded the 2017 Best Open Source Software by the prestigious ACM Multimedia Society.","title":"How?"},{"location":"ml-engineering/posts/2019-05-10-why-tensorlayer/#quick-recap","text":"Speed: It is basically designed in order to speed-up experimentation and developments by providing a higher-level API to TensorFlow . Flexibility: TensorLayer APIs are transparent in nature hence leaving the user with massive hooks that support diverse low-level tuning. Customizable : TensorLayer is quite easily extensible as well as modifiable. Zero-cost Abstraction: TensorLayer has the ability to achieve the performance of TensorFlow at its fullest. Modular Reference Layers: The highlighting feature of TensorLayer lies in the IDE-like approach where the host of operations such as neural networks , their states, data and other parameters are assorted into easily accessible modules. When compared with TensorFlow , TensorLayer closely fares the same when classic ML models of neural networks are performed. Although it might look like a setback, it allows the layer module in TensorLayer to cater to customized models. But on the other hand, it uses indexing concepts for quicker row selection while handling datasets. Also, the cache is stored locally to handle workloads with larger data for optimal performance. In addition, it also helps the workflow module to implement Deep Learning models using \u2018asynchronous training \u2019.","title":"Quick Recap:"},{"location":"ml-engineering/posts/2019-05-10-why-tensorlayer/#how-does-tensorlayer-work","text":"TensorLayer relies on TensorFlow\u2019s computational engine for training using MongoDB as the storage backend. Citations: ArXiv Paper: TensorLayer: A Versatile Library for Efficient Deep Learning Development TensorLayer Documentation What Is TensorLayer And How Is It Different From TensorFlow\u2019s Other Machine Learning Libraries? Presenting TensorLayer for Researchers and Engineers: A transparent Deep Learning and Reinforcement Learning Library","title":"How does TensorLayer Work?"},{"location":"ml-engineering/posts/ds-best-practices/","tags":["machine learning","software engineering","best practices"],"text":"October 23, 2022 DRAFT In the past few years, data scientists have been deeply integrated inside not just the business process and decisions but in day-to-day workings of the product. With this shift, data scientists have somehow become an integral part of the product team.","title":"Best Engineering Practices for Data Scientists"},{"location":"ml-engineering/posts/mlops-open-problems/","tags":["machine learning","software engineering","best practices"],"text":"December 12, 2022 ONGOING There are two ways of exploring MLOps. First, the industry way is by getting our hands dirty building something and using the tools, libraries, and frameworks available. This is a bad way of starting out. Reason being it\u2019s so easy to get overwhelmed and lose perspective when inundated with so many choices - each time-consuming, with a steep learning curve. So, here\u2019s a second way - the academic way. Let\u2019s get a bird\u2019s eye view of what\u2019s going on in the field. MLOps as an academic research field is still new with only a handful of directly-relevant papers. So, I compiled a list of some of my favorite papers \ud83d\udcdcin MLOps. Some Papers in MLOps (as of Oct 2022) \u00b6 The top one would be Machine Learning: The High-Interest Credit Card of Technical Debt by D. Sculley et al . We invited him as a guest on our MLOps Community podcast (Spotify/iTunes) Episode #32 - def worth listening to! If there's one I would def read it would be Machine Learning Operations (MLOps): Overview, Definition, and Architecture by Dominik, Niklas and Sebastian . It highlights necessary principles, components, and associated architecture and workflows in MLOps arxiv.org/abs/2205.02302 A recent one is Operationalizing Machine Learning: An Interview Study by Shreya et al which interviews 18 MLOps practitioners and discusses common practices across different stages of an ML project from experimentation ->deployment-> monitoring. While a guide for academia, but generally applicable best practices for all Data Scientists and ML Engineers is How to avoid machine learning pitfalls: a guide for academic researchers by Michael A. Lones While this one by Cote et al is the description of researchers' approach to designing a study that would hopefully guide how to build quality assurance tools in ML Software Systems (the study is yet to be out) but it does bring attention to an open challenge. Next is a paper that talks about how to address the eng challenges associated with distributed training if u don't have the necessary infrastructure to match the big corps with infinite compute and a million hyperparameters. Training Transformers Together Of course, the list won't be complete without a discussion of Jupyter NBs. But what would be the performance difference b/w notebooks vs scripts and the pros and cons of each? \ud83d\udcdcA Large-Scale Comparison of Python Code in Jupyter Notebooks and Scripts But what about Production Infrastructure? How to cater to data stalls in the pre-processing pipeline? Understanding Data Storage and Ingestion for Large-Scale Deep Recommendation Model Training Last, how can all the progress in machine learning guide the future of chip design? The paper by Jeff Dean provides an interesting outlook on hardware for software folks. The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design Core Challenges in MLOps [Quick Thoughts: Dec 2022] \u00b6 Note Inspired by Advent of Code, Advent of Data 2022 is 24 data science, data engineering, machine learning articles by 24 authors until Christmas. This write-up is a re-post from the first post for Advent of Data: MLOps isn\u2019t DevOps for ML for by me. MLOps requires a lot of moving variables across the business, data, code as well as model engineering at each phase of the machine learning lifecycle introducing questions that are unique to ML. Over the past few years, the adoption of cloud engineering, understanding of big data, and massive popularity of the open-source libraries for predictive modeling have made it easier for everyone to be enticed by the possibility to generate user insights or personalize their \u201csoftware 1.0\u201d by hiring a team of data scientists. However, to most companies\u2019 constant surprise, data scientists alone have been far from adequately equipped to handle/guide the end-to-end deployment or monitor or debug these models once deployed in production. It truly needs an army, some in-house (DE/MLE) and some outsourced to the A.I. tooling/SaaS companies. While every data team at different companies is substantially different however there are some core challenges that are common to everyone. For a machine learning model to be considered successful, it must be able to generate stakeholder buy-in. Thus, it becomes incredibly important to tie the models to business KPIs instead of the accuracy scores (F1, recall, precision, ROC, AUC). However, with business KPIs changing every so often through different stages it becomes incredibly hard to measure the model performances. For any business to build powerful and reliable ML models, investing effort into creating and maintaining a data catalog becomes crucial to track meta-data and while debugging, to retrieve information on which data source is the model being trained on. While building a data catalog may not seem like a hard task but the real challenge is building relevancy into data discovery. This is often when a lot of companies give up. If you instead decide to opt for a commercial solution, most of the out-of-the-box commercial data cataloging solutions do not adapt well to different organizations\u2019 data needs and cost several kidneys and more. Requesting a feature-add can put you on nothing short of an eight-ten month-long waitlist, optimistically speaking, if the requested feature is even aligned with their product plan. The final option i.e. to build an in-house solution requires upfront investment and a team with an excellent understanding of user-friendly database design practices thus making it a time and resource-consuming process. To make it even harder, there is a lack of documentation around best practices about creating, managing, and scaling an in-house data-cataloging tool and certain evaluation/compliance metrics so as to not end up with an incomplete catalog esp with the new live data being streamed into the system making the effort futile at best. Your machine-learning model is only as good as your data . For any data science project to be successful, data quality and more importantly labeled data quantity are the biggest defining factors. However, best practices on data evaluation about how to standardize and normalize new incoming data are still case-by-case considerations. Most training environments need to come pre-loaded with few checks and balances based on the different stages of model deployment. For example, for a model that is being tested for production, has a random seed been set on the model to make sure that the data is divided the same way every time the code is run? While there are many advantages to using commercial feature stores however they can also introduce inflexibility and also limit the customization of models and sometimes you simply don\u2019t need them (more on this in a next month\u2019s post). This inspires many to go with open-source solutions and develop their own on top of say Feast or DVC. While batch features may be easier to maintain, real-time features are sometimes inescapable for several reasons. Real-time features introduce a lot of complexity into the system, especially around back-filling real-time data from streaming sources with data-sharing restrictions. This requires not only technical but also process controls that are often not talked about. Recently, there has been more discussion around Data Contracts however, they are not yet a commonly accepted practice across organizations. There is a lack of commonly well-defined best practices around creating model version control or project structures at different stages from exploration to deployment . Cookie cutter is one of the efforts toward developing a unified project structure for cross-team collaborations. Undefined/poorly defined prerequisites about when to push a model in production can create unnecessary bugs and introduce delays during monitoring and debugging. Code Reviews how much time should be spent on code review in different stages especially given the model behavior may not accurately represent live training data and how frequently should they happen? Different companies currently have different systems for it. While some prefer one-off deployment, others have more clustered deployment stages eg. test, dev, staging, shadow, and A/B for business-critical pipelines with different review stages and guidelines. However, even the end-to-end tools do not have any in-built support for the same. It\u2019s very much only institutional knowledge as of now as to what makes good quality production code. While it is clear to everyone that test-driven development is critical to catch minor errors early in the deployment stage however how much time and effort be invested in the same given there are large samples of data that can only be gathered once the model is deployed in production? To use static validation sets to test the models in production which can introduce bias or dynamic validation sets to more closely resemble live data and address localized shifts in data. Should we use model registries or change only config files instead of the model thus making it easier to debug? For the former, if model validation passes all checks, the model is usually passed to a model registry. Having clearly defined rule-based tests to make sure the model outputs are not incorrect while factoring in when is it okay to give incorrect outputs (for eg. shopping recommendations) vs to give no outputs (for eg. cancer prescriptions). Best practices around code quality and a need for similar deployment environments. While most data scientists prefer working with Jupyter NBs however the way code is usually written in NBs (copy-paste) instead of re-using functions, can introduce unnecessary bugs and introduce technical debt affecting the model as well as the integration code when the notebook owner leaves the team. While experiment tracking tools and dashboards have added quite some observability to the model runs, contextual changes still remain majorly undocumented. While sandbox tools to stress-test can be quite useful in some scenarios, however in others for eg. recsys it may not generate any useful information whatsoever. The warnings about which alerts are critical and require a quick migration to a failsafe model (for eg. hate speech, racial or gender bias) and which ones are mere information to be factored in for the next model configuration phase still require close human monitoring. I am working on a longer blog-post of the same for MLOps.Community - to recieve it when it goes out first subscribe to my Substack . It's free. I send out one letter a month exclusively on MLOps. Some Papers in MLOps (as of Dec 2022) \u00b6 Some more MLOps papers \ud83d\udcdc you may find interesting to read a little further into the best practices and challenges with machine learning models deployed in production. \u2935\ufe0f This is not a paper but a blogpost from Microsoft around the same lines as Google's 2014 paper about High Credit Card Interest Debt paper by Sculley et al - \ud83d\udcacTechnical debt in Machine Learning: Pay off this \u201chigh interest rate credit card\u201d sooner rather than later. Testing and monitoring are key considerations for ensuring the production-readiness of an ML system, and for reducing the technical debt of ML systems. In this paper, The ML test score: A rubric for ML production readiness and technical debt reduction the authors present 28 specific tests and monitoring needs. In this paper Large-scale machine learning systems in real-world industrial settings , the authors identify a total of 23 challenges and 8 solutions related to the development and maintenance of large-scale ML-based software systems in industrial setting. Remember distill.pub's Research Debt article that caused a massive debate in ML Res community about Reproducibility? In this paper, Do machine learning platforms provide out-of-the-box reproducibility , the authors propose a framework for it. \ud83d\ude0d by none other than Zachary Lipton et al - a no-brainer if you have read his excellent work in interpretability. Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift proposes a two-sample-testing-based approach for dim reduction. One of my fav papers is Large scale distributed neural network training through online distillation by Rohan Anil et al that talks about the test-time cost for ensemble modelling and an alternative i.e. online distillation that's far more cost effective. Privacy and security is such an important part of software development however, not often talked about in ML. In this paper, Adversarial Machine Learning-Industry Perspectives the authors interviewed 28 orgs to propose amends in the Security Development Lifecycle for industrial-grade software in ML era. Last but not the least, is Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure . It proposes a rigorous framework for dataset development transparency that supports decision-making and accountability.","title":"Open Problems in MLOps"},{"location":"ml-engineering/posts/mlops-open-problems/#some-papers-in-mlops-as-of-oct-2022","text":"The top one would be Machine Learning: The High-Interest Credit Card of Technical Debt by D. Sculley et al . We invited him as a guest on our MLOps Community podcast (Spotify/iTunes) Episode #32 - def worth listening to! If there's one I would def read it would be Machine Learning Operations (MLOps): Overview, Definition, and Architecture by Dominik, Niklas and Sebastian . It highlights necessary principles, components, and associated architecture and workflows in MLOps arxiv.org/abs/2205.02302 A recent one is Operationalizing Machine Learning: An Interview Study by Shreya et al which interviews 18 MLOps practitioners and discusses common practices across different stages of an ML project from experimentation ->deployment-> monitoring. While a guide for academia, but generally applicable best practices for all Data Scientists and ML Engineers is How to avoid machine learning pitfalls: a guide for academic researchers by Michael A. Lones While this one by Cote et al is the description of researchers' approach to designing a study that would hopefully guide how to build quality assurance tools in ML Software Systems (the study is yet to be out) but it does bring attention to an open challenge. Next is a paper that talks about how to address the eng challenges associated with distributed training if u don't have the necessary infrastructure to match the big corps with infinite compute and a million hyperparameters. Training Transformers Together Of course, the list won't be complete without a discussion of Jupyter NBs. But what would be the performance difference b/w notebooks vs scripts and the pros and cons of each? \ud83d\udcdcA Large-Scale Comparison of Python Code in Jupyter Notebooks and Scripts But what about Production Infrastructure? How to cater to data stalls in the pre-processing pipeline? Understanding Data Storage and Ingestion for Large-Scale Deep Recommendation Model Training Last, how can all the progress in machine learning guide the future of chip design? The paper by Jeff Dean provides an interesting outlook on hardware for software folks. The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design","title":"Some Papers in MLOps (as of Oct 2022)"},{"location":"ml-engineering/posts/mlops-open-problems/#core-challenges-in-mlops-quick-thoughts-dec-2022","text":"Note Inspired by Advent of Code, Advent of Data 2022 is 24 data science, data engineering, machine learning articles by 24 authors until Christmas. This write-up is a re-post from the first post for Advent of Data: MLOps isn\u2019t DevOps for ML for by me. MLOps requires a lot of moving variables across the business, data, code as well as model engineering at each phase of the machine learning lifecycle introducing questions that are unique to ML. Over the past few years, the adoption of cloud engineering, understanding of big data, and massive popularity of the open-source libraries for predictive modeling have made it easier for everyone to be enticed by the possibility to generate user insights or personalize their \u201csoftware 1.0\u201d by hiring a team of data scientists. However, to most companies\u2019 constant surprise, data scientists alone have been far from adequately equipped to handle/guide the end-to-end deployment or monitor or debug these models once deployed in production. It truly needs an army, some in-house (DE/MLE) and some outsourced to the A.I. tooling/SaaS companies. While every data team at different companies is substantially different however there are some core challenges that are common to everyone. For a machine learning model to be considered successful, it must be able to generate stakeholder buy-in. Thus, it becomes incredibly important to tie the models to business KPIs instead of the accuracy scores (F1, recall, precision, ROC, AUC). However, with business KPIs changing every so often through different stages it becomes incredibly hard to measure the model performances. For any business to build powerful and reliable ML models, investing effort into creating and maintaining a data catalog becomes crucial to track meta-data and while debugging, to retrieve information on which data source is the model being trained on. While building a data catalog may not seem like a hard task but the real challenge is building relevancy into data discovery. This is often when a lot of companies give up. If you instead decide to opt for a commercial solution, most of the out-of-the-box commercial data cataloging solutions do not adapt well to different organizations\u2019 data needs and cost several kidneys and more. Requesting a feature-add can put you on nothing short of an eight-ten month-long waitlist, optimistically speaking, if the requested feature is even aligned with their product plan. The final option i.e. to build an in-house solution requires upfront investment and a team with an excellent understanding of user-friendly database design practices thus making it a time and resource-consuming process. To make it even harder, there is a lack of documentation around best practices about creating, managing, and scaling an in-house data-cataloging tool and certain evaluation/compliance metrics so as to not end up with an incomplete catalog esp with the new live data being streamed into the system making the effort futile at best. Your machine-learning model is only as good as your data . For any data science project to be successful, data quality and more importantly labeled data quantity are the biggest defining factors. However, best practices on data evaluation about how to standardize and normalize new incoming data are still case-by-case considerations. Most training environments need to come pre-loaded with few checks and balances based on the different stages of model deployment. For example, for a model that is being tested for production, has a random seed been set on the model to make sure that the data is divided the same way every time the code is run? While there are many advantages to using commercial feature stores however they can also introduce inflexibility and also limit the customization of models and sometimes you simply don\u2019t need them (more on this in a next month\u2019s post). This inspires many to go with open-source solutions and develop their own on top of say Feast or DVC. While batch features may be easier to maintain, real-time features are sometimes inescapable for several reasons. Real-time features introduce a lot of complexity into the system, especially around back-filling real-time data from streaming sources with data-sharing restrictions. This requires not only technical but also process controls that are often not talked about. Recently, there has been more discussion around Data Contracts however, they are not yet a commonly accepted practice across organizations. There is a lack of commonly well-defined best practices around creating model version control or project structures at different stages from exploration to deployment . Cookie cutter is one of the efforts toward developing a unified project structure for cross-team collaborations. Undefined/poorly defined prerequisites about when to push a model in production can create unnecessary bugs and introduce delays during monitoring and debugging. Code Reviews how much time should be spent on code review in different stages especially given the model behavior may not accurately represent live training data and how frequently should they happen? Different companies currently have different systems for it. While some prefer one-off deployment, others have more clustered deployment stages eg. test, dev, staging, shadow, and A/B for business-critical pipelines with different review stages and guidelines. However, even the end-to-end tools do not have any in-built support for the same. It\u2019s very much only institutional knowledge as of now as to what makes good quality production code. While it is clear to everyone that test-driven development is critical to catch minor errors early in the deployment stage however how much time and effort be invested in the same given there are large samples of data that can only be gathered once the model is deployed in production? To use static validation sets to test the models in production which can introduce bias or dynamic validation sets to more closely resemble live data and address localized shifts in data. Should we use model registries or change only config files instead of the model thus making it easier to debug? For the former, if model validation passes all checks, the model is usually passed to a model registry. Having clearly defined rule-based tests to make sure the model outputs are not incorrect while factoring in when is it okay to give incorrect outputs (for eg. shopping recommendations) vs to give no outputs (for eg. cancer prescriptions). Best practices around code quality and a need for similar deployment environments. While most data scientists prefer working with Jupyter NBs however the way code is usually written in NBs (copy-paste) instead of re-using functions, can introduce unnecessary bugs and introduce technical debt affecting the model as well as the integration code when the notebook owner leaves the team. While experiment tracking tools and dashboards have added quite some observability to the model runs, contextual changes still remain majorly undocumented. While sandbox tools to stress-test can be quite useful in some scenarios, however in others for eg. recsys it may not generate any useful information whatsoever. The warnings about which alerts are critical and require a quick migration to a failsafe model (for eg. hate speech, racial or gender bias) and which ones are mere information to be factored in for the next model configuration phase still require close human monitoring. I am working on a longer blog-post of the same for MLOps.Community - to recieve it when it goes out first subscribe to my Substack . It's free. I send out one letter a month exclusively on MLOps.","title":"Core Challenges in MLOps [Quick Thoughts: Dec 2022]"},{"location":"ml-engineering/posts/mlops-open-problems/#some-papers-in-mlops-as-of-dec-2022","text":"Some more MLOps papers \ud83d\udcdc you may find interesting to read a little further into the best practices and challenges with machine learning models deployed in production. \u2935\ufe0f This is not a paper but a blogpost from Microsoft around the same lines as Google's 2014 paper about High Credit Card Interest Debt paper by Sculley et al - \ud83d\udcacTechnical debt in Machine Learning: Pay off this \u201chigh interest rate credit card\u201d sooner rather than later. Testing and monitoring are key considerations for ensuring the production-readiness of an ML system, and for reducing the technical debt of ML systems. In this paper, The ML test score: A rubric for ML production readiness and technical debt reduction the authors present 28 specific tests and monitoring needs. In this paper Large-scale machine learning systems in real-world industrial settings , the authors identify a total of 23 challenges and 8 solutions related to the development and maintenance of large-scale ML-based software systems in industrial setting. Remember distill.pub's Research Debt article that caused a massive debate in ML Res community about Reproducibility? In this paper, Do machine learning platforms provide out-of-the-box reproducibility , the authors propose a framework for it. \ud83d\ude0d by none other than Zachary Lipton et al - a no-brainer if you have read his excellent work in interpretability. Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift proposes a two-sample-testing-based approach for dim reduction. One of my fav papers is Large scale distributed neural network training through online distillation by Rohan Anil et al that talks about the test-time cost for ensemble modelling and an alternative i.e. online distillation that's far more cost effective. Privacy and security is such an important part of software development however, not often talked about in ML. In this paper, Adversarial Machine Learning-Industry Perspectives the authors interviewed 28 orgs to propose amends in the Security Development Lifecycle for industrial-grade software in ML era. Last but not the least, is Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure . It proposes a rigorous framework for dataset development transparency that supports decision-making and accountability.","title":"Some Papers in MLOps (as of Dec 2022)"},{"location":"podcast/posts/legal-tech/","text":"Episode 1 \u00b6 Wondering about how copyright works for online data? What is fair data use for Data Scientists? What are the legal implications of building A.I. Products using non-consented data? Releasing soon on Amazon Music, Spotify, iTunes Episode 2 \u00b6 What is permissionless innovation? We break out open-source software litigation and intellectual property for listeners. Releasing soon on Amazon Music, Spotify, iTunes","title":"Podcast"},{"location":"podcast/posts/legal-tech/#episode-1","text":"Wondering about how copyright works for online data? What is fair data use for Data Scientists? What are the legal implications of building A.I. Products using non-consented data? Releasing soon on Amazon Music, Spotify, iTunes","title":"Episode 1"},{"location":"podcast/posts/legal-tech/#episode-2","text":"What is permissionless innovation? We break out open-source software litigation and intellectual property for listeners. Releasing soon on Amazon Music, Spotify, iTunes","title":"Episode 2"},{"location":"venture-capital/posts/creating-a-pitchdeck/","tags":["venture capital","fundraising","startup essays"],"text":"October 23, 2022 DRAFT Should we hire a designer? Yes and No. Allow me to explain.","title":"How to Create a Winning Pitchdeck"},{"location":"venture-capital/posts/due-dilligence/","tags":["venture capital","fundraising","startup essays"],"text":"October 23, 2022 DRAFT How much due dilligence is enough? It depends. But 2 weeks.","title":"How to carry out due dilligence as an A.I. VC Fund?"},{"location":"tags/","text":"PCG \u00b6 Developing A.I. using Games annotated papers \u00b6 Dense Associative Memory for Pattern Recognition automl \u00b6 Problems and Challenges in AutoML best practices \u00b6 Best Engineering Practices for Data Scientists What's new in MLOps? content creation \u00b6 So you want to start a podcast? Why you shouldn't be a content creator? deep learning \u00b6 Dense Associative Memory for Pattern Recognition Developing A.I. using Games Why TensorLayer? fundraising \u00b6 How to Create a Winning Pitchdeck How to carry out due dilligence as an A.I. VC Fund? hopfield networks \u00b6 Dense Associative Memory for Pattern Recognition human behaviour \u00b6 Why you shouldn't be a content creator? large scale ML systems \u00b6 Why TensorLayer? machine learning \u00b6 Problems and Challenges in AutoML Best Engineering Practices for Data Scientists What's new in MLOps? meta-learning \u00b6 Problems and Challenges in AutoML multiagent systems \u00b6 Developing A.I. using Games neural architecture search \u00b6 Problems and Challenges in AutoML open source \u00b6 Why TensorLayer? podcasting \u00b6 So you want to start a podcast? programming \u00b6 Why Re-Learn Your Core Skills? software engineering \u00b6 Why Re-Learn Your Core Skills? Best Engineering Practices for Data Scientists What's new in MLOps? startup essays \u00b6 How to Create a Winning Pitchdeck How to carry out due dilligence as an A.I. VC Fund? venture capital \u00b6 How to Create a Winning Pitchdeck How to carry out due dilligence as an A.I. VC Fund?","title":"Sections"},{"location":"tags/#pcg","text":"Developing A.I. using Games","title":"PCG"},{"location":"tags/#annotated-papers","text":"Dense Associative Memory for Pattern Recognition","title":"annotated papers"},{"location":"tags/#automl","text":"Problems and Challenges in AutoML","title":"automl"},{"location":"tags/#best-practices","text":"Best Engineering Practices for Data Scientists What's new in MLOps?","title":"best practices"},{"location":"tags/#content-creation","text":"So you want to start a podcast? Why you shouldn't be a content creator?","title":"content creation"},{"location":"tags/#deep-learning","text":"Dense Associative Memory for Pattern Recognition Developing A.I. using Games Why TensorLayer?","title":"deep learning"},{"location":"tags/#fundraising","text":"How to Create a Winning Pitchdeck How to carry out due dilligence as an A.I. VC Fund?","title":"fundraising"},{"location":"tags/#hopfield-networks","text":"Dense Associative Memory for Pattern Recognition","title":"hopfield networks"},{"location":"tags/#human-behaviour","text":"Why you shouldn't be a content creator?","title":"human behaviour"},{"location":"tags/#large-scale-ml-systems","text":"Why TensorLayer?","title":"large scale ML systems"},{"location":"tags/#machine-learning","text":"Problems and Challenges in AutoML Best Engineering Practices for Data Scientists What's new in MLOps?","title":"machine learning"},{"location":"tags/#meta-learning","text":"Problems and Challenges in AutoML","title":"meta-learning"},{"location":"tags/#multiagent-systems","text":"Developing A.I. using Games","title":"multiagent systems"},{"location":"tags/#neural-architecture-search","text":"Problems and Challenges in AutoML","title":"neural architecture search"},{"location":"tags/#open-source","text":"Why TensorLayer?","title":"open source"},{"location":"tags/#podcasting","text":"So you want to start a podcast?","title":"podcasting"},{"location":"tags/#programming","text":"Why Re-Learn Your Core Skills?","title":"programming"},{"location":"tags/#software-engineering","text":"Why Re-Learn Your Core Skills? Best Engineering Practices for Data Scientists What's new in MLOps?","title":"software engineering"},{"location":"tags/#startup-essays","text":"How to Create a Winning Pitchdeck How to carry out due dilligence as an A.I. VC Fund?","title":"startup essays"},{"location":"tags/#venture-capital","text":"How to Create a Winning Pitchdeck How to carry out due dilligence as an A.I. VC Fund?","title":"venture capital"}]}